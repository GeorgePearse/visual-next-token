# Visual Next Token - RL-Based Image Navigation

Curiosity-driven reinforcement learning for learning semantic paths through images by maximizing prediction error of future visual tokens.

[![Documentation](https://img.shields.io/badge/docs-mkdocs-blue)](https://georgepearse.github.io/visual-next-token/)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)
[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)

## Overview

Visual Next Token implements a novel approach to image navigation where an agent learns to explore images by **maximizing prediction error** (not accuracy) in semantic feature space. This curiosity-driven approach forces the agent to seek information-dense regions, naturally following semantic co-occurrence patterns like car â†’ road â†’ sky.

### Key Insight

Traditional approaches predict **what** comes next. We flip this:

- âŒ **Traditional**: Maximize prediction accuracy â†’ agent seeks boring, predictable regions
- âœ… **Our approach**: Maximize prediction error â†’ agent seeks surprising, information-dense regions

### The "Car Color Problem"

**Challenge**: Cars can be any color - pixel-level prediction penalizes irrelevant variations.

**Solution**: Use DINOv2 semantic features where red car â‰ˆ blue car in embedding space, but car â‰  road.

## Quick Start

```bash
# Clone repository
git clone https://github.com/georgepearse/visual-next-token.git
cd visual-next-token

# Install dependencies
pip install torch torchvision numpy opencv-python matplotlib

# Train RL navigator (quick test)
python experiments/train_rl_navigator.py --config quick_test

# Visualize learned paths
python experiments/visualize_rl_paths.py \
    --checkpoint checkpoints/rl_navigator/final.pt \
    --n_episodes 5
```

## Features

### ğŸ§  RL-Based Image Navigation
- **Curiosity-driven exploration** using prediction error as intrinsic reward
- **Two-phase training**: frozen encoder â†’ fine-tuned encoder
- **Semantic invariance**: DINOv2 features solve appearance variation issues
- **Exponential distance weighting** for multi-step lookahead planning

### ğŸ”¬ Multiple Intrinsic Motivation Methods
- **ICM (Intrinsic Curiosity Module)**: Forward dynamics prediction
- **RND (Random Network Distillation)**: Fixed target network prediction

### ğŸ¯ Production-Ready Components
- PPO policy optimization with GAE
- Hierarchical action spaces (base + jump/scout actions)
- Comprehensive training and visualization tools
- Complete documentation with paper summaries

## Architecture

```
Image (RGB) â†’ DINOv2 Encoder â†’ Semantic Features (z)
                                      â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚                                   â”‚
                    â–¼                                   â–¼
          Navigation Policy (Ï€)            Forward Dynamics (P)
              PPO + GAE                    ICM or RND
                    â”‚                                   â”‚
                    â”‚                                   â–¼
                    â”‚                          Prediction Error
                    â”‚                         (Intrinsic Reward)
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â–¼
                         PPO Update
```

## Project Structure

```
visual-next-token/
â”œâ”€â”€ techniques/
â”‚   â””â”€â”€ rl_navigation/          # RL navigation implementation
â”‚       â”œâ”€â”€ encoder.py          # DINOv2 semantic encoder
â”‚       â”œâ”€â”€ environment.py      # MDP for image navigation
â”‚       â”œâ”€â”€ policy.py           # PPO actor-critic
â”‚       â”œâ”€â”€ forward_dynamics.py # ICM / RND
â”‚       â”œâ”€â”€ trainer.py          # Two-phase training
â”‚       â”œâ”€â”€ extensions.py       # Jump/scout actions
â”‚       â””â”€â”€ config.py           # Hyperparameters
â”œâ”€â”€ experiments/
â”‚   â”œâ”€â”€ train_rl_navigator.py  # Training script
â”‚   â””â”€â”€ visualize_rl_paths.py  # Visualization
â”œâ”€â”€ references/
â”‚   â””â”€â”€ rl_navigation/          # Key papers with summaries
â”œâ”€â”€ docs/                       # MkDocs documentation
â””â”€â”€ README.md
```

## Documentation

Comprehensive documentation available at: **https://georgepearse.github.io/visual-next-token/**

Includes:
- ğŸ“š Detailed architecture explanations
- ğŸš€ Quick start guides
- ğŸ“„ Research paper summaries with code connections
- ğŸ”§ API reference

## Configuration Presets

| Config | Phase 1 | Phase 2 | Use Case |
|--------|---------|---------|----------|
| `quick_test` | 100 | 50 | Testing/debugging |
| `default` | 10,000 | 5,000 | Standard training |
| `rnd` | 10,000 | 5,000 | Use RND instead of ICM |
| `long` | 20,000 | 10,000 | Extended training with larger model |

## Research Foundation

Our implementation builds on five key papers:

1. **[ICM](references/rl_navigation/01_curiosity_driven_exploration_ICM.md)** - Curiosity-driven exploration (Pathak et al., 2017)
2. **[RND](references/rl_navigation/02_random_network_distillation_RND.md)** - Random network distillation (Burda et al., 2018)
3. **[PPO](references/rl_navigation/03_proximal_policy_optimization_PPO.md)** - Policy optimization (Schulman et al., 2017)
4. **[DINOv2](references/rl_navigation/04_dinov2_visual_features.md)** - Semantic features (Oquab et al., 2023)
5. **[GAE](references/rl_navigation/05_generalized_advantage_estimation_GAE.md)** - Advantage estimation (Schulman et al., 2015)

## Example Usage

```python
from techniques.rl_navigation import RLTrainer
import cv2

# Load image
image = cv2.imread("my_image.jpg")
image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

# Create trainer
trainer = RLTrainer(
    image=image,
    encoder_name="dinov2_vits14",
    phase1_episodes=10000,  # Frozen encoder
    phase2_episodes=5000,   # Fine-tuned encoder
    use_rnd=False,          # Use ICM (or True for RND)
)

# Train
trainer.train()
```

## Requirements

- Python 3.8+
- PyTorch 2.0+
- NumPy
- OpenCV
- Matplotlib

DINOv2 models are downloaded automatically via `torch.hub` on first use.

## Citation

If you use this work, please cite the foundational papers:

```bibtex
@inproceedings{pathak2017curiosity,
  title={Curiosity-driven exploration by self-supervised prediction},
  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},
  booktitle={ICML},
  year={2017}
}

@article{oquab2023dinov2,
  title={DINOv2: Learning Robust Visual Features without Supervision},
  author={Oquab, Maxime and others},
  journal={arXiv preprint arXiv:2304.07193},
  year={2023}
}
```

See [References](https://georgepearse.github.io/visual-next-token/references/) for complete citation information.

## License

MIT License - see LICENSE file for details.

## Contributing

Contributions welcome! Please see our [documentation](https://georgepearse.github.io/visual-next-token/) for more information.
