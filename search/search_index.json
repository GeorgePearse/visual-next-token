{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Visual Next Token - RL-Based Image Navigation","text":"<p>Welcome to Visual Next Token! This project implements curiosity-driven reinforcement learning for learning semantic paths through images by maximizing prediction accuracy over a rolling window of future visual tokens.</p>"},{"location":"#what-is-visual-next-token","title":"What is Visual Next Token?","text":"<p>Visual Next Token explores how agents can learn to navigate images by seeking semantically coherent paths. The core insight: maximizing prediction accuracy over a rolling window guides the agent to paths where it can build predictive understanding - from cars to roads to sky, following co-occurrence patterns in visual scenes.</p> <p>Example: Hitting a car edge gives poor initial prediction (color jump), but then excellent predictions of \"more car\" regions. High rolling-window accuracy = semantic coherence = information-rich path.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#rl-based-image-navigation","title":"\ud83e\udde0 RL-Based Image Navigation","text":"<ul> <li>Curiosity-driven exploration using prediction error as intrinsic reward</li> <li>Two-phase training: frozen encoder \u2192 fine-tuned encoder</li> <li>Semantic invariance: DINOv2 features solve the \"car color problem\"</li> <li>Exponential distance weighting for multi-step lookahead planning</li> </ul>"},{"location":"#multiple-intrinsic-motivation-methods","title":"\ud83d\udd2c Multiple Intrinsic Motivation Methods","text":"<ul> <li>ICM (Intrinsic Curiosity Module): Forward dynamics prediction</li> <li>RND (Random Network Distillation): Fixed target network prediction</li> </ul>"},{"location":"#production-ready-components","title":"\ud83c\udfaf Production-Ready Components","text":"<ul> <li>PPO policy optimization with GAE</li> <li>Hierarchical action spaces (base + jump/scout actions)</li> <li>Comprehensive training and visualization tools</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from techniques.rl_navigation import RLTrainer\n\n# Train RL navigator on your image\ntrainer = RLTrainer(\n    image=my_image,\n    encoder_name=\"dinov2_vits14\",\n    phase1_episodes=10000,  # Frozen encoder\n    phase2_episodes=5000,   # Fine-tuned encoder\n)\n\ntrainer.train()\n</code></pre> <pre><code># Visualize learned paths\npython experiments/visualize_rl_paths.py \\\n    --checkpoint checkpoints/rl_navigator/final.pt \\\n    --image my_image.jpg \\\n    --n_episodes 5\n</code></pre>"},{"location":"#the-core-insight","title":"The Core Insight","text":"<p>Traditional approaches predict what pixels come next. We flip this:</p> <p>Our Approach</p> <p>Maximize rolling-window prediction ACCURACY \u2192 Agent seeks semantically coherent paths</p> <ul> <li>High multi-step accuracy = understanding semantic regions (car \u2192 more car)</li> <li>Agent learns paths with rich semantic transitions</li> <li>Semantic features (DINOv2) ensure car color doesn't matter</li> </ul> <p>Traditional Single-Step Approach</p> <p>Maximize immediate prediction ACCURACY \u2192 Agent seeks trivially predictable regions</p> <ul> <li>Stays in uniform sky, blank walls</li> <li>No incentive to explore semantic structure</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>image-ssl/\n\u251c\u2500\u2500 techniques/\n\u2502   \u2514\u2500\u2500 rl_navigation/          # RL navigation implementation\n\u2502       \u251c\u2500\u2500 encoder.py          # DINOv2 semantic encoder\n\u2502       \u251c\u2500\u2500 environment.py      # MDP for image navigation\n\u2502       \u251c\u2500\u2500 policy.py           # PPO actor-critic\n\u2502       \u251c\u2500\u2500 forward_dynamics.py # ICM / RND\n\u2502       \u251c\u2500\u2500 trainer.py          # Two-phase training\n\u2502       \u251c\u2500\u2500 extensions.py       # Jump/scout actions\n\u2502       \u2514\u2500\u2500 config.py           # Hyperparameters\n\u251c\u2500\u2500 experiments/\n\u2502   \u251c\u2500\u2500 train_rl_navigator.py  # Training script\n\u2502   \u2514\u2500\u2500 visualize_rl_paths.py  # Visualization\n\u2514\u2500\u2500 references/\n    \u2514\u2500\u2500 rl_navigation/          # Key papers with summaries\n</code></pre>"},{"location":"#research-foundation","title":"Research Foundation","text":"<p>Our implementation builds on five key papers:</p> <ol> <li>ICM - Curiosity-driven exploration (Pathak et al., 2017)</li> <li>RND - Random network distillation (Burda et al., 2018)</li> <li>PPO - Policy optimization (Schulman et al., 2017)</li> <li>DINOv2 - Semantic features (Oquab et al., 2023)</li> <li>GAE - Advantage estimation (Schulman et al., 2015)</li> </ol>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>:material-clock-fast:{ .lg .middle } Quick Start</p> <p>Get up and running in minutes</p> <p>:octicons-arrow-right-24: Installation</p> </li> <li> <p>:fontawesome-solid-brain:{ .lg .middle } RL Navigation</p> <p>Deep dive into curiosity-driven navigation</p> <p>:octicons-arrow-right-24: Architecture</p> </li> <li> <p>:material-file-document:{ .lg .middle } Research Papers</p> <p>Understand the theoretical foundations</p> <p>:octicons-arrow-right-24: References</p> </li> <li> <p>:material-api:{ .lg .middle } API Reference</p> <p>Detailed API documentation</p> <p>:octicons-arrow-right-24: API</p> </li> </ul>"},{"location":"development/todo/","title":"Project TODO List","text":"<p>This page tracks the development status of Visual Next Token, including completed features, ongoing work, and planned improvements.</p> <p>Last Updated: 2025-11-02</p>"},{"location":"development/todo/#completed","title":"\u2705 Completed","text":""},{"location":"development/todo/#core-rl-navigation-implementation","title":"Core RL Navigation Implementation","text":"<ul> <li>[x] Semantic encoder wrapper (DINOv2) with freeze/unfreeze capabilities</li> <li>[x] Image navigation environment (MDP formulation)</li> <li>[x] Forward dynamics model (ICM) for intrinsic motivation</li> <li>[x] Random Network Distillation (RND) as alternative intrinsic motivation</li> <li>[x] PPO policy with actor-critic architecture</li> <li>[x] GAE (Generalized Advantage Estimation) for advantage computation</li> <li>[x] Two-phase training system (frozen \u2192 fine-tuned encoder)</li> <li>[x] Configuration management with multiple presets</li> <li>[x] Training script with checkpoint/resume support</li> <li>[x] Visualization script for learned paths</li> <li>[x] Extended action spaces (jump/scout actions)</li> </ul>"},{"location":"development/todo/#documentation","title":"Documentation","text":"<ul> <li>[x] MkDocs Material documentation site</li> <li>[x] Comprehensive README with quick start</li> <li>[x] Installation guide</li> <li>[x] Quick start tutorial</li> <li>[x] RL navigation architecture documentation</li> <li>[x] Research paper summaries (ICM, RND, PPO, DINOv2, GAE)</li> <li>[x] References directory with detailed paper notes</li> <li>[x] GitHub Pages deployment</li> <li>[x] Conceptual framing clarified (rolling-window accuracy)</li> </ul>"},{"location":"development/todo/#repository-setup","title":"Repository Setup","text":"<ul> <li>[x] Repository renamed to <code>visual-next-token</code></li> <li>[x] Git repository initialized and pushed</li> <li>[x] GitHub Pages enabled</li> <li>[x] Documentation deployed</li> </ul>"},{"location":"development/todo/#in-progress","title":"\ud83d\udea7 In Progress","text":""},{"location":"development/todo/#documentation-improvements","title":"Documentation Improvements","text":"<ul> <li>[ ] Complete architecture deep-dive page (techniques/rl_navigation/architecture.md)</li> <li>[ ] Training guide with best practices (techniques/rl_navigation/training.md)</li> <li>[ ] Extensions guide for jump/scout actions (techniques/rl_navigation/extensions.md)</li> <li>[ ] API reference documentation (api/rl-navigation.md)</li> <li>[ ] Contributing guide (development/contributing.md)</li> </ul>"},{"location":"development/todo/#code-quality","title":"Code Quality","text":"<ul> <li>[ ] Add type hints throughout codebase</li> <li>[ ] Add docstrings to all public methods</li> <li>[ ] Set up pytest test suite</li> <li>[ ] Add unit tests for core components</li> <li>[ ] Add integration tests for training pipeline</li> </ul>"},{"location":"development/todo/#high-priority-todo","title":"\ud83d\udccb High Priority TODO","text":""},{"location":"development/todo/#critical-features","title":"Critical Features","text":""},{"location":"development/todo/#1-reward-mechanism-alignment","title":"1. Reward Mechanism Alignment","text":"<p>Status: Documentation updated, code needs review Priority: High Description: Ensure the reward computation correctly implements rolling-window prediction accuracy, not just prediction error.</p> <p>Files to review: - <code>techniques/rl_navigation/environment.py</code> (lines 150-250) - <code>techniques/rl_navigation/forward_dynamics.py</code></p> <p>Current implementation: <pre><code># environment.py:_compute_lookahead_reward\ndef _compute_lookahead_reward(self, current_pos):\n    total_reward = 0.0\n    for distance in range(1, self.reward_horizon):\n        weight = np.exp(-self.reward_lambda * distance)\n        # Currently: prediction error\n        # Should verify: rolling-window accuracy\n</code></pre></p> <p>Action items: - [ ] Verify lookahead reward implements rolling-window accuracy - [ ] Consider inverting sign if currently using error (make it accuracy-based) - [ ] Add comprehensive comments explaining the rolling-window framing - [ ] Update variable names to reflect accuracy vs error</p>"},{"location":"development/todo/#2-testing-infrastructure","title":"2. Testing Infrastructure","text":"<p>Status: Not started Priority: High Description: Add comprehensive test suite before making significant changes.</p> <p>Action items: - [ ] Set up pytest configuration - [ ] Add tests for encoder (freeze/unfreeze, feature extraction) - [ ] Add tests for environment (state transitions, reward computation) - [ ] Add tests for policy (action sampling, evaluation) - [ ] Add tests for forward dynamics (prediction, intrinsic reward) - [ ] Add integration test for full training loop (quick config) - [ ] Set up CI/CD with GitHub Actions</p>"},{"location":"development/todo/#3-experiment-validation","title":"3. Experiment Validation","text":"<p>Status: Not started Priority: High Description: Run experiments to validate that the system works as intended.</p> <p>Action items: - [ ] Run quick_test config on synthetic image - [ ] Verify agent explores semantic regions (not random walk) - [ ] Measure coverage and path statistics - [ ] Compare RND vs ICM on same image - [ ] Test on real-world images (ImageNet samples) - [ ] Document experimental results</p>"},{"location":"development/todo/#medium-priority-todo","title":"\ud83d\udcca Medium Priority TODO","text":""},{"location":"development/todo/#feature-enhancements","title":"Feature Enhancements","text":""},{"location":"development/todo/#1-advanced-visualization","title":"1. Advanced Visualization","text":"<p>Files: <code>experiments/visualize_rl_paths.py</code></p> <ul> <li>[ ] Add heatmap of visited regions</li> <li>[ ] Show prediction error/accuracy over trajectory</li> <li>[ ] Visualize semantic feature space (t-SNE/UMAP)</li> <li>[ ] Animate path exploration over time</li> <li>[ ] Compare learned paths to baselines (random, greedy)</li> </ul>"},{"location":"development/todo/#2-training-improvements","title":"2. Training Improvements","text":"<p>Files: <code>techniques/rl_navigation/trainer.py</code></p> <ul> <li>[ ] Add TensorBoard logging</li> <li>[ ] Implement early stopping based on coverage</li> <li>[ ] Add learning rate scheduling</li> <li>[ ] Support distributed training (multi-GPU)</li> <li>[ ] Add curriculum learning (start with simple images)</li> </ul>"},{"location":"development/todo/#3-environment-extensions","title":"3. Environment Extensions","text":"<p>Files: <code>techniques/rl_navigation/environment.py</code>, <code>techniques/rl_navigation/extensions.py</code></p> <ul> <li>[ ] Support multi-image training (generalization)</li> <li>[ ] Add image augmentation during training</li> <li>[ ] Implement hierarchical navigation (coarse \u2192 fine)</li> <li>[ ] Add semantic segmentation integration (ground truth comparison)</li> <li>[ ] Support different patch sizes (adaptive resolution)</li> </ul>"},{"location":"development/todo/#4-documentation-pages","title":"4. Documentation Pages","text":"<p>Directory: <code>docs/</code></p> <ul> <li>[ ] Create architecture deep-dive (with diagrams)</li> <li>[ ] Write training guide with hyperparameter tuning tips</li> <li>[ ] Document extensions (jump/scout) with usage examples</li> <li>[ ] Add API reference (auto-generated from docstrings)</li> <li>[ ] Create troubleshooting guide</li> <li>[ ] Add gallery of learned paths on different images</li> </ul>"},{"location":"development/todo/#research-experiments-todo","title":"\ud83d\udd2c Research &amp; Experiments TODO","text":""},{"location":"development/todo/#novel-research-directions","title":"Novel Research Directions","text":""},{"location":"development/todo/#1-multi-image-generalization","title":"1. Multi-Image Generalization","text":"<p>Description: Train on multiple images, test generalization to unseen images.</p> <ul> <li>[ ] Implement multi-image dataset loader</li> <li>[ ] Modify trainer to sample images per episode</li> <li>[ ] Test on held-out validation images</li> <li>[ ] Compare to single-image overfitting</li> </ul>"},{"location":"development/todo/#2-semantic-segmentation-integration","title":"2. Semantic Segmentation Integration","text":"<p>Description: Use segmentation masks to evaluate semantic exploration quality.</p> <ul> <li>[ ] Integrate segmentation model (e.g., SAM)</li> <li>[ ] Measure coverage of different semantic classes</li> <li>[ ] Evaluate if agent prioritizes rare objects</li> <li>[ ] Compare to coverage-based baselines</li> </ul>"},{"location":"development/todo/#3-ablation-studies","title":"3. Ablation Studies","text":"<p>Description: Understand what components are critical.</p> <p>Experiments: - [ ] Frozen vs fine-tuned encoder (Phase 1 only vs Phase 2) - [ ] ICM vs RND intrinsic motivation - [ ] Different DINOv2 model sizes (vits14 vs vitb14 vs vitl14) - [ ] Coverage bonus weight (0.0 vs 0.1 vs 0.5) - [ ] Reward horizon (5 vs 10 vs 20 steps) - [ ] Policy architecture (hidden dim, layers)</p>"},{"location":"development/todo/#4-comparison-to-baselines","title":"4. Comparison to Baselines","text":"<p>Description: Establish that RL approach outperforms simpler alternatives.</p> <p>Baselines: - [ ] Random walk - [ ] Greedy (always move to highest-error neighbor) - [ ] Saliency-based navigation - [ ] Optical flow following - [ ] Edge-following heuristic</p>"},{"location":"development/todo/#known-issues","title":"\ud83d\udc1b Known Issues","text":""},{"location":"development/todo/#critical-bugs","title":"Critical Bugs","text":"<p>None currently identified</p>"},{"location":"development/todo/#minor-issues","title":"Minor Issues","text":"<ol> <li>MkDocs warnings for missing pages</li> <li>Several linked pages in navigation don't exist yet</li> <li>Warnings about broken internal links</li> <li> <p>Fix: Create placeholder pages or remove from nav</p> </li> <li> <p>Extension tests not implemented</p> </li> <li><code>techniques/rl_navigation/extensions.py</code> has test code but needs PyTorch installed</li> <li> <p>Fix: Add to test suite with proper dependencies</p> </li> <li> <p>Documentation links inconsistency</p> </li> <li>Some use <code>rl-navigation</code> (kebab-case), some use <code>rl_navigation</code> (snake_case)</li> <li>Fix: Standardize on snake_case to match directory structure</li> </ol>"},{"location":"development/todo/#ideas-future-work","title":"\ud83d\udca1 Ideas &amp; Future Work","text":""},{"location":"development/todo/#long-term-research-directions","title":"Long-term Research Directions","text":""},{"location":"development/todo/#1-transfer-learning","title":"1. Transfer Learning","text":"<ul> <li>Pre-train navigator on large image dataset</li> <li>Fine-tune on specific domains (medical, satellite)</li> <li>Investigate what navigation patterns transfer</li> </ul>"},{"location":"development/todo/#2-multi-agent-exploration","title":"2. Multi-Agent Exploration","text":"<ul> <li>Multiple agents exploring same image</li> <li>Communication between agents</li> <li>Collaborative coverage optimization</li> </ul>"},{"location":"development/todo/#3-active-vision","title":"3. Active Vision","text":"<ul> <li>Agent controls camera in 3D environment</li> <li>Navigate real-world scenes (not just images)</li> <li>Integration with robotics simulators</li> </ul>"},{"location":"development/todo/#4-hierarchical-semantic-navigation","title":"4. Hierarchical Semantic Navigation","text":"<ul> <li>Learn high-level semantic goals (\"find animals\")</li> <li>Decompose into low-level navigation primitives</li> <li>Options framework for temporally extended actions</li> </ul>"},{"location":"development/todo/#5-self-supervised-pre-training","title":"5. Self-Supervised Pre-training","text":"<ul> <li>Use learned navigation to generate training data</li> <li>Discover semantic segmentation from exploration</li> <li>Bootstrap better feature representations</li> </ul>"},{"location":"development/todo/#documentation-todo","title":"\ud83d\udcdd Documentation TODO","text":""},{"location":"development/todo/#pages-to-create","title":"Pages to Create","text":"<ol> <li>techniques/rl_navigation/architecture.md</li> <li>System architecture diagram</li> <li>Component interaction details</li> <li>Data flow visualization</li> <li> <p>Mathematical formulations</p> </li> <li> <p>techniques/rl_navigation/training.md</p> </li> <li>Step-by-step training walkthrough</li> <li>Hyperparameter tuning guide</li> <li>Common issues and solutions</li> <li> <p>Performance optimization tips</p> </li> <li> <p>techniques/rl_navigation/extensions.md</p> </li> <li>Jump/scout action usage</li> <li>Hierarchical policy details</li> <li>Custom action space design</li> <li> <p>Integration examples</p> </li> <li> <p>api/rl-navigation.md</p> </li> <li>Auto-generated API reference</li> <li>Class documentation</li> <li>Method signatures</li> <li> <p>Usage examples</p> </li> <li> <p>development/contributing.md</p> </li> <li>Contribution guidelines</li> <li>Code style guide</li> <li>PR process</li> <li>Development setup</li> </ol>"},{"location":"development/todo/#documentation-improvements_1","title":"Documentation Improvements","text":"<ul> <li>[ ] Add more code examples throughout</li> <li>[ ] Create interactive Jupyter notebooks</li> <li>[ ] Add video demonstrations of learned paths</li> <li>[ ] Improve math rendering (LaTeX)</li> <li>[ ] Add bibliography/citations page</li> <li>[ ] Create FAQ section</li> </ul>"},{"location":"development/todo/#milestones","title":"\ud83c\udfaf Milestones","text":""},{"location":"development/todo/#milestone-1-core-validation-target-1-week","title":"Milestone 1: Core Validation (Target: 1 week)","text":"<ul> <li>[ ] Fix reward mechanism if needed</li> <li>[ ] Run basic experiments</li> <li>[ ] Validate semantic exploration behavior</li> <li>[ ] Document experimental results</li> </ul>"},{"location":"development/todo/#milestone-2-testing-quality-target-2-weeks","title":"Milestone 2: Testing &amp; Quality (Target: 2 weeks)","text":"<ul> <li>[ ] Complete test suite</li> <li>[ ] Add CI/CD</li> <li>[ ] Type hints throughout</li> <li>[ ] Documentation complete</li> </ul>"},{"location":"development/todo/#milestone-3-research-publication-target-1-2-months","title":"Milestone 3: Research Publication (Target: 1-2 months)","text":"<ul> <li>[ ] Run comprehensive experiments</li> <li>[ ] Compare to baselines</li> <li>[ ] Ablation studies</li> <li>[ ] Write paper draft</li> </ul>"},{"location":"development/todo/#questions-decisions-needed","title":"\ud83d\udcde Questions &amp; Decisions Needed","text":""},{"location":"development/todo/#open-questions","title":"Open Questions","text":"<ol> <li>Reward formulation: Is current implementation truly rolling-window accuracy, or does it need adjustment?</li> <li>Current: Exponentially-weighted sum of prediction errors</li> <li> <p>Target: Should we invert to accuracy? Add baseline comparison?</p> </li> <li> <p>Training stability: Are there any known stability issues with current setup?</p> </li> <li>Two-phase training seems sound</li> <li> <p>Need empirical validation</p> </li> <li> <p>Evaluation metrics: What metrics best capture \"semantic exploration quality\"?</p> </li> <li>Coverage alone insufficient</li> <li>Need semantic diversity metrics</li> <li> <p>Consider segmentation-based evaluation</p> </li> <li> <p>Baseline comparisons: Which baselines are most important?</p> </li> <li>Random walk (essential)</li> <li>Greedy prediction error (natural comparison)</li> <li>Saliency-based (vision literature)</li> </ol>"},{"location":"development/todo/#design-decisions","title":"Design Decisions","text":"<ol> <li>Should we support multi-image training by default?</li> <li>Pro: Better generalization</li> <li>Con: More complex, slower training</li> <li> <p>Decision: Add as optional mode, keep single-image as default</p> </li> <li> <p>Visualization: real-time or post-hoc only?</p> </li> <li>Real-time helps debugging but slows training</li> <li> <p>Decision: Post-hoc by default, optional real-time flag</p> </li> <li> <p>Testing: unit vs integration focus?</p> </li> <li>Both needed, but which first?</li> <li>Decision: Start with integration tests (full training), then unit tests</li> </ol>"},{"location":"development/todo/#changelog","title":"\ud83d\udd04 Changelog","text":""},{"location":"development/todo/#2025-11-02","title":"2025-11-02","text":"<ul> <li>\u2705 Created comprehensive TODO list</li> <li>\u2705 Identified reward mechanism review as high priority</li> <li>\u2705 Documented completed RL navigation implementation</li> <li>\u2705 Listed all pending documentation pages</li> <li>\u2705 Added research directions and ablation study plans</li> </ul>"},{"location":"development/todo/#how-to-update-this-todo","title":"How to Update This TODO","text":"<p>When you complete a task: 1. Move from <code>[ ]</code> to <code>[x]</code> 2. Update the date in Changelog 3. Add notes about implementation details if relevant</p> <p>When you add a new task: 1. Choose appropriate priority section 2. Add clear description and action items 3. Link to relevant files/docs 4. Update Changelog with addition</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>PyTorch 2.0+</li> <li>CUDA-capable GPU (recommended, but CPU works)</li> </ul>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/georgepearse/image-ssl.git\ncd image-ssl\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>The project requires:</p> <pre><code>torch&gt;=2.0.0\ntorchvision&gt;=0.15.0\nnumpy&gt;=1.24.0\nopencv-python&gt;=4.7.0\nmatplotlib&gt;=3.7.0\n</code></pre> <p>For RL navigation specifically:</p> <ul> <li>DINOv2: Loaded automatically via <code>torch.hub</code></li> <li>Gym: For RL environment interface (optional)</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Test basic imports\nfrom techniques.rl_navigation import (\n    RLTrainer,\n    SemanticEncoder,\n    NavigationPolicy,\n)\n\nprint(\"\u2713 Installation successful!\")\n</code></pre>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":"<p>Check if CUDA is available:</p> <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Device count: {torch.cuda.device_count()}\")\n</code></pre> <p>The code automatically selects GPU if available, otherwise falls back to CPU.</p>"},{"location":"getting-started/installation/#download-pre-trained-models","title":"Download Pre-trained Models","text":"<p>DINOv2 models are downloaded automatically on first use via <code>torch.hub</code>. Available models:</p> Model Parameters Feature Dim Download Size <code>dinov2_vits14</code> 21M 384 ~84 MB <code>dinov2_vitb14</code> 86M 768 ~330 MB <code>dinov2_vitl14</code> 300M 1024 ~1.1 GB <code>dinov2_vitg14</code> 1.1B 1536 ~4.2 GB <p>First run will download the selected model to <code>~/.cache/torch/hub/</code>.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first RL navigation experiment</li> <li>RL Navigation Architecture - Understand the system design</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get started with RL-based image navigation in 5 minutes!</p>"},{"location":"getting-started/quickstart/#1-train-on-test-image","title":"1. Train on Test Image","text":"<p>The fastest way to see the system in action:</p> <pre><code>python experiments/train_rl_navigator.py --config quick_test\n</code></pre> <p>This uses: - Synthetic test image (colored shapes on gradient) - 100 episodes Phase 1 + 50 episodes Phase 2 - Small model (<code>dinov2_vits14</code>) - Should complete in ~10 minutes on GPU</p>"},{"location":"getting-started/quickstart/#2-train-on-your-own-image","title":"2. Train on Your Own Image","text":"<pre><code>python experiments/train_rl_navigator.py \\\n    --image path/to/your/image.jpg \\\n    --config quick_test\n</code></pre> <p>The image will be automatically resized to 512\u00d7512 if larger.</p>"},{"location":"getting-started/quickstart/#3-visualize-learned-paths","title":"3. Visualize Learned Paths","text":"<p>After training completes:</p> <pre><code>python experiments/visualize_rl_paths.py \\\n    --checkpoint checkpoints/rl_navigator/final.pt \\\n    --n_episodes 5\n</code></pre> <p>This will: - Load the trained policy - Run 5 episodes on the test image - Save visualizations to <code>experiments/outputs/rl_paths/</code> - Display a comparison of the learned paths</p>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"getting-started/quickstart/#training-logs","title":"Training Logs","text":"<pre><code>Phase 1 - Episode 10/100\n  Avg Reward: 12.34\n  Avg Length: 234.5\n  Coverage: 45.23%\n  Pred Error: 0.0234\n  Policy Loss: 0.1234\n  Value Loss: 0.0456\n  Entropy: 1.234\n</code></pre> Metric Meaning Avg Reward Intrinsic reward (prediction error + coverage bonus) Avg Length Steps per episode (max 500) Coverage % of image visited Pred Error Forward model prediction error Policy Loss PPO policy loss Value Loss Value function MSE Entropy Policy entropy (exploration)"},{"location":"getting-started/quickstart/#visualizations","title":"Visualizations","text":"<p>The visualization script creates:</p> <ul> <li><code>rl_path_ep1.png</code> - Individual episode paths</li> <li><code>comparison.png</code> - Side-by-side comparison of first 4 episodes</li> </ul> <p>Paths show: - Green circle: Start position - Red circle: End position - Colored line: Path taken</p>"},{"location":"getting-started/quickstart/#configuration-presets","title":"Configuration Presets","text":"Config Phase 1 Phase 2 Use Case <code>quick_test</code> 100 50 Testing/debugging <code>default</code> 10000 5000 Standard training <code>rnd</code> 10000 5000 Use RND instead of ICM <code>long</code> 20000 10000 Extended training with larger model"},{"location":"getting-started/quickstart/#using-different-configurations","title":"Using Different Configurations","text":""},{"location":"getting-started/quickstart/#rnd-random-network-distillation","title":"RND (Random Network Distillation)","text":"<pre><code>python experiments/train_rl_navigator.py --config rnd\n</code></pre> <p>RND is an alternative to forward dynamics: - More stable training - No action conditioning needed - Good for complex state spaces</p>"},{"location":"getting-started/quickstart/#long-training-with-larger-model","title":"Long Training with Larger Model","text":"<pre><code>python experiments/train_rl_navigator.py --config long\n</code></pre> <p>Uses: - <code>dinov2_vitb14</code> (768-dim features vs 384-dim) - 30k total episodes (20k + 10k) - Better final performance, but slower</p>"},{"location":"getting-started/quickstart/#custom-configuration","title":"Custom Configuration","text":"<p>For full control, use Python API:</p> <pre><code>from techniques.rl_navigation import RLTrainer\nimport cv2\n\n# Load your image\nimage = cv2.imread(\"my_image.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Create trainer with custom config\ntrainer = RLTrainer(\n    image=image,\n    encoder_name=\"dinov2_vits14\",\n    phase1_episodes=5000,\n    phase2_episodes=2000,\n    policy_lr=1e-4,\n    predictor_lr=5e-4,\n    use_rnd=False,\n    reward_lambda=0.1,\n    coverage_bonus_weight=0.05,\n    save_dir=\"my_checkpoints\",\n)\n\n# Train\ntrainer.train()\n</code></pre>"},{"location":"getting-started/quickstart/#resuming-from-checkpoint","title":"Resuming from Checkpoint","text":"<pre><code>python experiments/train_rl_navigator.py \\\n    --resume checkpoints/rl_navigator/phase1_ep1000.pt\n</code></pre>"},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quickstart/#out-of-memory","title":"Out of Memory","text":"<p>If you get CUDA OOM errors:</p> <ol> <li>Use smaller model: <code>dinov2_vits14</code> instead of <code>vitb14</code></li> <li>Reduce rollout steps in config</li> <li>Use CPU: <code>--device cpu</code> (slower)</li> </ol>"},{"location":"getting-started/quickstart/#slow-training","title":"Slow Training","text":"<ul> <li>Ensure GPU is being used: check <code>Device: cuda</code> in output</li> <li>Use <code>quick_test</code> config first</li> <li>Consider reducing image size in training script</li> </ul>"},{"location":"getting-started/quickstart/#low-coverage","title":"Low Coverage","text":"<p>If the agent doesn't explore much:</p> <ul> <li>Increase <code>coverage_bonus_weight</code> (default 0.1)</li> <li>Check that prediction error is non-zero</li> <li>Verify entropy is &gt; 0 (policy is exploring)</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Deep Dive - Understand how it works</li> <li>Training Guide - Advanced training strategies</li> <li>Extensions - Jump/scout actions</li> </ul>"},{"location":"references/","title":"Research References","text":"<p>This section contains detailed summaries of the key papers that form the theoretical foundation of our RL-based image navigation implementation.</p>"},{"location":"references/#overview","title":"Overview","text":"<p>Our approach synthesizes ideas from five major research areas:</p> <ol> <li>Curiosity-Driven RL - Using prediction error as intrinsic motivation</li> <li>Semantic Visual Features - Self-supervised learning for robust representations</li> <li>Policy Optimization - Stable, sample-efficient RL algorithms</li> <li>Advantage Estimation - Variance reduction in policy gradients</li> <li>Hierarchical RL - Extended action spaces for long-range planning</li> </ol>"},{"location":"references/#key-papers","title":"Key Papers","text":""},{"location":"references/#rl-navigation-foundation","title":"RL Navigation Foundation","text":"<p>These five papers directly inform our implementation:</p> Paper Year Contribution Our Implementation ICM 2017 Curiosity via prediction error <code>ForwardDynamicsModel</code> RND 2018 Random network distillation <code>RNDIntrinsicMotivation</code> PPO 2017 Clipped policy optimization <code>PPOTrainer</code> DINOv2 2023 Semantic visual features <code>SemanticEncoder</code> GAE 2015 Advantage estimation <code>compute_gae()</code>"},{"location":"references/#quick-reference","title":"Quick Reference","text":""},{"location":"references/#intrinsic-motivation","title":"Intrinsic Motivation","text":"<p>When to use ICM vs RND?</p> <ul> <li>ICM (Forward Dynamics):</li> <li>Action-relevant exploration</li> <li>Learn what actions lead where</li> <li>More sample efficient</li> <li> <p>Can be less stable</p> </li> <li> <p>RND (Random Network Distillation):</p> </li> <li>State-space novelty</li> <li>More stable training</li> <li>Simpler implementation</li> <li>Less action-focused</li> </ul>"},{"location":"references/#feature-learning","title":"Feature Learning","text":"<p>Why DINOv2 over other encoders?</p> <p>DINOv2 provides: - \u2705 Pre-trained on diverse data (142M images) - \u2705 Semantic invariance (car color problem) - \u2705 Patch-level features (natural for navigation) - \u2705 Multiple model sizes (21M - 1.1B params) - \u2705 Strong zero-shot transfer</p> <p>Alternatives considered: - CLIP: Language-biased, less dense features - ResNet: Lower-level features, less semantic - MAE: Reconstruction-focused, not semantic</p>"},{"location":"references/#policy-optimization","title":"Policy Optimization","text":"<p>Why PPO over other methods?</p> <p>PPO balances: - \u2705 Sample efficiency (vs A3C, REINFORCE) - \u2705 Stability (vs TRPO complexity) - \u2705 Simplicity (vs SAC, TD3) - \u2705 Works with discrete actions</p> <p>For image navigation: - Episodes can be 500+ steps - Need stable learning (don't want collapse) - Discrete action space (8 directions) - PPO is battle-tested choice</p>"},{"location":"references/#how-papers-relate","title":"How Papers Relate","text":"<pre><code>Image \u2192 DINOv2 \u2192 Features\n         [#4]        \u2502\n                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                      \u2502\n         \u25bc                      \u25bc\n    Policy (\u03c0)           Forward Model (P)\n    PPO [#3]             ICM/RND [#1,#2]\n         \u2502                      \u2502\n         \u2502                      \u25bc\n         \u2502              Prediction Error\n         \u2502              (Intrinsic Reward)\n         \u2502                      \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u25bc\n              PPO Update\n              with GAE [#5]\n</code></pre>"},{"location":"references/#the-critical-insights","title":"The Critical Insights","text":"<p>1. Flip the Reward (ICM)</p> <p>Don't predict accurately \u2192 predict poorly!</p> <p>High prediction error = novel/surprising = informative</p> <p>2. Semantic Space (DINOv2)</p> <p>Predict in feature space, not pixel space</p> <p>Red car \u2248 Blue car in embeddings, \u2260 in pixels</p> <p>3. Stable Updates (PPO)</p> <p>Clip policy updates to prevent catastrophic collapse</p> <pre><code>ratio = \u03c0_new / \u03c0_old\nclipped_ratio = clip(ratio, 1-\u03b5, 1+\u03b5)\nloss = -min(ratio * A, clipped_ratio * A)\n</code></pre> <p>4. Two-Phase Training (Novel)</p> <p>Phase 1: Frozen encoder \u2192 stable learning Phase 2: Fine-tune encoder \u2192 task adaptation</p> <p>5. Gradient Decoupling (Novel)</p> <p>Policy uses <code>features.detach()</code> \u2192 no policy gradients into encoder</p> <p>Encoder only updated via predictor loss</p>"},{"location":"references/#additional-reading","title":"Additional Reading","text":""},{"location":"references/#hierarchical-rl","title":"Hierarchical RL","text":"<p>For jump/scout actions extension:</p> <ul> <li>Sutton et al., \"Between MDPs and semi-MDPs\" (1999)</li> <li>Kulkarni et al., \"Hierarchical Deep RL\" (2016)</li> <li>Bacon et al., \"The Option-Critic Architecture\" (2017)</li> </ul>"},{"location":"references/#exploration-in-rl","title":"Exploration in RL","text":"<p>Related intrinsic motivation methods:</p> <ul> <li>Schmidhuber, \"Formal Theory of Creativity\" (2010)</li> <li>Stadie et al., \"Incentivizing Exploration\" (2015)</li> <li>Badia et al., \"Never Give Up\" (2020)</li> </ul>"},{"location":"references/#self-supervised-vision","title":"Self-Supervised Vision","text":"<p>Other feature learning approaches:</p> <ul> <li>Chen et al., \"SimCLR\" (2020)</li> <li>He et al., \"Masked Autoencoders\" (2021)</li> <li>Caron et al., \"DINOv1\" (2021)</li> </ul>"},{"location":"references/#citation-guide","title":"Citation Guide","text":"<p>When citing our work or building upon it, please cite the relevant foundation papers.</p> <p>Minimal citation (just ICM + DINOv2): <pre><code>@inproceedings{pathak2017curiosity,\n  title={Curiosity-driven exploration by self-supervised prediction},\n  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},\n  booktitle={ICML},\n  year={2017}\n}\n\n@article{oquab2023dinov2,\n  title={DINOv2: Learning Robust Visual Features without Supervision},\n  author={Oquab, Maxime and others},\n  journal={arXiv preprint arXiv:2304.07193},\n  year={2023}\n}\n</code></pre></p> <p>Complete citation (all five papers):</p> <p>See individual paper pages for full BibTeX.</p>"},{"location":"references/#next-steps","title":"Next Steps","text":"<ul> <li> <p>:material-file-document:{ .lg .middle } Read the Papers</p> <p>Detailed summaries with code connections</p> <p>:octicons-arrow-right-24: RL Navigation Papers</p> </li> <li> <p>:fontawesome-solid-brain:{ .lg .middle } Understand the Architecture</p> <p>How papers combine into working system</p> <p>:octicons-arrow-right-24: Architecture</p> </li> <li> <p>:material-code-braces:{ .lg .middle } See the Code</p> <p>Implementation details and API</p> <p>:octicons-arrow-right-24: API Reference</p> </li> </ul>"},{"location":"references/rl_navigation/","title":"RL Navigation: Key Papers and References","text":"<p>This directory contains detailed summaries of the foundational papers used in our RL-based image navigation implementation.</p>"},{"location":"references/rl_navigation/#overview","title":"Overview","text":"<p>Our approach combines several key techniques from recent RL and vision research:</p> <ol> <li>Curiosity-Driven Exploration - Intrinsic motivation via prediction error</li> <li>Semantic Feature Learning - DINOv2 for appearance-invariant representations</li> <li>Policy Optimization - PPO for stable, sample-efficient learning</li> <li>Advantage Estimation - GAE for variance reduction</li> </ol>"},{"location":"references/rl_navigation/#papers","title":"Papers","text":""},{"location":"references/rl_navigation/#1-curiosity-driven-exploration-by-self-supervised-prediction-icm","title":"1. Curiosity-driven Exploration by Self-supervised Prediction (ICM)","text":"<p>Pathak et al., ICML 2017</p> <ul> <li>Core Idea: Use prediction error as intrinsic reward signal</li> <li>Key Innovation: Learn features via inverse dynamics to focus on agent-relevant aspects</li> <li>In Our Code: <code>ForwardDynamicsModel</code> class (forward_dynamics.py)</li> </ul> <p>Why it matters: Solves the exploration problem - agent seeks information-dense regions by maximizing prediction error in feature space.</p>"},{"location":"references/rl_navigation/#2-exploration-by-random-network-distillation-rnd","title":"2. Exploration by Random Network Distillation (RND)","text":"<p>Burda et al., 2018</p> <ul> <li>Core Idea: Predict fixed random network outputs for intrinsic motivation</li> <li>Key Innovation: Simpler than ICM, no action conditioning needed</li> <li>In Our Code: <code>RNDIntrinsicMotivation</code> class (forward_dynamics.py)</li> </ul> <p>Why it matters: Alternative to ICM with more stable training. Useful when action-conditioned prediction is difficult.</p>"},{"location":"references/rl_navigation/#3-proximal-policy-optimization-ppo","title":"3. Proximal Policy Optimization (PPO)","text":"<p>Schulman et al., 2017</p> <ul> <li>Core Idea: Clip policy updates to prevent catastrophic collapse</li> <li>Key Innovation: Trust region benefits with first-order optimization</li> <li>In Our Code: <code>PPOTrainer</code> class (policy.py)</li> </ul> <p>Why it matters: The workhorse policy learning algorithm. Enables multiple epochs of minibatch updates on collected experience.</p>"},{"location":"references/rl_navigation/#4-dinov2-learning-robust-visual-features-without-supervision","title":"4. DINOv2: Learning Robust Visual Features without Supervision","text":"<p>Oquab et al., 2023</p> <ul> <li>Core Idea: Self-supervised vision transformer trained on 142M curated images</li> <li>Key Innovation: Features work across domains without fine-tuning</li> <li>In Our Code: <code>SemanticEncoder</code> class (encoder.py)</li> </ul> <p>Why it matters: Solves the \"car color problem\" - provides semantic features invariant to low-level appearance variations. Red car and blue car \u2192 same embedding.</p>"},{"location":"references/rl_navigation/#5-generalized-advantage-estimation-gae","title":"5. Generalized Advantage Estimation (GAE)","text":"<p>Schulman et al., 2015</p> <ul> <li>Core Idea: Balance bias-variance tradeoff in advantage estimation</li> <li>Key Innovation: Exponentially-weighted n-step advantages (analogous to TD(\u03bb))</li> <li>In Our Code: <code>PPOTrainer.compute_gae()</code> (policy.py:190-223)</li> </ul> <p>Why it matters: Reduces variance in policy gradients while controlling bias. Critical for stable learning in our long-horizon image navigation task.</p>"},{"location":"references/rl_navigation/#how-they-fit-together","title":"How They Fit Together","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Image (RGB)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502  DINOv2 Encoder (E)  \u2502 \u25c4\u2500\u2500 Paper #4\n          \u2502  Semantic Features   \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502                   \u2502\n           \u25bc                   \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Policy (\u03c0)  \u2502   \u2502 Forward Dynamics \u2502 \u25c4\u2500\u2500 Paper #1 or #2\n    \u2502   PPO       \u2502   \u2502  (P) or RND      \u2502     (ICM or RND)\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                   \u2502\n           \u2502                   \u25bc\n           \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502          \u2502 Prediction Error\u2502\n           \u2502          \u2502 (Intrinsic      \u2502\n           \u2502          \u2502  Reward)        \u2502\n           \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                   \u2502\n           \u25bc                   \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   PPO Update with GAE          \u2502 \u25c4\u2500\u2500 Papers #3 &amp; #5\n    \u2502   - Clipped objective          \u2502\n    \u2502   - Advantage estimation       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"references/rl_navigation/#implementation-architecture","title":"Implementation Architecture","text":""},{"location":"references/rl_navigation/#phase-1-frozen-encoder-10k-episodes","title":"Phase 1: Frozen Encoder (10k episodes)","text":"<ul> <li>DINOv2 features frozen (pre-trained)</li> <li>Policy learns navigation with stable semantic space</li> <li>Forward model/RND learns to predict transitions</li> </ul>"},{"location":"references/rl_navigation/#phase-2-fine-tuned-encoder-5k-episodes","title":"Phase 2: Fine-Tuned Encoder (5k episodes)","text":"<ul> <li>Unfreeze top 2 transformer layers</li> <li>Very small learning rate (1e-5)</li> <li>Encoder adapts to task-specific features</li> <li>Critical: Policy uses <code>z.detach()</code> to prevent gradient corruption</li> </ul>"},{"location":"references/rl_navigation/#key-design-decisions","title":"Key Design Decisions","text":"Decision Papers Rationale Prediction error as reward #1, #2 Encourages seeking information-dense regions Semantic features (not pixels) #4 Invariance to irrelevant variations (car color) Two-phase training #4 Stable features first, then task adaptation PPO policy learning #3 Sample efficiency + stability GAE advantages #5 Variance reduction for long episodes Exponential distance weighting Novel Multi-step lookahead planning"},{"location":"references/rl_navigation/#additional-reading","title":"Additional Reading","text":""},{"location":"references/rl_navigation/#hierarchical-rl-for-jump-actions-extension","title":"Hierarchical RL (for jump actions extension)","text":"<ul> <li>Options Framework: Sutton et al., \"Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning\" (1999)</li> <li>Hierarchical DQN: Kulkarni et al., \"Hierarchical Deep Reinforcement Learning\" (2016)</li> </ul>"},{"location":"references/rl_navigation/#intrinsic-motivation","title":"Intrinsic Motivation","text":"<ul> <li>Empowerment: Mohamed &amp; Rezende, \"Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning\" (2015)</li> <li>NGU: Badia et al., \"Never Give Up: Learning Directed Exploration Strategies\" (2020)</li> </ul>"},{"location":"references/rl_navigation/#vision-transformers","title":"Vision Transformers","text":"<ul> <li>ViT: Dosovitskiy et al., \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (2020)</li> <li>DINO (v1): Caron et al., \"Emerging Properties in Self-Supervised Vision Transformers\" (2021)</li> </ul>"},{"location":"references/rl_navigation/#citation","title":"Citation","text":"<p>If you use this implementation or build upon these ideas, please cite the relevant papers:</p> <pre><code>@inproceedings{pathak2017curiosity,\n  title={Curiosity-driven exploration by self-supervised prediction},\n  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},\n  booktitle={ICML},\n  year={2017}\n}\n\n@article{burda2018exploration,\n  title={Exploration by random network distillation},\n  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},\n  journal={arXiv preprint arXiv:1810.12894},\n  year={2018}\n}\n\n@article{schulman2017proximal,\n  title={Proximal policy optimization algorithms},\n  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},\n  journal={arXiv preprint arXiv:1707.06347},\n  year={2017}\n}\n\n@article{oquab2023dinov2,\n  title={DINOv2: Learning Robust Visual Features without Supervision},\n  author={Oquab, Maxime and others},\n  journal={arXiv preprint arXiv:2304.07193},\n  year={2023}\n}\n\n@article{schulman2015high,\n  title={High-dimensional continuous control using generalized advantage estimation},\n  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},\n  journal={arXiv preprint arXiv:1506.02438},\n  year={2015}\n}\n</code></pre>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/","title":"Curiosity-driven Exploration by Self-supervised Prediction (ICM)","text":""},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#paper-information","title":"Paper Information","text":"<p>arXiv ID: 1705.05363 Submitted: May 15, 2017 Venue: ICML 2017 Field: Computer Science - Machine Learning</p>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#authors","title":"Authors","text":"<ul> <li>Deepak Pathak</li> <li>Pulkit Agrawal</li> <li>Alexei A. Efros</li> <li>Trevor Darrell</li> </ul>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#abstract","title":"Abstract","text":"<p>The paper addresses exploration in environments with sparse or absent external rewards. The authors propose using curiosity as an intrinsic motivation signal, formulated through an agent's prediction error regarding action consequences in a learned visual feature space. Their approach leverages a self-supervised inverse dynamics model and scales effectively to high-dimensional state spaces like images while avoiding direct pixel prediction.</p>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Novel Curiosity Framework: Formulates intrinsic motivation as prediction error in a learned feature space, enabling exploration without external rewards.</p> </li> <li> <p>Scalability to Visual Domains: Successfully handles image-based environments by operating on learned representations rather than raw pixels.</p> </li> <li> <p>Environmental Relevance: The method focuses on agent-relevant environmental aspects, ignoring irrelevant visual changes.</p> </li> <li> <p>Comprehensive Evaluation: Demonstrates effectiveness across three scenarios:</p> </li> <li>Sparse reward environments requiring fewer interactions</li> <li>Unrewarded exploration showing efficient behavior</li> <li>Generalization to new game levels leveraging prior knowledge</li> </ol>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/1705.05363</li> <li>Website: pathak22.github.io/noreward-rl/</li> <li>PDF: Available on arXiv</li> <li>Code &amp; Demo: Available via project website</li> </ul>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>This paper forms the theoretical foundation for our ForwardDynamicsModel class. The ICM (Intrinsic Curiosity Module) approach:</p> <ul> <li> <p>Forward Model: Predicts next features from current features + action   <pre><code>predicted_features = forward_model(features_t, action)\nintrinsic_reward = ||predicted_features - features_t1||\u00b2\n</code></pre></p> </li> <li> <p>Feature Learning: Uses inverse dynamics to learn agent-relevant features (solves \"car color problem\")</p> </li> <li>Prediction Error as Reward: High prediction error = novel/surprising state = exploration bonus</li> </ul> <p>In our code (techniques/rl_navigation/forward_dynamics.py:122-133): <pre><code>class ForwardDynamicsModel:\n    def compute_intrinsic_reward(self, features_t, action, features_t1):\n        predicted_features = self.forward(features_t, action)\n        return torch.sum((predicted_features - features_t1) ** 2, dim=-1)\n</code></pre></p> <p>Our implementation differs by: 1. Using pre-trained DINOv2 features instead of learning features from scratch 2. Two-phase training: frozen encoder \u2192 fine-tuned encoder 3. Exponential distance weighting for multi-step lookahead rewards</p>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/","title":"Exploration by Random Network Distillation (RND)","text":""},{"location":"references/rl_navigation/02_random_network_distillation_RND/#paper-information","title":"Paper Information","text":"<p>ArXiv ID: 1810.12894 Submission Date: October 30, 2018 Field: Computer Science &gt; Machine Learning</p>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#authors","title":"Authors","text":"<ul> <li>Yuri Burda</li> <li>Harrison Edwards</li> <li>Amos Storkey</li> <li>Oleg Klimov</li> </ul>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#abstract","title":"Abstract","text":"<p>The paper introduces \"an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed.\" The mechanism leverages prediction error from a neural network trained on fixed random features as an intrinsic reward signal.</p>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Random Network Distillation (RND) Bonus: A novel exploration mechanism based on measuring how well a trainable predictor can estimate features from a fixed random network.</p> </li> <li> <p>Flexible Reward Combination: A method for adaptively merging intrinsic (exploration) and extrinsic (task) rewards during training.</p> </li> <li> <p>State-of-the-Art Results: Achieved breakthrough performance on Montezuma's Revenge, \"the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state,\" occasionally completing the first level.</p> </li> <li> <p>Hard Exploration Benchmarks: Demonstrated significant improvements across multiple challenging Atari games known for sparse reward signals.</p> </li> </ol>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/1810.12894</li> <li>DOI: https://doi.org/10.48550/arXiv.1810.12894</li> <li>Subject Categories: Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML)</li> </ul>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>RND provides an alternative intrinsic motivation mechanism to ICM. Our implementation includes both options (see techniques/rl_navigation/forward_dynamics.py:143-275):</p>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#rnd-architecture","title":"RND Architecture","text":"<pre><code>class RNDIntrinsicMotivation:\n    def __init__(self, feature_dim, hidden_dim=512):\n        # Fixed random target network (never updated)\n        self.target_net = self._build_network(feature_dim, hidden_dim)\n        for param in self.target_net.parameters():\n            param.requires_grad = False\n\n        # Trainable predictor network\n        self.predictor_net = self._build_network(feature_dim, hidden_dim)\n\n    def compute_intrinsic_reward(self, features):\n        with torch.no_grad():\n            target = self.target_net(features)\n        prediction = self.predictor_net(features)\n        return torch.sum((prediction - target) ** 2, dim=-1)\n</code></pre>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#rnd-vs-icm-comparison","title":"RND vs ICM Comparison","text":"Aspect RND ICM (Forward Dynamics) Prediction Target Fixed random features Next state features Requires Actions No Yes Novelty Signal Feature novelty Transition novelty Stability More stable (fixed target) Requires careful tuning Agent Relevance Less focused More action-relevant"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#usage-in-our-code","title":"Usage in Our Code","text":"<pre><code># Create RND-based trainer\ntrainer = RLTrainer(\n    image=image,\n    use_rnd=True,  # Use RND instead of forward dynamics\n    ...\n)\n\n# Or use config\nfrom techniques.rl_navigation.config import get_config\nconfig = get_config(\"rnd\")\n</code></pre> <p>RND is particularly useful when: - Action-conditioned prediction is difficult - You want more stable intrinsic rewards - Feature-space novelty is the primary exploration signal</p>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/","title":"Proximal Policy Optimization Algorithms (PPO)","text":""},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#paper-information","title":"Paper Information","text":"<p>arXiv ID: 1707.06347 Category: Computer Science &gt; Machine Learning (cs.LG) Submitted: July 20, 2017 Last Revised: August 28, 2017 (v2)</p>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#authors","title":"Authors","text":"<ul> <li>John Schulman</li> <li>Filip Wolski</li> <li>Prafulla Dhariwal</li> <li>Alec Radford</li> <li>Oleg Klimov</li> </ul>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#abstract","title":"Abstract","text":"<p>The authors introduce a novel approach to reinforcement learning through \"a new family of policy gradient methods\" that alternates between environment interaction and objective function optimization. Unlike conventional policy gradient techniques that perform single updates per sample, this work presents a method enabling \"multiple epochs of minibatch updates.\" The resulting algorithm, termed PPO, incorporates advantages from trust region methods while maintaining simplicity and improved empirical sample efficiency. Testing encompasses robotic simulation and Atari games, demonstrating superior performance relative to comparable online policy approaches.</p>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Novel Objective Function: Enables multiple minibatch optimization epochs per data collection phase, improving computational efficiency</p> </li> <li> <p>Simplified Implementation: Maintains benefits of trust region policy optimization (TRPO) while reducing implementation complexity</p> </li> <li> <p>Enhanced Performance: Empirical results show improved sample complexity and wall-time efficiency across benchmark tasks</p> </li> <li> <p>Versatile Application: Successfully applies to diverse domains including simulated robotics and video game playing</p> </li> </ol>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/1707.06347</li> <li>DOI: https://doi.org/10.48550/arXiv.1707.06347</li> <li>OpenAI Blog: https://openai.com/blog/openai-baselines-ppo/</li> </ul>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>PPO is the core policy learning algorithm in our navigation system. Implementation in techniques/rl_navigation/policy.py:148-341.</p>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#ppo-clipped-objective","title":"PPO Clipped Objective","text":"<p>The key innovation is the clipped surrogate objective that prevents too-large policy updates:</p> <pre><code>class PPOTrainer:\n    def update(self, rollout_buffer, n_epochs=4, batch_size=64):\n        # Compute probability ratio\n        ratio = torch.exp(log_probs - batch_old_log_probs)\n\n        # Clipped surrogate objective\n        surr1 = ratio * batch_advantages\n        surr2 = torch.clamp(\n            ratio,\n            1 - self.clip_epsilon,  # Typically 0.2\n            1 + self.clip_epsilon\n        ) * batch_advantages\n        policy_loss = -torch.min(surr1, surr2).mean()\n</code></pre>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#complete-ppo-update","title":"Complete PPO Update","text":"<p>Our implementation includes all PPO components:</p> <ol> <li>Policy Loss (clipped objective)</li> <li>Value Loss (MSE with returns)</li> <li>Entropy Bonus (exploration)</li> </ol> <pre><code># Total loss (policy.py:308-312)\nloss = (\n    policy_loss\n    + self.value_loss_coef * value_loss\n    + self.entropy_coef * entropy_loss\n)\n</code></pre>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#ppo-hyperparameters-configpy","title":"PPO Hyperparameters (config.py)","text":"<pre><code>@dataclass\nclass RLConfig:\n    gamma: float = 0.99           # Discount factor\n    gae_lambda: float = 0.95      # GAE lambda\n    clip_epsilon: float = 0.2     # PPO clipping\n\n    rollout_steps: int = 2048     # Steps per rollout\n    ppo_epochs: int = 4           # PPO update epochs\n    ppo_batch_size: int = 64\n    max_grad_norm: float = 0.5\n</code></pre>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#why-ppo-for-image-navigation","title":"Why PPO for Image Navigation?","text":"<p>PPO is ideal for our task because:</p> <ol> <li>Sample Efficiency: Reuses experience through multiple epochs</li> <li>Stability: Clipped objective prevents catastrophic policy collapse</li> <li>Simplicity: No complex second-order optimization (vs TRPO)</li> <li>Works with Continuous Features: Handles high-dimensional DINOv2 features (384-1024 dims)</li> </ol>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#training-loop-trainerpy295-343","title":"Training Loop (trainer.py:295-343)","text":"<pre><code>def train(self):\n    # Phase 1: Frozen encoder\n    for episode in range(self.phase1_episodes):\n        rollout = self.collect_rollout()  # Gather experiences\n        ppo_metrics = self.ppo_trainer.update(rollout)  # PPO update\n\n    # Phase 2: Fine-tuned encoder\n    self.encoder.unfreeze_top_layers(n_layers=2)\n    for episode in range(self.phase2_episodes):\n        ...\n</code></pre>"},{"location":"references/rl_navigation/04_dinov2_visual_features/","title":"DINOv2: Learning Robust Visual Features without Supervision","text":""},{"location":"references/rl_navigation/04_dinov2_visual_features/#paper-information","title":"Paper Information","text":"<p>arXiv ID: 2304.07193 Field: Computer Science &gt; Computer Vision and Pattern Recognition Submitted: April 14, 2023 (v1) Last Revised: February 2, 2024 (v2) DOI: https://doi.org/10.48550/arXiv.2304.07193</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#authors","title":"Authors","text":"<p>Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#abstract","title":"Abstract","text":"<p>The paper demonstrates that \"existing pretraining methods, especially self-supervised methods, can produce features that work across image distributions and tasks without finetuning if trained on enough curated data.\" The researchers combine established techniques to scale pretraining efforts and introduce methods for accelerating and stabilizing large-scale training.</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Dataset Pipeline: Created an automatic pipeline for building a diverse, curated image dataset rather than relying on unstructured data typically used in self-supervised learning</p> </li> <li> <p>Model Scale: Trained a Vision Transformer with one billion parameters and distilled it into smaller, more practical models</p> </li> <li> <p>Performance: Achieved results surpassing OpenCLIP across most benchmarks at both image and pixel levels</p> </li> <li> <p>Technical Optimizations: Implemented multiple techniques focused on improving training efficiency and stability at scale</p> </li> </ol>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/2304.07193</li> <li>Project Page: https://dinov2.metademolab.com/</li> <li>GitHub: https://github.com/facebookresearch/dinov2</li> <li>Models: Available via torch.hub</li> </ul>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>DINOv2 is the semantic encoder backbone that solves the critical \"car color problem\" - the need for features invariant to low-level variations. Implementation in techniques/rl_navigation/encoder.py.</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#the-car-color-problem","title":"The Car Color Problem","text":"<p>Problem: Pixel-level prediction penalizes irrelevant variations - Red car vs Blue car \u2192 huge pixel difference - Model shouldn't be penalized for unpredictable color</p> <p>Solution: Semantic feature space - Red car \u2192 <code>embedding_car</code> - Blue car \u2192 <code>embedding_car</code> - Prediction in semantic space, not pixel space</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#dinov2-in-our-code","title":"DINOv2 in Our Code","text":"<pre><code>class SemanticEncoder(nn.Module):\n    def __init__(self, model_name=\"dinov2_vits14\", freeze=True):\n        # Load pre-trained DINOv2\n        self.model = torch.hub.load(\n            \"facebookresearch/dinov2\",\n            model_name\n        )\n        self.feature_dim = self.model.embed_dim  # 384/768/1024\n\n        if freeze:\n            self.freeze()  # Phase 1: frozen features\n</code></pre>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#available-models","title":"Available Models","text":"Model Parameters Feature Dim Patch Size <code>dinov2_vits14</code> 21M 384 14\u00d714 <code>dinov2_vitb14</code> 86M 768 14\u00d714 <code>dinov2_vitl14</code> 300M 1024 14\u00d714 <code>dinov2_vitg14</code> 1.1B 1536 14\u00d714"},{"location":"references/rl_navigation/04_dinov2_visual_features/#two-phase-training-strategy","title":"Two-Phase Training Strategy","text":"<p>Our implementation uses a critical two-phase approach:</p> <p>Phase 1: Frozen Encoder (10k episodes) <pre><code>self.encoder.freeze()  # All parameters frozen\n# Stable semantic space\n# Policy and predictor learn with fixed features\n</code></pre></p> <p>Phase 2: Fine-tuned Encoder (5k episodes) <pre><code>self.encoder.unfreeze_top_layers(n_layers=2)  # Last 2 transformer blocks\n# encoder_lr = 1e-5  # Very small!\n# Task-specific feature adaptation\n</code></pre></p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#why-this-matters","title":"Why This Matters","text":"<ol> <li>Semantic Invariance: DINOv2 features capture object identity, not appearance</li> <li>Different colored cars \u2192 similar features</li> <li>Different car poses \u2192 similar features</li> <li> <p>Car vs road vs sky \u2192 different features</p> </li> <li> <p>Zero-Shot Transfer: Pre-trained on 142M images</p> </li> <li>Works on any image domain</li> <li> <p>No task-specific training needed for Phase 1</p> </li> <li> <p>Patch-Level Features: Natural grid structure for image navigation    <pre><code># Image: 224\u00d7224 pixels\n# DINOv2 patches: 16\u00d716 grid (patch_size=14)\n# Features: (16, 16, 384) for vits14\n</code></pre></p> </li> </ol>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#feature-extraction-encoderpy189-226","title":"Feature Extraction (encoder.py:189-226)","text":"<pre><code>def get_patch_features_at_position(self, image, position, patch_radius=1):\n    \"\"\"Get local patch features around a position.\"\"\"\n    # Convert pixel position \u2192 patch coordinates\n    patch_row = position[0] // self.patch_size\n    patch_col = position[1] // self.patch_size\n\n    # Extract local neighborhood\n    features = self.patch_features[\n        max(0, patch_row - patch_radius):patch_row + patch_radius + 1,\n        max(0, patch_col - patch_radius):patch_col + patch_radius + 1,\n    ]\n\n    # Average pool \u2192 single feature vector\n    return features.mean(dim=(0, 1))\n</code></pre>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#co-training-architecture","title":"Co-Training Architecture","text":"<pre><code>Image \u2192 DINOv2 \u2192 Features (z) \u2192 Forward Model (P) \u2192 Predicted Features (\u1e91)\n                     \u2193                                         \u2193\n                 Policy (\u03c0)                            Prediction Error\n                     \u2193                                         \u2193\n                  Action                              Intrinsic Reward\n</code></pre> <p>Critical: Policy uses <code>z.detach()</code> to prevent policy gradients from corrupting encoder features!</p> <pre><code># trainer.py:203\nwith torch.no_grad():\n    action, log_prob, value = self.policy.act(features.detach())\n</code></pre> <p>Encoder only updated via predictor gradients in Phase 2.</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation (GAE)","text":""},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#paper-information","title":"Paper Information","text":"<p>arXiv ID: 1506.02438 Submitted: June 8, 2015 (v1) Last Revised: October 20, 2018 (v6) Subject Areas: Machine Learning (cs.LG), Robotics (cs.RO), Systems and Control (eess.SY)</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#authors","title":"Authors","text":"<ul> <li>John Schulman</li> <li>Philipp Moritz</li> <li>Sergey Levine</li> <li>Michael Jordan</li> <li>Pieter Abbeel</li> </ul>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#abstract","title":"Abstract","text":"<p>The paper introduces an approach to policy gradient methods in reinforcement learning that addresses two key challenges: high sample complexity and training instability. The authors employ value functions to reduce variance in policy gradient estimates while introducing some bias through \"an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda).\" For stability, they use trust region optimization for both policy and value function neural networks.</p> <p>The method demonstrates strong performance on complex 3D locomotion tasks, including bipedal and quadrupedal robot locomotion, with policies learned directly from raw kinematics to joint torques\u2014requiring simulated experience equivalent to 1-2 weeks of real time for 3D bipeds.</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#key-contributions","title":"Key Contributions","text":"<ul> <li>Variance reduction technique: Leverages value functions with a generalized advantage estimation method to improve sample efficiency</li> <li>Stable optimization: Applies trust region methods to both policy and value functions</li> <li>End-to-end learning: Direct mapping from kinematics to control without hand-crafted representations</li> <li>Empirical validation: Successful learning of complex locomotion behaviors in high-dimensional continuous control domains</li> </ul>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/1506.02438</li> </ul>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>GAE provides the advantage estimation used in our PPO implementation to compute policy gradients with reduced variance. Implementation in techniques/rl_navigation/policy.py:190-223.</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#the-bias-variance-tradeoff","title":"The Bias-Variance Tradeoff","text":"<p>Problem: Policy gradient estimation - High variance \u2192 slow learning, unstable - High bias \u2192 incorrect gradients, poor convergence</p> <p>Solution: GAE with lambda parameter to balance bias-variance</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#gae-formula","title":"GAE Formula","text":"<p>The advantage function estimates \"how much better is action a than average at state s\":</p> <pre><code>A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\n</code></pre> <p>GAE uses an exponentially-weighted average of n-step advantages:</p> <pre><code>GAE(\u03bb) = (1-\u03bb) * [A^(1) + \u03bb*A^(2) + \u03bb\u00b2*A^(3) + ...]\n</code></pre> <p>where: <pre><code>A^(n) = r_t + \u03b3*r_{t+1} + ... + \u03b3^{n-1}*r_{t+n-1} + \u03b3^n*V(s_{t+n}) - V(s_t)\n</code></pre></p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#implementation-policypy190-223","title":"Implementation (policy.py:190-223)","text":"<pre><code>def compute_gae(self, rewards, values, dones):\n    \"\"\"\n    Compute Generalized Advantage Estimation (GAE).\n\n    Args:\n        rewards: Rewards (T,)\n        values: Value estimates (T+1,) - includes next value\n        dones: Done flags (T,)\n\n    Returns:\n        advantages: GAE advantages (T,)\n        returns: Discounted returns (T,)\n    \"\"\"\n    advantages = torch.zeros_like(rewards)\n    last_advantage = 0\n\n    # Compute advantages backwards in time\n    for t in reversed(range(len(rewards))):\n        if t == len(rewards) - 1:\n            next_value = values[t + 1]\n        else:\n            next_value = values[t + 1]\n\n        # TD error: \u03b4_t = r_t + \u03b3*V(s_{t+1}) - V(s_t)\n        delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n\n        # GAE: A_t = \u03b4_t + \u03b3*\u03bb*A_{t+1}\n        advantages[t] = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_advantage\n        last_advantage = advantages[t]\n\n    # Returns = advantages + values\n    returns = advantages + values[:-1]\n\n    return advantages, returns\n</code></pre>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#gae-hyperparameter-lambda","title":"GAE Hyperparameter: Lambda (\u03bb)","text":"<p>Controls the bias-variance tradeoff:</p> \u03bb value Behavior Variance Bias \u03bb = 0 1-step TD (bootstrapping) Low High \u03bb = 1 Monte Carlo (full returns) High Low \u03bb = 0.95 Balanced (our default) Medium Medium <p>In our config (config.py:26): <pre><code>gae_lambda: float = 0.95  # GAE lambda\n</code></pre></p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#why-gae-for-image-navigation","title":"Why GAE for Image Navigation?","text":"<ol> <li>Long Episodes: Our episodes can be 500+ steps</li> <li>Monte Carlo (\u03bb=1) would have huge variance</li> <li> <p>Pure TD (\u03bb=0) would have high bias</p> </li> <li> <p>Sparse Rewards: Intrinsic rewards are noisy</p> </li> <li>GAE smooths reward signal</li> <li> <p>Reduces impact of prediction noise</p> </li> <li> <p>Credit Assignment: Multi-step lookahead rewards</p> </li> <li>GAE helps assign credit across time</li> <li>Important for exponentially-weighted future prediction</li> </ol>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#integration-with-ppo-policypy251-256","title":"Integration with PPO (policy.py:251-256)","text":"<pre><code># Compute advantages with GAE\nadvantages, returns = self.compute_gae(rewards, values, dones)\n\n# Normalize advantages (reduce variance)\nadvantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n# Use in PPO loss\npolicy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\nvalue_loss = F.mse_loss(values_pred, returns)\n</code></pre>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#empirical-results","title":"Empirical Results","text":"<p>The paper showed GAE significantly improves: - Sample efficiency (fewer episodes to convergence) - Training stability (smoother learning curves) - Final performance (higher rewards)</p> <p>These benefits directly apply to our image navigation task, where we need stable learning with limited interaction budget (15k episodes total).</p>"},{"location":"techniques/rl_navigation/","title":"RL-Based Image Navigation","text":""},{"location":"techniques/rl_navigation/#overview","title":"Overview","text":"<p>RL-based image navigation learns semantic paths through images by maximizing prediction accuracy over a rolling window of future semantic features. An agent learns to navigate by finding paths where it can build up predictive understanding of semantic regions.</p>"},{"location":"techniques/rl_navigation/#the-core-idea","title":"The Core Idea","text":""},{"location":"techniques/rl_navigation/#traditional-single-step-prediction","title":"Traditional Single-Step Prediction \u274c","text":"<p>Most approaches optimize immediate prediction accuracy:</p> <pre><code>Reward = -||predicted_pixels - actual_pixels||\u00b2\n</code></pre> <p>Problem: Agent seeks trivially predictable regions (uniform sky, blank walls)</p>"},{"location":"techniques/rl_navigation/#our-approach-rolling-window-accuracy","title":"Our Approach: Rolling Window Accuracy \u2705","text":"<p>We maximize prediction accuracy over multiple future steps:</p> <pre><code>Reward = \u03a3 exp(-\u03bb\u00b7d) \u00b7 accuracy_at_distance_d\n       d=1 to horizon\n</code></pre> <p>Result: Agent seeks semantically coherent paths!</p> <p>Example - hitting a car edge: 1. Step 0\u21921: Poor prediction (road \u2192 car color jump) 2. Steps 1\u219210: Excellent predictions (\"more car\" \u2192 \"more car\") 3. High rolling-window accuracy = semantic coherence = information-rich</p> <p>This is fundamentally different from staying in uniform regions (which have high single-step accuracy but no semantic structure).</p>"},{"location":"techniques/rl_navigation/#why-this-works","title":"Why This Works","text":"<p>The key insight: rolling-window prediction accuracy correlates with semantic coherence.</p> <p>Consider navigating from a car:</p> Next Region Initial Error Subsequent Accuracy Rolling Window Score More of same car Low High (redundant) Medium Road beneath High High (new object, then coherent) High Sky above High High (new context, then coherent) High Uniform wall Low High (but trivial) Low (no structure) <p>The agent learns to follow paths with semantic transitions that build predictive understanding.</p>"},{"location":"techniques/rl_navigation/#the-car-color-problem","title":"The \"Car Color Problem\"","text":"<p>A critical challenge: cars can be any color.</p>"},{"location":"techniques/rl_navigation/#problem","title":"Problem","text":"<p>If we predict in pixel space: - Red car \u2192 Blue car: HUGE prediction error - Model is penalized for unpredictable but irrelevant variation</p>"},{"location":"techniques/rl_navigation/#solution-semantic-features","title":"Solution: Semantic Features","text":"<p>Use DINOv2 to map pixels \u2192 semantic space:</p> <pre><code># Pixel space (\u274c)\nred_car = [255, 0, 0, ...]  # 3\u00d7224\u00d7224 = 150,528 dims\nblue_car = [0, 0, 255, ...] # Very different!\n\n# Semantic space (\u2705)\nencoder(red_car) = [0.12, -0.34, ...]  # 384 dims\nencoder(blue_car) = [0.11, -0.33, ...] # Nearly identical!\n</code></pre> <p>DINOv2 features are invariant to appearance, capturing object identity: - Different colored cars \u2192 similar embeddings - Car vs road \u2192 different embeddings</p>"},{"location":"techniques/rl_navigation/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    RGB Image                         \u2502\n\u2502                  (H \u00d7 W \u00d7 3)                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502  Semantic Encoder    \u2502\n          \u2502     (DINOv2)         \u2502\n          \u2502  Frozen \u2192 Fine-tuned \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502                   \u2502\n           \u2502    Features (z)   \u2502 \u25c4\u2500\u2500 Detach for policy!\n           \u2502                   \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                       \u2502\n         \u25bc                       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Policy    \u2502     \u2502 Forward Dynamics \u2502\n  \u2502    (\u03c0)      \u2502     \u2502      (P)         \u2502\n  \u2502  Actor-     \u2502     \u2502  Predict next    \u2502\n  \u2502  Critic     \u2502     \u2502  features from   \u2502\n  \u2502             \u2502     \u2502  current + action\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                     \u2502\n         \u2502 Action              \u2502 Predicted z'\n         \u25bc                     \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Environment Step                 \u2502\n  \u2502   - Execute action (move)          \u2502\n  \u2502   - Get actual next features (z')  \u2502\n  \u2502   - Compute prediction error:      \u2502\n  \u2502     reward = ||P(z,a) - z'||\u00b2     \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 PPO Update  \u2502\n    \u2502 - Policy    \u2502\n    \u2502 - Value fn  \u2502\n    \u2502 - Predictor \u2502\n    \u2502 - Encoder*  \u2502 * Phase 2 only\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"techniques/rl_navigation/#key-components","title":"Key Components","text":""},{"location":"techniques/rl_navigation/#1-semantic-encoder-e","title":"1. Semantic Encoder (E)","text":"<p>Role: Map pixels \u2192 semantic features</p> <pre><code>class SemanticEncoder(nn.Module):\n    def __init__(self, model_name=\"dinov2_vits14\"):\n        self.model = torch.hub.load(\"facebookresearch/dinov2\", model_name)\n        self.freeze()  # Phase 1: frozen\n\n    def unfreeze_top_layers(self, n_layers=2):\n        # Phase 2: fine-tune last 2 transformer blocks\n        ...\n</code></pre> <p>Why DINOv2? - Pre-trained on 142M images - Semantic features out-of-the-box - Patch-level features (14\u00d714 grid) - Multiple model sizes (21M - 1.1B params)</p>"},{"location":"techniques/rl_navigation/#2-forward-dynamics-model-p","title":"2. Forward Dynamics Model (P)","text":"<p>Role: Predict next features from current features + action</p> <pre><code>class ForwardDynamicsModel(nn.Module):\n    def forward(self, features_t, action):\n        action_onehot = F.one_hot(action, num_classes=8)\n        x = torch.cat([features_t, action_onehot], dim=-1)\n        return self.model(x)  # \u2192 predicted features_{t+1}\n\n    def compute_intrinsic_reward(self, features_t, action, features_t1):\n        predicted = self.forward(features_t, action)\n        return torch.sum((predicted - features_t1) ** 2, dim=-1)\n</code></pre> <p>Alternative: RND (Random Network Distillation) - Fixed random target network - No action conditioning - More stable for some tasks</p>"},{"location":"techniques/rl_navigation/#3-navigation-policy","title":"3. Navigation Policy (\u03c0)","text":"<p>Role: Choose actions based on semantic features</p> <pre><code>class NavigationPolicy(nn.Module):\n    def __init__(self, feature_dim, action_dim=8):\n        self.actor = ...   # features \u2192 action logits\n        self.critic = ...  # features \u2192 value estimate\n\n    def act(self, features):\n        logits, value = self.forward(features)\n        action = Categorical(logits=logits).sample()\n        return action, log_prob, value\n</code></pre> <p>Actions: 8-connected movement - 0: RIGHT, 1: DOWN-RIGHT, 2: DOWN, ... - Extensions: jump actions, scout actions</p>"},{"location":"techniques/rl_navigation/#4-image-navigation-environment","title":"4. Image Navigation Environment","text":"<p>Role: MDP formulation for navigation</p> <p>State: <code>(position, visited_mask, features)</code></p> <p>Action: 8-connected movement (or extended with jumps)</p> <p>Reward: <pre><code># Prediction error (intrinsic curiosity)\npred_error = predictor.compute_intrinsic_reward(z_t, action, z_t1)\n\n# Multi-step lookahead (exponentially weighted)\nlookahead = sum(exp(-\u03bb*d) * prediction_error_at_distance_d\n                for d in range(1, horizon))\n\n# Coverage bonus (prevent loops)\ncoverage_bonus = \u03b2 / sqrt(visit_count)\n\ntotal_reward = pred_error + lookahead + coverage_bonus\n</code></pre></p>"},{"location":"techniques/rl_navigation/#two-phase-training","title":"Two-Phase Training","text":"<p>Critical design choice: don't train encoder and policy simultaneously!</p>"},{"location":"techniques/rl_navigation/#phase-1-frozen-encoder-10k-episodes","title":"Phase 1: Frozen Encoder (10k episodes)","text":"<pre><code>encoder.freeze()  # All parameters frozen\n\n# Train policy and predictor with stable features\nfor episode in range(10000):\n    rollout = collect_rollout()\n    update_policy(rollout)      # PPO\n    update_predictor(rollout)   # Forward dynamics\n</code></pre> <p>Why freeze? - Policy needs stable feature space to learn - Prevents encoder collapse - Focuses learning on navigation, not features</p>"},{"location":"techniques/rl_navigation/#phase-2-fine-tuned-encoder-5k-episodes","title":"Phase 2: Fine-Tuned Encoder (5k episodes)","text":"<pre><code>encoder.unfreeze_top_layers(n_layers=2)\nencoder_lr = 1e-5  # Very small!\n\nfor episode in range(5000):\n    rollout = collect_rollout()\n    update_policy(rollout)\n    update_predictor(rollout)\n    # Encoder updated via predictor gradients only!\n</code></pre> <p>Critical: Policy uses <code>features.detach()</code> to prevent policy loss from corrupting encoder:</p> <pre><code># trainer.py:203\nwith torch.no_grad():\n    action, log_prob, value = policy.act(features.detach())\n</code></pre> <p>Encoder only updated through predictor loss!</p>"},{"location":"techniques/rl_navigation/#learned-behaviors","title":"Learned Behaviors","text":"<p>After training, the agent learns to:</p> <ol> <li>Seek semantic boundaries</li> <li>Car \u2192 road, road \u2192 sky transitions</li> <li> <p>Object edges have high prediction error</p> </li> <li> <p>Follow co-occurrence patterns</p> </li> <li>Cars \u2192 roads \u2192 lane markers</li> <li> <p>Buildings \u2192 windows \u2192 sky</p> </li> <li> <p>Avoid redundant exploration</p> </li> <li>Coverage bonus prevents loops</li> <li> <p>Visited regions have lower value</p> </li> <li> <p>Maximize information gain</p> </li> <li>High-frequency saccades in information-dense regions</li> <li>Quick traversal through uniform areas</li> </ol>"},{"location":"techniques/rl_navigation/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Details - Deep dive into components</li> <li>Training Guide - Best practices and tips</li> <li>Extensions - Jump/scout actions for long-range exploration</li> </ul>"}]}