{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Visual Next Token - RL-Based Image Navigation","text":"<p>Welcome to Visual Next Token! This project implements curiosity-driven reinforcement learning for learning semantic paths through images by maximizing prediction accuracy over a rolling window of future visual tokens.</p>"},{"location":"#what-is-visual-next-token","title":"What is Visual Next Token?","text":"<p>Visual Next Token explores how agents can learn to navigate images by seeking semantically coherent paths. The core insight: maximizing prediction accuracy over a rolling window guides the agent to paths where it can build predictive understanding - from cars to roads to sky, following co-occurrence patterns in visual scenes.</p> <p>Example: Hitting a car edge gives poor initial prediction (color jump), but then excellent predictions of \"more car\" regions. High rolling-window accuracy = semantic coherence = information-rich path.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#rl-based-image-navigation","title":"\ud83e\udde0 RL-Based Image Navigation","text":"<ul> <li>Curiosity-driven exploration using prediction error as intrinsic reward</li> <li>Two-phase training: frozen encoder \u2192 fine-tuned encoder</li> <li>Semantic invariance: DINOv2 features solve the \"car color problem\"</li> <li>Exponential distance weighting for multi-step lookahead planning</li> </ul>"},{"location":"#multiple-intrinsic-motivation-methods","title":"\ud83d\udd2c Multiple Intrinsic Motivation Methods","text":"<ul> <li>ICM (Intrinsic Curiosity Module): Forward dynamics prediction</li> <li>RND (Random Network Distillation): Fixed target network prediction</li> </ul>"},{"location":"#production-ready-components","title":"\ud83c\udfaf Production-Ready Components","text":"<ul> <li>PPO policy optimization with GAE</li> <li>Hierarchical action spaces (base + jump/scout actions)</li> <li>Comprehensive training and visualization tools</li> </ul>"},{"location":"#quick-example","title":"Quick Example","text":"<pre><code>from techniques.rl_navigation import RLTrainer\n\n# Train RL navigator on your image\ntrainer = RLTrainer(\n    image=my_image,\n    encoder_name=\"dinov2_vits14\",\n    phase1_episodes=10000,  # Frozen encoder\n    phase2_episodes=5000,   # Fine-tuned encoder\n)\n\ntrainer.train()\n</code></pre> <pre><code># Visualize learned paths\npython experiments/visualize_rl_paths.py \\\n    --checkpoint checkpoints/rl_navigator/final.pt \\\n    --image my_image.jpg \\\n    --n_episodes 5\n</code></pre>"},{"location":"#the-core-insight","title":"The Core Insight","text":"<p>Traditional approaches predict what pixels come next. We flip this:</p> <p>Our Approach</p> <p>Maximize rolling-window prediction ACCURACY \u2192 Agent seeks semantically coherent paths</p> <ul> <li>High multi-step accuracy = understanding semantic regions (car \u2192 more car)</li> <li>Agent learns paths with rich semantic transitions</li> <li>Semantic features (DINOv2) ensure car color doesn't matter</li> </ul> <p>Traditional Single-Step Approach</p> <p>Maximize immediate prediction ACCURACY \u2192 Agent seeks trivially predictable regions</p> <ul> <li>Stays in uniform sky, blank walls</li> <li>No incentive to explore semantic structure</li> </ul>"},{"location":"#project-structure","title":"Project Structure","text":"<pre><code>image-ssl/\n\u251c\u2500\u2500 techniques/\n\u2502   \u2514\u2500\u2500 rl_navigation/          # RL navigation implementation\n\u2502       \u251c\u2500\u2500 encoder.py          # DINOv2 semantic encoder\n\u2502       \u251c\u2500\u2500 environment.py      # MDP for image navigation\n\u2502       \u251c\u2500\u2500 policy.py           # PPO actor-critic\n\u2502       \u251c\u2500\u2500 forward_dynamics.py # ICM / RND\n\u2502       \u251c\u2500\u2500 trainer.py          # Two-phase training\n\u2502       \u251c\u2500\u2500 extensions.py       # Jump/scout actions\n\u2502       \u2514\u2500\u2500 config.py           # Hyperparameters\n\u251c\u2500\u2500 experiments/\n\u2502   \u251c\u2500\u2500 train_rl_navigator.py  # Training script\n\u2502   \u2514\u2500\u2500 visualize_rl_paths.py  # Visualization\n\u2514\u2500\u2500 references/\n    \u2514\u2500\u2500 rl_navigation/          # Key papers with summaries\n</code></pre>"},{"location":"#research-foundation","title":"Research Foundation","text":"<p>Our implementation builds on five key papers:</p> <ol> <li>ICM - Curiosity-driven exploration (Pathak et al., 2017)</li> <li>RND - Random network distillation (Burda et al., 2018)</li> <li>PPO - Policy optimization (Schulman et al., 2017)</li> <li>DINOv2 - Semantic features (Oquab et al., 2023)</li> <li>GAE - Advantage estimation (Schulman et al., 2015)</li> </ol>"},{"location":"#next-steps","title":"Next Steps","text":"<ul> <li> <p>:material-clock-fast:{ .lg .middle } Quick Start</p> <p>Get up and running in minutes</p> <p>:octicons-arrow-right-24: Installation</p> </li> <li> <p>:fontawesome-solid-brain:{ .lg .middle } RL Navigation</p> <p>Deep dive into curiosity-driven navigation</p> <p>:octicons-arrow-right-24: Architecture</p> </li> <li> <p>:material-file-document:{ .lg .middle } Research Papers</p> <p>Understand the theoretical foundations</p> <p>:octicons-arrow-right-24: References</p> </li> <li> <p>:material-api:{ .lg .middle } API Reference</p> <p>Detailed API documentation</p> <p>:octicons-arrow-right-24: API</p> </li> </ul>"},{"location":"api/rl_navigation/","title":"API Reference: RL Navigation","text":"<p>Detailed API documentation for the RL navigation module.</p>"},{"location":"api/rl_navigation/#core-components","title":"Core Components","text":""},{"location":"api/rl_navigation/#semanticencoder","title":"SemanticEncoder","text":"<p>File: <code>techniques/rl_navigation/encoder.py</code></p> <p>DINOv2-based semantic feature extractor.</p> <pre><code>from techniques.rl_navigation import SemanticEncoder\n\nencoder = SemanticEncoder(\n    model_name=\"dinov2_vits14\",  # Model variant\n    patch_size=14,                # Patch size (DINOv2 default)\n    freeze=True,                  # Freeze parameters initially\n    device=\"cuda\"                 # Device\n)\n</code></pre>"},{"location":"api/rl_navigation/#methods","title":"Methods","text":""},{"location":"api/rl_navigation/#freeze","title":"<code>freeze()</code>","text":"<p>Freeze all encoder parameters (Phase 1 training).</p> <pre><code>encoder.freeze()\n</code></pre>"},{"location":"api/rl_navigation/#unfreeze_top_layersn_layers2","title":"<code>unfreeze_top_layers(n_layers=2)</code>","text":"<p>Unfreeze top transformer layers (Phase 2 training).</p> <p>Parameters: - <code>n_layers</code> (int): Number of top layers to unfreeze</p> <pre><code>encoder.unfreeze_top_layers(n_layers=2)\n</code></pre>"},{"location":"api/rl_navigation/#forwardimage","title":"<code>forward(image)</code>","text":"<p>Extract patch features from image.</p> <p>Parameters: - <code>image</code> (torch.Tensor): RGB image (B, 3, H, W)</p> <p>Returns: - <code>features</code> (torch.Tensor): Patch features (B, num_patches_h, num_patches_w, feature_dim)</p>"},{"location":"api/rl_navigation/#get_patch_features_at_positionimage-position-patch_radius1","title":"<code>get_patch_features_at_position(image, position, patch_radius=1)</code>","text":"<p>Extract local patch features around a pixel position.</p> <p>Parameters: - <code>image</code> (torch.Tensor): RGB image - <code>position</code> (tuple): (row, col) pixel coordinates - <code>patch_radius</code> (int): Radius of patch neighborhood</p> <p>Returns: - <code>features</code> (torch.Tensor): Averaged local features (feature_dim,)</p>"},{"location":"api/rl_navigation/#navigationpolicy","title":"NavigationPolicy","text":"<p>File: <code>techniques/rl_navigation/policy.py</code></p> <p>PPO-based actor-critic policy.</p> <pre><code>from techniques.rl_navigation import NavigationPolicy\n\npolicy = NavigationPolicy(\n    feature_dim=384,    # DINOv2 vits14 dimension\n    action_dim=8,       # 8-connected movement\n    hidden_dim=256      # Hidden layer size\n)\n</code></pre>"},{"location":"api/rl_navigation/#methods_1","title":"Methods","text":""},{"location":"api/rl_navigation/#forwardfeatures","title":"<code>forward(features)</code>","text":"<p>Forward pass through policy.</p> <p>Parameters: - <code>features</code> (torch.Tensor): Semantic features (batch_size, feature_dim)</p> <p>Returns: - <code>action_logits</code> (torch.Tensor): Action distribution logits (batch_size, action_dim) - <code>value</code> (torch.Tensor): Value estimate (batch_size, 1)</p>"},{"location":"api/rl_navigation/#actfeatures-deterministicfalse","title":"<code>act(features, deterministic=False)</code>","text":"<p>Sample action from policy.</p> <p>Parameters: - <code>features</code> (torch.Tensor): Semantic features - <code>deterministic</code> (bool): If True, take argmax; else sample</p> <p>Returns: - <code>action</code> (torch.Tensor): Sampled action index - <code>log_prob</code> (torch.Tensor): Log probability of action - <code>value</code> (torch.Tensor): Value estimate</p> <pre><code>action, log_prob, value = policy.act(features, deterministic=True)\n</code></pre>"},{"location":"api/rl_navigation/#evaluate_actionsfeatures-actions","title":"<code>evaluate_actions(features, actions)</code>","text":"<p>Evaluate actions for PPO update.</p> <p>Parameters: - <code>features</code> (torch.Tensor): Semantic features (batch_size, feature_dim) - <code>actions</code> (torch.Tensor): Actions taken (batch_size,)</p> <p>Returns: - <code>log_probs</code> (torch.Tensor): Log probabilities - <code>values</code> (torch.Tensor): Value estimates - <code>entropy</code> (torch.Tensor): Action distribution entropy</p>"},{"location":"api/rl_navigation/#forwarddynamicsmodel","title":"ForwardDynamicsModel","text":"<p>File: <code>techniques/rl_navigation/forward_dynamics.py</code></p> <p>Predicts next semantic features given current features and action.</p> <pre><code>from techniques.rl_navigation import ForwardDynamicsModel\n\npredictor = ForwardDynamicsModel(\n    feature_dim=384,\n    action_dim=8,\n    hidden_dim=512\n)\n</code></pre>"},{"location":"api/rl_navigation/#methods_2","title":"Methods","text":""},{"location":"api/rl_navigation/#forwardfeatures_t-action","title":"<code>forward(features_t, action)</code>","text":"<p>Predict next features.</p> <p>Parameters: - <code>features_t</code> (torch.Tensor): Current features (batch_size, feature_dim) - <code>action</code> (torch.Tensor): Action indices (batch_size,)</p> <p>Returns: - <code>predicted_features</code> (torch.Tensor): Predicted next features (batch_size, feature_dim)</p>"},{"location":"api/rl_navigation/#compute_intrinsic_rewardfeatures_t-action-features_t1","title":"<code>compute_intrinsic_reward(features_t, action, features_t1)</code>","text":"<p>Compute prediction error as intrinsic reward.</p> <p>Parameters: - <code>features_t</code> (torch.Tensor): Current features - <code>action</code> (torch.Tensor): Actions taken - <code>features_t1</code> (torch.Tensor): Actual next features</p> <p>Returns: - <code>reward</code> (torch.Tensor): Prediction error (batch_size,)</p> <pre><code>reward = predictor.compute_intrinsic_reward(feat_t, action, feat_t1)\n</code></pre>"},{"location":"api/rl_navigation/#rndintrinsicmotivation","title":"RNDIntrinsicMotivation","text":"<p>File: <code>techniques/rl_navigation/forward_dynamics.py</code></p> <p>Random Network Distillation for intrinsic motivation.</p> <pre><code>from techniques.rl_navigation import RNDIntrinsicMotivation\n\nrnd = RNDIntrinsicMotivation(\n    feature_dim=384,\n    hidden_dim=512\n)\n</code></pre>"},{"location":"api/rl_navigation/#methods_3","title":"Methods","text":""},{"location":"api/rl_navigation/#compute_intrinsic_rewardfeatures","title":"<code>compute_intrinsic_reward(features)</code>","text":"<p>Compute RND prediction error.</p> <p>Parameters: - <code>features</code> (torch.Tensor): State features</p> <p>Returns: - <code>reward</code> (torch.Tensor): RND prediction error</p>"},{"location":"api/rl_navigation/#imagenavigationenv","title":"ImageNavigationEnv","text":"<p>File: <code>techniques/rl_navigation/environment.py</code></p> <p>MDP environment for image navigation.</p> <pre><code>from techniques.rl_navigation import ImageNavigationEnv\n\nenv = ImageNavigationEnv(\n    image=rgb_image,              # numpy array (H, W, 3)\n    encoder=encoder,              # SemanticEncoder\n    predictor=predictor,          # ForwardDynamicsModel or RND\n    max_steps=500,                # Episode length\n    reward_horizon=10,            # Lookahead steps\n    reward_lambda=0.1,            # Exponential decay\n    coverage_bonus_weight=0.1,    # Coverage bonus\n    device=\"cuda\"\n)\n</code></pre>"},{"location":"api/rl_navigation/#methods_4","title":"Methods","text":""},{"location":"api/rl_navigation/#reset","title":"<code>reset()</code>","text":"<p>Reset environment to random starting position.</p> <p>Returns: - <code>state</code> (dict): Initial state with keys:   - <code>\"position\"</code>: (row, col) tuple   - <code>\"features\"</code>: Semantic features tensor   - <code>\"visited\"</code>: Visited mask</p> <pre><code>state = env.reset()\nposition = state[\"position\"]\nfeatures = state[\"features\"]\n</code></pre>"},{"location":"api/rl_navigation/#stepaction","title":"<code>step(action)</code>","text":"<p>Execute action and return next state.</p> <p>Parameters: - <code>action</code> (int): Action index (0-7 for base actions)</p> <p>Returns: - <code>state</code> (dict): Next state - <code>reward</code> (float): Reward for transition - <code>done</code> (bool): Episode termination flag - <code>info</code> (dict): Additional information</p> <pre><code>next_state, reward, done, info = env.step(action)\n</code></pre>"},{"location":"api/rl_navigation/#get_statistics","title":"<code>get_statistics()</code>","text":"<p>Get episode statistics.</p> <p>Returns: - <code>stats</code> (dict): Statistics with keys:   - <code>\"coverage\"</code>: Fraction of image visited   - <code>\"path_length\"</code>: Number of steps taken   - <code>\"unique_positions\"</code>: Number of unique positions</p>"},{"location":"api/rl_navigation/#rltrainer","title":"RLTrainer","text":"<p>File: <code>techniques/rl_navigation/trainer.py</code></p> <p>Two-phase trainer orchestrating the full training loop.</p> <pre><code>from techniques.rl_navigation import RLTrainer\n\ntrainer = RLTrainer(\n    image=rgb_image,\n    encoder_name=\"dinov2_vits14\",\n    device=\"cuda\",\n    phase1_episodes=10000,\n    phase2_episodes=5000,\n    policy_lr=3e-4,\n    predictor_lr=1e-3,\n    encoder_lr=1e-5,\n    # ... many more parameters (see config.py)\n)\n</code></pre>"},{"location":"api/rl_navigation/#methods_5","title":"Methods","text":""},{"location":"api/rl_navigation/#train","title":"<code>train()</code>","text":"<p>Run full two-phase training.</p> <pre><code>trainer.train()\n</code></pre>"},{"location":"api/rl_navigation/#save_checkpointfilename","title":"<code>save_checkpoint(filename)</code>","text":"<p>Save training checkpoint.</p> <p>Parameters: - <code>filename</code> (str): Checkpoint filename</p> <pre><code>trainer.save_checkpoint(\"checkpoint.pt\")\n</code></pre>"},{"location":"api/rl_navigation/#load_checkpointfilename","title":"<code>load_checkpoint(filename)</code>","text":"<p>Load training checkpoint.</p> <p>Parameters: - <code>filename</code> (str): Checkpoint filename</p> <pre><code>trainer.load_checkpoint(\"checkpoint.pt\")\n</code></pre>"},{"location":"api/rl_navigation/#configuration","title":"Configuration","text":""},{"location":"api/rl_navigation/#rlconfig","title":"RLConfig","text":"<p>File: <code>techniques/rl_navigation/config.py</code></p> <p>Configuration dataclass for hyperparameters.</p> <pre><code>from techniques.rl_navigation.config import RLConfig, get_config\n\n# Get preset config\nconfig = get_config(\"default\")\n\n# Or create custom\nconfig = RLConfig(\n    encoder_name=\"dinov2_vits14\",\n    phase1_episodes=10000,\n    phase2_episodes=5000,\n    policy_lr=3e-4,\n    # ... etc\n)\n</code></pre>"},{"location":"api/rl_navigation/#available-presets","title":"Available Presets","text":"<pre><code>config = get_config(\"default\")      # Standard training\nconfig = get_config(\"quick_test\")   # Fast testing\nconfig = get_config(\"rnd\")          # Use RND\nconfig = get_config(\"long\")         # Extended training\n</code></pre>"},{"location":"api/rl_navigation/#extensions","title":"Extensions","text":""},{"location":"api/rl_navigation/#extendedactionspace","title":"ExtendedActionSpace","text":"<p>File: <code>techniques/rl_navigation/extensions.py</code></p> <p>Extended action space with jump/scout actions.</p> <pre><code>from techniques.rl_navigation.extensions import ExtendedActionSpace\n\naction_space = ExtendedActionSpace(\n    jump_distance=7,\n    use_jumps=True,\n    use_scouts=False\n)\n\nnext_pos, action_type = action_space.get_next_position(\n    current_pos=(100, 100),\n    action_id=9,  # Jump action\n    image_shape=(224, 224)\n)\n</code></pre>"},{"location":"api/rl_navigation/#hierarchicalpolicy","title":"HierarchicalPolicy","text":"<p>File: <code>techniques/rl_navigation/extensions.py</code></p> <p>Two-level policy for extended action spaces.</p> <pre><code>from techniques.rl_navigation.extensions import HierarchicalPolicy\n\npolicy = HierarchicalPolicy(\n    feature_dim=384,\n    action_space=action_space,\n    hidden_dim=256\n)\n</code></pre>"},{"location":"api/rl_navigation/#constants","title":"Constants","text":""},{"location":"api/rl_navigation/#action-space","title":"Action Space","text":"<pre><code># Base 8-connected actions\nACTIONS = {\n    0: (0, 1),    # RIGHT\n    1: (1, 1),    # DOWN_RIGHT\n    2: (1, 0),    # DOWN\n    3: (1, -1),   # DOWN_LEFT\n    4: (0, -1),   # LEFT\n    5: (-1, -1),  # UP_LEFT\n    6: (-1, 0),   # UP\n    7: (-1, 1),   # UP_RIGHT\n}\n</code></pre>"},{"location":"api/rl_navigation/#type-hints","title":"Type Hints","text":"<p>The codebase uses type hints. Example:</p> <pre><code>from typing import Tuple, Dict, Optional\nimport torch\n\ndef get_features(\n    position: Tuple[int, int],\n    encoder: SemanticEncoder,\n    image: torch.Tensor\n) -&gt; torch.Tensor:\n    ...\n</code></pre>"},{"location":"api/rl_navigation/#next-steps","title":"Next Steps","text":"<ul> <li>See Architecture for system design</li> <li>Check Training Guide for usage</li> <li>Review Extensions for jump/scout actions</li> </ul>"},{"location":"development/contributing/","title":"Contributing","text":"<p>Guidelines for contributing to Visual Next Token.</p>"},{"location":"development/contributing/#welcome","title":"Welcome!","text":"<p>Thank you for your interest in contributing to Visual Next Token! This project explores curiosity-driven reinforcement learning for image navigation, and we welcome contributions of all kinds.</p>"},{"location":"development/contributing/#ways-to-contribute","title":"Ways to Contribute","text":""},{"location":"development/contributing/#1-report-bugs","title":"1. \ud83d\udc1b Report Bugs","text":"<p>Found a bug? Please create an issue with: - Clear description of the problem - Steps to reproduce - Expected vs actual behavior - Environment details (OS, Python version, GPU) - Error messages or logs</p>"},{"location":"development/contributing/#2-suggest-features","title":"2. \ud83d\udca1 Suggest Features","text":"<p>Have an idea? Open an issue with: - Use case description - Proposed solution - Alternative approaches considered - Potential challenges</p>"},{"location":"development/contributing/#3-improve-documentation","title":"3. \ud83d\udcdd Improve Documentation","text":"<p>Documentation contributions are highly valued: - Fix typos or unclear explanations - Add examples or tutorials - Improve API documentation - Create guides for common tasks</p>"},{"location":"development/contributing/#4-contribute-research","title":"4. \ud83d\udd2c Contribute Research","text":"<p>Share your experiments: - Novel reward formulations - Ablation studies - Baseline comparisons - New application domains</p>"},{"location":"development/contributing/#5-submit-code","title":"5. \ud83d\udcbb Submit Code","text":"<p>Code contributions welcome: - Bug fixes - New features - Performance improvements - Test coverage</p>"},{"location":"development/contributing/#development-setup","title":"Development Setup","text":""},{"location":"development/contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/visual-next-token.git\ncd visual-next-token\n\n# Add upstream remote\ngit remote add upstream https://github.com/georgepearse/visual-next-token.git\n</code></pre>"},{"location":"development/contributing/#2-create-virtual-environment","title":"2. Create Virtual Environment","text":"<pre><code>python -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n\n# Install development dependencies\npip install pytest black isort mypy\n</code></pre>"},{"location":"development/contributing/#3-create-branch","title":"3. Create Branch","text":"<pre><code>git checkout -b feature/your-feature-name\n# or\ngit checkout -b fix/bug-description\n</code></pre>"},{"location":"development/contributing/#code-style","title":"Code Style","text":""},{"location":"development/contributing/#python-style","title":"Python Style","text":"<p>We follow PEP 8 with some modifications:</p> <pre><code># Format code with black\nblack techniques/ experiments/\n\n# Sort imports with isort\nisort techniques/ experiments/\n\n# Type check with mypy (coming soon)\nmypy techniques/\n</code></pre>"},{"location":"development/contributing/#conventions","title":"Conventions","text":"<ul> <li>Line length: 88 characters (black default)</li> <li>Imports: Sorted with isort</li> <li>Type hints: Encouraged for all public functions</li> <li>Docstrings: Google style</li> </ul> <p>Example: <pre><code>def compute_reward(\n    features_t: torch.Tensor,\n    features_t1: torch.Tensor,\n    action: torch.Tensor\n) -&gt; torch.Tensor:\n    \"\"\"Compute rolling-window prediction accuracy reward.\n\n    Args:\n        features_t: Current semantic features (batch_size, feature_dim)\n        features_t1: Next semantic features (batch_size, feature_dim)\n        action: Action taken (batch_size,)\n\n    Returns:\n        reward: Computed reward (batch_size,)\n    \"\"\"\n    ...\n</code></pre></p>"},{"location":"development/contributing/#testing","title":"Testing","text":""},{"location":"development/contributing/#running-tests","title":"Running Tests","text":"<pre><code># Run all tests\npytest\n\n# Run specific test file\npytest tests/test_encoder.py\n\n# Run with coverage\npytest --cov=techniques tests/\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<p>Place tests in <code>tests/</code> directory:</p> <pre><code># tests/test_encoder.py\nimport pytest\nimport torch\nfrom techniques.rl_navigation import SemanticEncoder\n\ndef test_encoder_freeze():\n    encoder = SemanticEncoder(model_name=\"dinov2_vits14\")\n    encoder.freeze()\n\n    # Check all parameters frozen\n    for param in encoder.parameters():\n        assert not param.requires_grad\n\ndef test_encoder_unfreeze():\n    encoder = SemanticEncoder(model_name=\"dinov2_vits14\")\n    encoder.freeze()\n    encoder.unfreeze_top_layers(n_layers=2)\n\n    # Check some parameters unfrozen\n    unfrozen_params = [p for p in encoder.parameters() if p.requires_grad]\n    assert len(unfrozen_params) &gt; 0\n</code></pre>"},{"location":"development/contributing/#pull-request-process","title":"Pull Request Process","text":""},{"location":"development/contributing/#1-update-your-branch","title":"1. Update Your Branch","text":"<pre><code>git fetch upstream\ngit rebase upstream/main\n</code></pre>"},{"location":"development/contributing/#2-make-your-changes","title":"2. Make Your Changes","text":"<ul> <li>Write clear, concise commit messages</li> <li>Keep commits focused and atomic</li> <li>Add tests for new features</li> <li>Update documentation</li> </ul>"},{"location":"development/contributing/#3-run-checks","title":"3. Run Checks","text":"<pre><code># Format code\nblack techniques/ experiments/\nisort techniques/ experiments/\n\n# Run tests\npytest\n\n# Check types (if mypy configured)\nmypy techniques/\n</code></pre>"},{"location":"development/contributing/#4-push-and-create-pr","title":"4. Push and Create PR","text":"<pre><code>git push origin feature/your-feature-name\n</code></pre> <p>Then create a Pull Request on GitHub with: - Clear description of changes - Link to related issues - Screenshots/examples if applicable - Checklist of completed items</p>"},{"location":"development/contributing/#pr-template","title":"PR Template","text":"<pre><code>## Description\nBrief description of changes\n\n## Related Issues\nFixes #123\n\n## Changes Made\n- [ ] Added feature X\n- [ ] Updated documentation\n- [ ] Added tests\n- [ ] Ran formatting (black, isort)\n\n## Testing\nDescribe how you tested the changes\n\n## Checklist\n- [ ] Code follows style guidelines\n- [ ] Tests pass\n- [ ] Documentation updated\n- [ ] No breaking changes (or documented)\n</code></pre>"},{"location":"development/contributing/#commit-message-guidelines","title":"Commit Message Guidelines","text":"<p>Use conventional commits format:</p> <pre><code>type(scope): short description\n\nLonger description if needed\n\nFixes #123\n</code></pre> <p>Types: - <code>feat</code>: New feature - <code>fix</code>: Bug fix - <code>docs</code>: Documentation only - <code>style</code>: Code style (formatting, no logic change) - <code>refactor</code>: Code refactoring - <code>test</code>: Adding tests - <code>chore</code>: Maintenance tasks</p> <p>Examples: <pre><code>feat(encoder): add support for DINOv2 vitg14 model\n\nAdds support for the 1.1B parameter DINOv2 model variant.\nIncludes configuration preset and memory optimization.\n\nFixes #45\n</code></pre></p> <pre><code>fix(environment): correct rolling-window reward computation\n\nPrevious implementation used prediction error instead of accuracy.\nUpdated to match documented rolling-window accuracy formulation.\n\nFixes #67\n</code></pre>"},{"location":"development/contributing/#documentation","title":"Documentation","text":""},{"location":"development/contributing/#building-docs-locally","title":"Building Docs Locally","text":"<pre><code># Install MkDocs\npip install mkdocs-material\n\n# Serve locally\nmkdocs serve\n\n# Open http://127.0.0.1:8000\n</code></pre>"},{"location":"development/contributing/#adding-new-pages","title":"Adding New Pages","text":"<ol> <li>Create <code>.md</code> file in <code>docs/</code></li> <li>Add to <code>mkdocs.yml</code> navigation</li> <li>Link from relevant pages</li> <li>Build and verify</li> </ol>"},{"location":"development/contributing/#research-contributions","title":"Research Contributions","text":""},{"location":"development/contributing/#sharing-experiments","title":"Sharing Experiments","text":"<p>Create an issue or discussion with: - Research question - Methodology - Results (with visualizations) - Code/config used - Conclusions</p>"},{"location":"development/contributing/#proposing-research-directions","title":"Proposing Research Directions","text":"<p>See TODO list for: - Planned experiments - Open questions - Research priorities</p>"},{"location":"development/contributing/#code-review-process","title":"Code Review Process","text":"<p>All contributions go through code review:</p> <ol> <li>Automated checks: CI/CD runs tests and linting</li> <li>Maintainer review: Code quality and design</li> <li>Feedback: Request changes if needed</li> <li>Approval: Merge when ready</li> </ol> <p>Review criteria: - Correctness - Code quality - Test coverage - Documentation - Performance impact</p>"},{"location":"development/contributing/#community-guidelines","title":"Community Guidelines","text":""},{"location":"development/contributing/#be-respectful","title":"Be Respectful","text":"<ul> <li>Welcoming to all contributors</li> <li>Constructive feedback</li> <li>Assume good intentions</li> <li>Focus on ideas, not people</li> </ul>"},{"location":"development/contributing/#ask-questions","title":"Ask Questions","text":"<ul> <li>No question is too basic</li> <li>Use GitHub Discussions for questions</li> <li>Check existing issues first</li> <li>Provide context and examples</li> </ul>"},{"location":"development/contributing/#give-credit","title":"Give Credit","text":"<ul> <li>Acknowledge contributions</li> <li>Link to related work</li> <li>Cite relevant papers</li> <li>Thank reviewers</li> </ul>"},{"location":"development/contributing/#license","title":"License","text":"<p>By contributing, you agree that your contributions will be licensed under the MIT License.</p>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":"<ul> <li>Questions: Open a GitHub Discussion</li> <li>Bugs: Create an Issue</li> <li>Security: Email (add contact info)</li> </ul>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in: - GitHub contributors page - Release notes - Documentation acknowledgments</p>"},{"location":"development/contributing/#next-steps","title":"Next Steps","text":"<ul> <li>Check TODO list for contribution ideas</li> <li>Review open issues</li> <li>Join discussions on GitHub</li> <li>Share your experiments!</li> </ul> <p>Thank you for contributing to Visual Next Token! \ud83d\ude80</p>"},{"location":"development/todo/","title":"Project TODO List","text":"<p>This page tracks the development status of Visual Next Token, including completed features, ongoing work, and planned improvements.</p> <p>Last Updated: 2025-11-02</p>"},{"location":"development/todo/#completed","title":"\u2705 Completed","text":""},{"location":"development/todo/#core-rl-navigation-implementation","title":"Core RL Navigation Implementation","text":"<ul> <li>[x] Semantic encoder wrapper (DINOv2) with freeze/unfreeze capabilities</li> <li>[x] Image navigation environment (MDP formulation)</li> <li>[x] Forward dynamics model (ICM) for intrinsic motivation</li> <li>[x] Random Network Distillation (RND) as alternative intrinsic motivation</li> <li>[x] PPO policy with actor-critic architecture</li> <li>[x] GAE (Generalized Advantage Estimation) for advantage computation</li> <li>[x] Two-phase training system (frozen \u2192 fine-tuned encoder)</li> <li>[x] Configuration management with multiple presets</li> <li>[x] Training script with checkpoint/resume support</li> <li>[x] Visualization script for learned paths</li> <li>[x] Extended action spaces (jump/scout actions)</li> </ul>"},{"location":"development/todo/#documentation","title":"Documentation","text":"<ul> <li>[x] MkDocs Material documentation site</li> <li>[x] Comprehensive README with quick start</li> <li>[x] Installation guide</li> <li>[x] Quick start tutorial</li> <li>[x] RL navigation architecture documentation</li> <li>[x] Research paper summaries (ICM, RND, PPO, DINOv2, GAE)</li> <li>[x] References directory with detailed paper notes</li> <li>[x] GitHub Pages deployment</li> <li>[x] Conceptual framing clarified (rolling-window accuracy)</li> </ul>"},{"location":"development/todo/#repository-setup","title":"Repository Setup","text":"<ul> <li>[x] Repository renamed to <code>visual-next-token</code></li> <li>[x] Git repository initialized and pushed</li> <li>[x] GitHub Pages enabled</li> <li>[x] Documentation deployed</li> </ul>"},{"location":"development/todo/#in-progress","title":"\ud83d\udea7 In Progress","text":""},{"location":"development/todo/#documentation-improvements","title":"Documentation Improvements","text":"<ul> <li>[x] Complete architecture deep-dive page (techniques/rl_navigation/architecture.md)</li> <li>[x] Training guide with best practices (techniques/rl_navigation/training.md)</li> <li>[x] Extensions guide for jump/scout actions (techniques/rl_navigation/extensions.md)</li> <li>[x] API reference documentation (api/rl_navigation.md)</li> <li>[x] Contributing guide (development/contributing.md)</li> </ul>"},{"location":"development/todo/#code-quality","title":"Code Quality","text":"<ul> <li>[ ] Add type hints throughout codebase</li> <li>[ ] Add docstrings to all public methods</li> <li>[ ] Set up pytest test suite</li> <li>[ ] Add unit tests for core components</li> <li>[ ] Add integration tests for training pipeline</li> </ul>"},{"location":"development/todo/#high-priority-todo","title":"\ud83d\udccb High Priority TODO","text":""},{"location":"development/todo/#critical-features","title":"Critical Features","text":""},{"location":"development/todo/#1-reward-mechanism-alignment","title":"1. Reward Mechanism Alignment","text":"<p>Status: Documentation updated, code needs review Priority: High Description: Ensure the reward computation correctly implements rolling-window prediction accuracy, not just prediction error.</p> <p>Files to review: - <code>techniques/rl_navigation/environment.py</code> (lines 150-250) - <code>techniques/rl_navigation/forward_dynamics.py</code></p> <p>Current implementation: <pre><code># environment.py:_compute_lookahead_reward\ndef _compute_lookahead_reward(self, current_pos):\n    total_reward = 0.0\n    for distance in range(1, self.reward_horizon):\n        weight = np.exp(-self.reward_lambda * distance)\n        # Currently: prediction error\n        # Should verify: rolling-window accuracy\n</code></pre></p> <p>Action items: - [ ] Verify lookahead reward implements rolling-window accuracy - [ ] Consider inverting sign if currently using error (make it accuracy-based) - [ ] Add comprehensive comments explaining the rolling-window framing - [ ] Update variable names to reflect accuracy vs error</p>"},{"location":"development/todo/#2-testing-infrastructure","title":"2. Testing Infrastructure","text":"<p>Status: Not started Priority: High Description: Add comprehensive test suite before making significant changes.</p> <p>Action items: - [ ] Set up pytest configuration - [ ] Add tests for encoder (freeze/unfreeze, feature extraction) - [ ] Add tests for environment (state transitions, reward computation) - [ ] Add tests for policy (action sampling, evaluation) - [ ] Add tests for forward dynamics (prediction, intrinsic reward) - [ ] Add integration test for full training loop (quick config) - [ ] Set up CI/CD with GitHub Actions</p>"},{"location":"development/todo/#3-experiment-validation","title":"3. Experiment Validation","text":"<p>Status: Not started Priority: High Description: Run experiments to validate that the system works as intended.</p> <p>Action items: - [ ] Run quick_test config on synthetic image - [ ] Verify agent explores semantic regions (not random walk) - [ ] Measure coverage and path statistics - [ ] Compare RND vs ICM on same image - [ ] Test on real-world images (ImageNet samples) - [ ] Document experimental results</p>"},{"location":"development/todo/#medium-priority-todo","title":"\ud83d\udcca Medium Priority TODO","text":""},{"location":"development/todo/#feature-enhancements","title":"Feature Enhancements","text":""},{"location":"development/todo/#1-advanced-visualization","title":"1. Advanced Visualization","text":"<p>Files: <code>experiments/visualize_rl_paths.py</code></p> <ul> <li>[ ] Add heatmap of visited regions</li> <li>[ ] Show prediction error/accuracy over trajectory</li> <li>[ ] Visualize semantic feature space (t-SNE/UMAP)</li> <li>[ ] Animate path exploration over time</li> <li>[ ] Compare learned paths to baselines (random, greedy)</li> </ul>"},{"location":"development/todo/#2-training-improvements","title":"2. Training Improvements","text":"<p>Files: <code>techniques/rl_navigation/trainer.py</code></p> <ul> <li>[ ] Add TensorBoard logging</li> <li>[ ] Implement early stopping based on coverage</li> <li>[ ] Add learning rate scheduling</li> <li>[ ] Support distributed training (multi-GPU)</li> <li>[ ] Add curriculum learning (start with simple images)</li> </ul>"},{"location":"development/todo/#3-environment-extensions","title":"3. Environment Extensions","text":"<p>Files: <code>techniques/rl_navigation/environment.py</code>, <code>techniques/rl_navigation/extensions.py</code></p> <ul> <li>[ ] Support multi-image training (generalization)</li> <li>[ ] Add image augmentation during training</li> <li>[ ] Implement hierarchical navigation (coarse \u2192 fine)</li> <li>[ ] Add semantic segmentation integration (ground truth comparison)</li> <li>[ ] Support different patch sizes (adaptive resolution)</li> </ul>"},{"location":"development/todo/#4-documentation-pages","title":"4. Documentation Pages","text":"<p>Directory: <code>docs/</code></p> <ul> <li>[ ] Create architecture deep-dive (with diagrams)</li> <li>[ ] Write training guide with hyperparameter tuning tips</li> <li>[ ] Document extensions (jump/scout) with usage examples</li> <li>[ ] Add API reference (auto-generated from docstrings)</li> <li>[ ] Create troubleshooting guide</li> <li>[ ] Add gallery of learned paths on different images</li> </ul>"},{"location":"development/todo/#research-experiments-todo","title":"\ud83d\udd2c Research &amp; Experiments TODO","text":""},{"location":"development/todo/#novel-research-directions","title":"Novel Research Directions","text":""},{"location":"development/todo/#1-multi-image-generalization","title":"1. Multi-Image Generalization","text":"<p>Description: Train on multiple images, test generalization to unseen images.</p> <ul> <li>[ ] Implement multi-image dataset loader</li> <li>[ ] Modify trainer to sample images per episode</li> <li>[ ] Test on held-out validation images</li> <li>[ ] Compare to single-image overfitting</li> </ul>"},{"location":"development/todo/#2-semantic-segmentation-integration","title":"2. Semantic Segmentation Integration","text":"<p>Description: Use segmentation masks to evaluate semantic exploration quality.</p> <ul> <li>[ ] Integrate segmentation model (e.g., SAM)</li> <li>[ ] Measure coverage of different semantic classes</li> <li>[ ] Evaluate if agent prioritizes rare objects</li> <li>[ ] Compare to coverage-based baselines</li> </ul>"},{"location":"development/todo/#3-ablation-studies","title":"3. Ablation Studies","text":"<p>Description: Understand what components are critical.</p> <p>Experiments: - [ ] Frozen vs fine-tuned encoder (Phase 1 only vs Phase 2) - [ ] ICM vs RND intrinsic motivation - [ ] Different DINOv2 model sizes (vits14 vs vitb14 vs vitl14) - [ ] Coverage bonus weight (0.0 vs 0.1 vs 0.5) - [ ] Reward horizon (5 vs 10 vs 20 steps) - [ ] Policy architecture (hidden dim, layers)</p>"},{"location":"development/todo/#4-comparison-to-baselines","title":"4. Comparison to Baselines","text":"<p>Description: Establish that RL approach outperforms simpler alternatives.</p> <p>Baselines: - [ ] Random walk - [ ] Greedy (always move to highest-error neighbor) - [ ] Saliency-based navigation - [ ] Optical flow following - [ ] Edge-following heuristic</p>"},{"location":"development/todo/#known-issues","title":"\ud83d\udc1b Known Issues","text":""},{"location":"development/todo/#critical-bugs","title":"Critical Bugs","text":"<p>None currently identified</p>"},{"location":"development/todo/#minor-issues","title":"Minor Issues","text":"<ol> <li>~~MkDocs warnings for missing pages~~ \u2705 RESOLVED</li> <li>~~Several linked pages in navigation don't exist yet~~</li> <li>~~Warnings about broken internal links~~</li> <li> <p>Fixed: Created all missing documentation pages (2025-11-02)</p> </li> <li> <p>Extension tests not implemented</p> </li> <li><code>techniques/rl_navigation/extensions.py</code> has test code but needs PyTorch installed</li> <li> <p>Fix: Add to test suite with proper dependencies</p> </li> <li> <p>~~Documentation links inconsistency~~ \u2705 RESOLVED</p> </li> <li>~~Some use <code>rl-navigation</code> (kebab-case), some use <code>rl_navigation</code> (snake_case)~~</li> <li>Fixed: Standardized on snake_case throughout (2025-11-02)</li> </ol>"},{"location":"development/todo/#ideas-future-work","title":"\ud83d\udca1 Ideas &amp; Future Work","text":""},{"location":"development/todo/#long-term-research-directions","title":"Long-term Research Directions","text":""},{"location":"development/todo/#1-transfer-learning","title":"1. Transfer Learning","text":"<ul> <li>Pre-train navigator on large image dataset</li> <li>Fine-tune on specific domains (medical, satellite)</li> <li>Investigate what navigation patterns transfer</li> </ul>"},{"location":"development/todo/#2-multi-agent-exploration","title":"2. Multi-Agent Exploration","text":"<ul> <li>Multiple agents exploring same image</li> <li>Communication between agents</li> <li>Collaborative coverage optimization</li> </ul>"},{"location":"development/todo/#3-active-vision","title":"3. Active Vision","text":"<ul> <li>Agent controls camera in 3D environment</li> <li>Navigate real-world scenes (not just images)</li> <li>Integration with robotics simulators</li> </ul>"},{"location":"development/todo/#4-hierarchical-semantic-navigation","title":"4. Hierarchical Semantic Navigation","text":"<ul> <li>Learn high-level semantic goals (\"find animals\")</li> <li>Decompose into low-level navigation primitives</li> <li>Options framework for temporally extended actions</li> </ul>"},{"location":"development/todo/#5-self-supervised-pre-training","title":"5. Self-Supervised Pre-training","text":"<ul> <li>Use learned navigation to generate training data</li> <li>Discover semantic segmentation from exploration</li> <li>Bootstrap better feature representations</li> </ul>"},{"location":"development/todo/#documentation-todo","title":"\ud83d\udcdd Documentation TODO","text":""},{"location":"development/todo/#pages-to-create","title":"Pages to Create","text":"<ol> <li>techniques/rl_navigation/architecture.md</li> <li>System architecture diagram</li> <li>Component interaction details</li> <li>Data flow visualization</li> <li> <p>Mathematical formulations</p> </li> <li> <p>techniques/rl_navigation/training.md</p> </li> <li>Step-by-step training walkthrough</li> <li>Hyperparameter tuning guide</li> <li>Common issues and solutions</li> <li> <p>Performance optimization tips</p> </li> <li> <p>techniques/rl_navigation/extensions.md</p> </li> <li>Jump/scout action usage</li> <li>Hierarchical policy details</li> <li>Custom action space design</li> <li> <p>Integration examples</p> </li> <li> <p>api/rl-navigation.md</p> </li> <li>Auto-generated API reference</li> <li>Class documentation</li> <li>Method signatures</li> <li> <p>Usage examples</p> </li> <li> <p>development/contributing.md</p> </li> <li>Contribution guidelines</li> <li>Code style guide</li> <li>PR process</li> <li>Development setup</li> </ol>"},{"location":"development/todo/#documentation-improvements_1","title":"Documentation Improvements","text":"<ul> <li>[ ] Add more code examples throughout</li> <li>[ ] Create interactive Jupyter notebooks</li> <li>[ ] Add video demonstrations of learned paths</li> <li>[ ] Improve math rendering (LaTeX)</li> <li>[ ] Add bibliography/citations page</li> <li>[ ] Create FAQ section</li> </ul>"},{"location":"development/todo/#milestones","title":"\ud83c\udfaf Milestones","text":""},{"location":"development/todo/#milestone-1-core-validation-target-1-week","title":"Milestone 1: Core Validation (Target: 1 week)","text":"<ul> <li>[ ] Fix reward mechanism if needed</li> <li>[ ] Run basic experiments</li> <li>[ ] Validate semantic exploration behavior</li> <li>[ ] Document experimental results</li> </ul>"},{"location":"development/todo/#milestone-2-testing-quality-target-2-weeks","title":"Milestone 2: Testing &amp; Quality (Target: 2 weeks)","text":"<ul> <li>[ ] Complete test suite</li> <li>[ ] Add CI/CD</li> <li>[ ] Type hints throughout</li> <li>[ ] Documentation complete</li> </ul>"},{"location":"development/todo/#milestone-3-research-publication-target-1-2-months","title":"Milestone 3: Research Publication (Target: 1-2 months)","text":"<ul> <li>[ ] Run comprehensive experiments</li> <li>[ ] Compare to baselines</li> <li>[ ] Ablation studies</li> <li>[ ] Write paper draft</li> </ul>"},{"location":"development/todo/#questions-decisions-needed","title":"\ud83d\udcde Questions &amp; Decisions Needed","text":""},{"location":"development/todo/#open-questions","title":"Open Questions","text":"<ol> <li>Reward formulation: Is current implementation truly rolling-window accuracy, or does it need adjustment?</li> <li>Current: Exponentially-weighted sum of prediction errors</li> <li> <p>Target: Should we invert to accuracy? Add baseline comparison?</p> </li> <li> <p>Training stability: Are there any known stability issues with current setup?</p> </li> <li>Two-phase training seems sound</li> <li> <p>Need empirical validation</p> </li> <li> <p>Evaluation metrics: What metrics best capture \"semantic exploration quality\"?</p> </li> <li>Coverage alone insufficient</li> <li>Need semantic diversity metrics</li> <li> <p>Consider segmentation-based evaluation</p> </li> <li> <p>Baseline comparisons: Which baselines are most important?</p> </li> <li>Random walk (essential)</li> <li>Greedy prediction error (natural comparison)</li> <li>Saliency-based (vision literature)</li> </ol>"},{"location":"development/todo/#design-decisions","title":"Design Decisions","text":"<ol> <li>Should we support multi-image training by default?</li> <li>Pro: Better generalization</li> <li>Con: More complex, slower training</li> <li> <p>Decision: Add as optional mode, keep single-image as default</p> </li> <li> <p>Visualization: real-time or post-hoc only?</p> </li> <li>Real-time helps debugging but slows training</li> <li> <p>Decision: Post-hoc by default, optional real-time flag</p> </li> <li> <p>Testing: unit vs integration focus?</p> </li> <li>Both needed, but which first?</li> <li>Decision: Start with integration tests (full training), then unit tests</li> </ol>"},{"location":"development/todo/#changelog","title":"\ud83d\udd04 Changelog","text":""},{"location":"development/todo/#2025-11-02","title":"2025-11-02","text":"<ul> <li>\u2705 Created comprehensive TODO list</li> <li>\u2705 Identified reward mechanism review as high priority</li> <li>\u2705 Documented completed RL navigation implementation</li> <li>\u2705 Listed all pending documentation pages</li> <li>\u2705 Added research directions and ablation study plans</li> <li>\u2705 Resolved all MkDocs warnings - created 5 comprehensive documentation pages:</li> <li>Architecture deep dive</li> <li>Training guide</li> <li>Extensions guide (jump/scout actions)</li> <li>API reference</li> <li>Contributing guidelines</li> <li>\u2705 Fixed link inconsistencies - standardized on snake_case throughout</li> <li>\u2705 Zero build warnings achieved</li> </ul>"},{"location":"development/todo/#how-to-update-this-todo","title":"How to Update This TODO","text":"<p>When you complete a task: 1. Move from <code>[ ]</code> to <code>[x]</code> 2. Update the date in Changelog 3. Add notes about implementation details if relevant</p> <p>When you add a new task: 1. Choose appropriate priority section 2. Add clear description and action items 3. Link to relevant files/docs 4. Update Changelog with addition</p>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.8+</li> <li>PyTorch 2.0+</li> <li>CUDA-capable GPU (recommended, but CPU works)</li> </ul>"},{"location":"getting-started/installation/#install-from-source","title":"Install from Source","text":"<pre><code># Clone the repository\ngit clone https://github.com/georgepearse/image-ssl.git\ncd image-ssl\n\n# Create virtual environment\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install dependencies\npip install -r requirements.txt\n</code></pre>"},{"location":"getting-started/installation/#dependencies","title":"Dependencies","text":"<p>The project requires:</p> <pre><code>torch&gt;=2.0.0\ntorchvision&gt;=0.15.0\nnumpy&gt;=1.24.0\nopencv-python&gt;=4.7.0\nmatplotlib&gt;=3.7.0\n</code></pre> <p>For RL navigation specifically:</p> <ul> <li>DINOv2: Loaded automatically via <code>torch.hub</code></li> <li>Gym: For RL environment interface (optional)</li> </ul>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<pre><code># Test basic imports\nfrom techniques.rl_navigation import (\n    RLTrainer,\n    SemanticEncoder,\n    NavigationPolicy,\n)\n\nprint(\"\u2713 Installation successful!\")\n</code></pre>"},{"location":"getting-started/installation/#gpu-support","title":"GPU Support","text":"<p>Check if CUDA is available:</p> <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"Device count: {torch.cuda.device_count()}\")\n</code></pre> <p>The code automatically selects GPU if available, otherwise falls back to CPU.</p>"},{"location":"getting-started/installation/#download-pre-trained-models","title":"Download Pre-trained Models","text":"<p>DINOv2 models are downloaded automatically on first use via <code>torch.hub</code>. Available models:</p> Model Parameters Feature Dim Download Size <code>dinov2_vits14</code> 21M 384 ~84 MB <code>dinov2_vitb14</code> 86M 768 ~330 MB <code>dinov2_vitl14</code> 300M 1024 ~1.1 GB <code>dinov2_vitg14</code> 1.1B 1536 ~4.2 GB <p>First run will download the selected model to <code>~/.cache/torch/hub/</code>.</p>"},{"location":"getting-started/installation/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start Guide - Run your first RL navigation experiment</li> <li>RL Navigation Architecture - Understand the system design</li> </ul>"},{"location":"getting-started/quickstart/","title":"Quick Start","text":"<p>Get started with RL-based image navigation in 5 minutes!</p>"},{"location":"getting-started/quickstart/#1-train-on-test-image","title":"1. Train on Test Image","text":"<p>The fastest way to see the system in action:</p> <pre><code>python experiments/train_rl_navigator.py --config quick_test\n</code></pre> <p>This uses: - Synthetic test image (colored shapes on gradient) - 100 episodes Phase 1 + 50 episodes Phase 2 - Small model (<code>dinov2_vits14</code>) - Should complete in ~10 minutes on GPU</p>"},{"location":"getting-started/quickstart/#2-train-on-your-own-image","title":"2. Train on Your Own Image","text":"<pre><code>python experiments/train_rl_navigator.py \\\n    --image path/to/your/image.jpg \\\n    --config quick_test\n</code></pre> <p>The image will be automatically resized to 512\u00d7512 if larger.</p>"},{"location":"getting-started/quickstart/#3-visualize-learned-paths","title":"3. Visualize Learned Paths","text":"<p>After training completes:</p> <pre><code>python experiments/visualize_rl_paths.py \\\n    --checkpoint checkpoints/rl_navigator/final.pt \\\n    --n_episodes 5\n</code></pre> <p>This will: - Load the trained policy - Run 5 episodes on the test image - Save visualizations to <code>experiments/outputs/rl_paths/</code> - Display a comparison of the learned paths</p>"},{"location":"getting-started/quickstart/#understanding-the-output","title":"Understanding the Output","text":""},{"location":"getting-started/quickstart/#training-logs","title":"Training Logs","text":"<pre><code>Phase 1 - Episode 10/100\n  Avg Reward: 12.34\n  Avg Length: 234.5\n  Coverage: 45.23%\n  Pred Error: 0.0234\n  Policy Loss: 0.1234\n  Value Loss: 0.0456\n  Entropy: 1.234\n</code></pre> Metric Meaning Avg Reward Intrinsic reward (prediction error + coverage bonus) Avg Length Steps per episode (max 500) Coverage % of image visited Pred Error Forward model prediction error Policy Loss PPO policy loss Value Loss Value function MSE Entropy Policy entropy (exploration)"},{"location":"getting-started/quickstart/#visualizations","title":"Visualizations","text":"<p>The visualization script creates:</p> <ul> <li><code>rl_path_ep1.png</code> - Individual episode paths</li> <li><code>comparison.png</code> - Side-by-side comparison of first 4 episodes</li> </ul> <p>Paths show: - Green circle: Start position - Red circle: End position - Colored line: Path taken</p>"},{"location":"getting-started/quickstart/#configuration-presets","title":"Configuration Presets","text":"Config Phase 1 Phase 2 Use Case <code>quick_test</code> 100 50 Testing/debugging <code>default</code> 10000 5000 Standard training <code>rnd</code> 10000 5000 Use RND instead of ICM <code>long</code> 20000 10000 Extended training with larger model"},{"location":"getting-started/quickstart/#using-different-configurations","title":"Using Different Configurations","text":""},{"location":"getting-started/quickstart/#rnd-random-network-distillation","title":"RND (Random Network Distillation)","text":"<pre><code>python experiments/train_rl_navigator.py --config rnd\n</code></pre> <p>RND is an alternative to forward dynamics: - More stable training - No action conditioning needed - Good for complex state spaces</p>"},{"location":"getting-started/quickstart/#long-training-with-larger-model","title":"Long Training with Larger Model","text":"<pre><code>python experiments/train_rl_navigator.py --config long\n</code></pre> <p>Uses: - <code>dinov2_vitb14</code> (768-dim features vs 384-dim) - 30k total episodes (20k + 10k) - Better final performance, but slower</p>"},{"location":"getting-started/quickstart/#custom-configuration","title":"Custom Configuration","text":"<p>For full control, use Python API:</p> <pre><code>from techniques.rl_navigation import RLTrainer\nimport cv2\n\n# Load your image\nimage = cv2.imread(\"my_image.jpg\")\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n# Create trainer with custom config\ntrainer = RLTrainer(\n    image=image,\n    encoder_name=\"dinov2_vits14\",\n    phase1_episodes=5000,\n    phase2_episodes=2000,\n    policy_lr=1e-4,\n    predictor_lr=5e-4,\n    use_rnd=False,\n    reward_lambda=0.1,\n    coverage_bonus_weight=0.05,\n    save_dir=\"my_checkpoints\",\n)\n\n# Train\ntrainer.train()\n</code></pre>"},{"location":"getting-started/quickstart/#resuming-from-checkpoint","title":"Resuming from Checkpoint","text":"<pre><code>python experiments/train_rl_navigator.py \\\n    --resume checkpoints/rl_navigator/phase1_ep1000.pt\n</code></pre>"},{"location":"getting-started/quickstart/#common-issues","title":"Common Issues","text":""},{"location":"getting-started/quickstart/#out-of-memory","title":"Out of Memory","text":"<p>If you get CUDA OOM errors:</p> <ol> <li>Use smaller model: <code>dinov2_vits14</code> instead of <code>vitb14</code></li> <li>Reduce rollout steps in config</li> <li>Use CPU: <code>--device cpu</code> (slower)</li> </ol>"},{"location":"getting-started/quickstart/#slow-training","title":"Slow Training","text":"<ul> <li>Ensure GPU is being used: check <code>Device: cuda</code> in output</li> <li>Use <code>quick_test</code> config first</li> <li>Consider reducing image size in training script</li> </ul>"},{"location":"getting-started/quickstart/#low-coverage","title":"Low Coverage","text":"<p>If the agent doesn't explore much:</p> <ul> <li>Increase <code>coverage_bonus_weight</code> (default 0.1)</li> <li>Check that prediction error is non-zero</li> <li>Verify entropy is &gt; 0 (policy is exploring)</li> </ul>"},{"location":"getting-started/quickstart/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Deep Dive - Understand how it works</li> <li>Training Guide - Advanced training strategies</li> <li>Extensions - Jump/scout actions</li> </ul>"},{"location":"references/","title":"Research References","text":"<p>This section contains detailed summaries of the key papers that form the theoretical foundation of our RL-based image navigation implementation.</p>"},{"location":"references/#overview","title":"Overview","text":"<p>Our approach synthesizes ideas from five major research areas:</p> <ol> <li>Curiosity-Driven RL - Using prediction error as intrinsic motivation</li> <li>Semantic Visual Features - Self-supervised learning for robust representations</li> <li>Policy Optimization - Stable, sample-efficient RL algorithms</li> <li>Advantage Estimation - Variance reduction in policy gradients</li> <li>Hierarchical RL - Extended action spaces for long-range planning</li> </ol>"},{"location":"references/#key-papers","title":"Key Papers","text":""},{"location":"references/#rl-navigation-foundation","title":"RL Navigation Foundation","text":"<p>These five papers directly inform our implementation:</p> Paper Year Contribution Our Implementation ICM 2017 Curiosity via prediction error <code>ForwardDynamicsModel</code> RND 2018 Random network distillation <code>RNDIntrinsicMotivation</code> PPO 2017 Clipped policy optimization <code>PPOTrainer</code> DINOv2 2023 Semantic visual features <code>SemanticEncoder</code> GAE 2015 Advantage estimation <code>compute_gae()</code>"},{"location":"references/#quick-reference","title":"Quick Reference","text":""},{"location":"references/#intrinsic-motivation","title":"Intrinsic Motivation","text":"<p>When to use ICM vs RND?</p> <ul> <li>ICM (Forward Dynamics):</li> <li>Action-relevant exploration</li> <li>Learn what actions lead where</li> <li>More sample efficient</li> <li> <p>Can be less stable</p> </li> <li> <p>RND (Random Network Distillation):</p> </li> <li>State-space novelty</li> <li>More stable training</li> <li>Simpler implementation</li> <li>Less action-focused</li> </ul>"},{"location":"references/#feature-learning","title":"Feature Learning","text":"<p>Why DINOv2 over other encoders?</p> <p>DINOv2 provides: - \u2705 Pre-trained on diverse data (142M images) - \u2705 Semantic invariance (car color problem) - \u2705 Patch-level features (natural for navigation) - \u2705 Multiple model sizes (21M - 1.1B params) - \u2705 Strong zero-shot transfer</p> <p>Alternatives considered: - CLIP: Language-biased, less dense features - ResNet: Lower-level features, less semantic - MAE: Reconstruction-focused, not semantic</p>"},{"location":"references/#policy-optimization","title":"Policy Optimization","text":"<p>Why PPO over other methods?</p> <p>PPO balances: - \u2705 Sample efficiency (vs A3C, REINFORCE) - \u2705 Stability (vs TRPO complexity) - \u2705 Simplicity (vs SAC, TD3) - \u2705 Works with discrete actions</p> <p>For image navigation: - Episodes can be 500+ steps - Need stable learning (don't want collapse) - Discrete action space (8 directions) - PPO is battle-tested choice</p>"},{"location":"references/#how-papers-relate","title":"How Papers Relate","text":"<pre><code>Image \u2192 DINOv2 \u2192 Features\n         [#4]        \u2502\n                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                      \u2502\n         \u25bc                      \u25bc\n    Policy (\u03c0)           Forward Model (P)\n    PPO [#3]             ICM/RND [#1,#2]\n         \u2502                      \u2502\n         \u2502                      \u25bc\n         \u2502              Prediction Error\n         \u2502              (Intrinsic Reward)\n         \u2502                      \u2502\n         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                    \u25bc\n              PPO Update\n              with GAE [#5]\n</code></pre>"},{"location":"references/#the-critical-insights","title":"The Critical Insights","text":"<p>1. Flip the Reward (ICM)</p> <p>Don't predict accurately \u2192 predict poorly!</p> <p>High prediction error = novel/surprising = informative</p> <p>2. Semantic Space (DINOv2)</p> <p>Predict in feature space, not pixel space</p> <p>Red car \u2248 Blue car in embeddings, \u2260 in pixels</p> <p>3. Stable Updates (PPO)</p> <p>Clip policy updates to prevent catastrophic collapse</p> <pre><code>ratio = \u03c0_new / \u03c0_old\nclipped_ratio = clip(ratio, 1-\u03b5, 1+\u03b5)\nloss = -min(ratio * A, clipped_ratio * A)\n</code></pre> <p>4. Two-Phase Training (Novel)</p> <p>Phase 1: Frozen encoder \u2192 stable learning Phase 2: Fine-tune encoder \u2192 task adaptation</p> <p>5. Gradient Decoupling (Novel)</p> <p>Policy uses <code>features.detach()</code> \u2192 no policy gradients into encoder</p> <p>Encoder only updated via predictor loss</p>"},{"location":"references/#additional-reading","title":"Additional Reading","text":""},{"location":"references/#hierarchical-rl","title":"Hierarchical RL","text":"<p>For jump/scout actions extension:</p> <ul> <li>Sutton et al., \"Between MDPs and semi-MDPs\" (1999)</li> <li>Kulkarni et al., \"Hierarchical Deep RL\" (2016)</li> <li>Bacon et al., \"The Option-Critic Architecture\" (2017)</li> </ul>"},{"location":"references/#exploration-in-rl","title":"Exploration in RL","text":"<p>Related intrinsic motivation methods:</p> <ul> <li>Schmidhuber, \"Formal Theory of Creativity\" (2010)</li> <li>Stadie et al., \"Incentivizing Exploration\" (2015)</li> <li>Badia et al., \"Never Give Up\" (2020)</li> </ul>"},{"location":"references/#self-supervised-vision","title":"Self-Supervised Vision","text":"<p>Other feature learning approaches:</p> <ul> <li>Chen et al., \"SimCLR\" (2020)</li> <li>He et al., \"Masked Autoencoders\" (2021)</li> <li>Caron et al., \"DINOv1\" (2021)</li> </ul>"},{"location":"references/#citation-guide","title":"Citation Guide","text":"<p>When citing our work or building upon it, please cite the relevant foundation papers.</p> <p>Minimal citation (just ICM + DINOv2): <pre><code>@inproceedings{pathak2017curiosity,\n  title={Curiosity-driven exploration by self-supervised prediction},\n  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},\n  booktitle={ICML},\n  year={2017}\n}\n\n@article{oquab2023dinov2,\n  title={DINOv2: Learning Robust Visual Features without Supervision},\n  author={Oquab, Maxime and others},\n  journal={arXiv preprint arXiv:2304.07193},\n  year={2023}\n}\n</code></pre></p> <p>Complete citation (all five papers):</p> <p>See individual paper pages for full BibTeX.</p>"},{"location":"references/#next-steps","title":"Next Steps","text":"<ul> <li> <p>:material-file-document:{ .lg .middle } Read the Papers</p> <p>Detailed summaries with code connections</p> <p>:octicons-arrow-right-24: RL Navigation Papers</p> </li> <li> <p>:fontawesome-solid-brain:{ .lg .middle } Understand the Architecture</p> <p>How papers combine into working system</p> <p>:octicons-arrow-right-24: Architecture</p> </li> <li> <p>:material-code-braces:{ .lg .middle } See the Code</p> <p>Implementation details and API</p> <p>:octicons-arrow-right-24: API Reference</p> </li> </ul>"},{"location":"references/rl_navigation/","title":"RL Navigation: Key Papers and References","text":"<p>This directory contains detailed summaries of the foundational papers used in our RL-based image navigation implementation.</p>"},{"location":"references/rl_navigation/#overview","title":"Overview","text":"<p>Our approach combines several key techniques from recent RL and vision research:</p> <ol> <li>Curiosity-Driven Exploration - Intrinsic motivation via prediction error</li> <li>Semantic Feature Learning - DINOv2 for appearance-invariant representations</li> <li>Policy Optimization - PPO for stable, sample-efficient learning</li> <li>Advantage Estimation - GAE for variance reduction</li> </ol>"},{"location":"references/rl_navigation/#papers","title":"Papers","text":""},{"location":"references/rl_navigation/#1-curiosity-driven-exploration-by-self-supervised-prediction-icm","title":"1. Curiosity-driven Exploration by Self-supervised Prediction (ICM)","text":"<p>Pathak et al., ICML 2017</p> <ul> <li>Core Idea: Use prediction error as intrinsic reward signal</li> <li>Key Innovation: Learn features via inverse dynamics to focus on agent-relevant aspects</li> <li>In Our Code: <code>ForwardDynamicsModel</code> class (forward_dynamics.py)</li> </ul> <p>Why it matters: Solves the exploration problem - agent seeks information-dense regions by maximizing prediction error in feature space.</p>"},{"location":"references/rl_navigation/#2-exploration-by-random-network-distillation-rnd","title":"2. Exploration by Random Network Distillation (RND)","text":"<p>Burda et al., 2018</p> <ul> <li>Core Idea: Predict fixed random network outputs for intrinsic motivation</li> <li>Key Innovation: Simpler than ICM, no action conditioning needed</li> <li>In Our Code: <code>RNDIntrinsicMotivation</code> class (forward_dynamics.py)</li> </ul> <p>Why it matters: Alternative to ICM with more stable training. Useful when action-conditioned prediction is difficult.</p>"},{"location":"references/rl_navigation/#3-proximal-policy-optimization-ppo","title":"3. Proximal Policy Optimization (PPO)","text":"<p>Schulman et al., 2017</p> <ul> <li>Core Idea: Clip policy updates to prevent catastrophic collapse</li> <li>Key Innovation: Trust region benefits with first-order optimization</li> <li>In Our Code: <code>PPOTrainer</code> class (policy.py)</li> </ul> <p>Why it matters: The workhorse policy learning algorithm. Enables multiple epochs of minibatch updates on collected experience.</p>"},{"location":"references/rl_navigation/#4-dinov2-learning-robust-visual-features-without-supervision","title":"4. DINOv2: Learning Robust Visual Features without Supervision","text":"<p>Oquab et al., 2023</p> <ul> <li>Core Idea: Self-supervised vision transformer trained on 142M curated images</li> <li>Key Innovation: Features work across domains without fine-tuning</li> <li>In Our Code: <code>SemanticEncoder</code> class (encoder.py)</li> </ul> <p>Why it matters: Solves the \"car color problem\" - provides semantic features invariant to low-level appearance variations. Red car and blue car \u2192 same embedding.</p>"},{"location":"references/rl_navigation/#5-generalized-advantage-estimation-gae","title":"5. Generalized Advantage Estimation (GAE)","text":"<p>Schulman et al., 2015</p> <ul> <li>Core Idea: Balance bias-variance tradeoff in advantage estimation</li> <li>Key Innovation: Exponentially-weighted n-step advantages (analogous to TD(\u03bb))</li> <li>In Our Code: <code>PPOTrainer.compute_gae()</code> (policy.py:190-223)</li> </ul> <p>Why it matters: Reduces variance in policy gradients while controlling bias. Critical for stable learning in our long-horizon image navigation task.</p>"},{"location":"references/rl_navigation/#how-they-fit-together","title":"How They Fit Together","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Image (RGB)                       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502  DINOv2 Encoder (E)  \u2502 \u25c4\u2500\u2500 Paper #4\n          \u2502  Semantic Features   \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502                   \u2502\n           \u25bc                   \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 Policy (\u03c0)  \u2502   \u2502 Forward Dynamics \u2502 \u25c4\u2500\u2500 Paper #1 or #2\n    \u2502   PPO       \u2502   \u2502  (P) or RND      \u2502     (ICM or RND)\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                   \u2502\n           \u2502                   \u25bc\n           \u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502          \u2502 Prediction Error\u2502\n           \u2502          \u2502 (Intrinsic      \u2502\n           \u2502          \u2502  Reward)        \u2502\n           \u2502          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                   \u2502\n           \u25bc                   \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502   PPO Update with GAE          \u2502 \u25c4\u2500\u2500 Papers #3 &amp; #5\n    \u2502   - Clipped objective          \u2502\n    \u2502   - Advantage estimation       \u2502\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"references/rl_navigation/#implementation-architecture","title":"Implementation Architecture","text":""},{"location":"references/rl_navigation/#phase-1-frozen-encoder-10k-episodes","title":"Phase 1: Frozen Encoder (10k episodes)","text":"<ul> <li>DINOv2 features frozen (pre-trained)</li> <li>Policy learns navigation with stable semantic space</li> <li>Forward model/RND learns to predict transitions</li> </ul>"},{"location":"references/rl_navigation/#phase-2-fine-tuned-encoder-5k-episodes","title":"Phase 2: Fine-Tuned Encoder (5k episodes)","text":"<ul> <li>Unfreeze top 2 transformer layers</li> <li>Very small learning rate (1e-5)</li> <li>Encoder adapts to task-specific features</li> <li>Critical: Policy uses <code>z.detach()</code> to prevent gradient corruption</li> </ul>"},{"location":"references/rl_navigation/#key-design-decisions","title":"Key Design Decisions","text":"Decision Papers Rationale Prediction error as reward #1, #2 Encourages seeking information-dense regions Semantic features (not pixels) #4 Invariance to irrelevant variations (car color) Two-phase training #4 Stable features first, then task adaptation PPO policy learning #3 Sample efficiency + stability GAE advantages #5 Variance reduction for long episodes Exponential distance weighting Novel Multi-step lookahead planning"},{"location":"references/rl_navigation/#additional-reading","title":"Additional Reading","text":""},{"location":"references/rl_navigation/#hierarchical-rl-for-jump-actions-extension","title":"Hierarchical RL (for jump actions extension)","text":"<ul> <li>Options Framework: Sutton et al., \"Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning\" (1999)</li> <li>Hierarchical DQN: Kulkarni et al., \"Hierarchical Deep Reinforcement Learning\" (2016)</li> </ul>"},{"location":"references/rl_navigation/#intrinsic-motivation","title":"Intrinsic Motivation","text":"<ul> <li>Empowerment: Mohamed &amp; Rezende, \"Variational Information Maximisation for Intrinsically Motivated Reinforcement Learning\" (2015)</li> <li>NGU: Badia et al., \"Never Give Up: Learning Directed Exploration Strategies\" (2020)</li> </ul>"},{"location":"references/rl_navigation/#vision-transformers","title":"Vision Transformers","text":"<ul> <li>ViT: Dosovitskiy et al., \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (2020)</li> <li>DINO (v1): Caron et al., \"Emerging Properties in Self-Supervised Vision Transformers\" (2021)</li> </ul>"},{"location":"references/rl_navigation/#citation","title":"Citation","text":"<p>If you use this implementation or build upon these ideas, please cite the relevant papers:</p> <pre><code>@inproceedings{pathak2017curiosity,\n  title={Curiosity-driven exploration by self-supervised prediction},\n  author={Pathak, Deepak and Agrawal, Pulkit and Efros, Alexei A and Darrell, Trevor},\n  booktitle={ICML},\n  year={2017}\n}\n\n@article{burda2018exploration,\n  title={Exploration by random network distillation},\n  author={Burda, Yuri and Edwards, Harrison and Storkey, Amos and Klimov, Oleg},\n  journal={arXiv preprint arXiv:1810.12894},\n  year={2018}\n}\n\n@article{schulman2017proximal,\n  title={Proximal policy optimization algorithms},\n  author={Schulman, John and Wolski, Filip and Dhariwal, Prafulla and Radford, Alec and Klimov, Oleg},\n  journal={arXiv preprint arXiv:1707.06347},\n  year={2017}\n}\n\n@article{oquab2023dinov2,\n  title={DINOv2: Learning Robust Visual Features without Supervision},\n  author={Oquab, Maxime and others},\n  journal={arXiv preprint arXiv:2304.07193},\n  year={2023}\n}\n\n@article{schulman2015high,\n  title={High-dimensional continuous control using generalized advantage estimation},\n  author={Schulman, John and Moritz, Philipp and Levine, Sergey and Jordan, Michael and Abbeel, Pieter},\n  journal={arXiv preprint arXiv:1506.02438},\n  year={2015}\n}\n</code></pre>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/","title":"Curiosity-driven Exploration by Self-supervised Prediction (ICM)","text":""},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#paper-information","title":"Paper Information","text":"<p>arXiv ID: 1705.05363 Submitted: May 15, 2017 Venue: ICML 2017 Field: Computer Science - Machine Learning</p>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#authors","title":"Authors","text":"<ul> <li>Deepak Pathak</li> <li>Pulkit Agrawal</li> <li>Alexei A. Efros</li> <li>Trevor Darrell</li> </ul>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#abstract","title":"Abstract","text":"<p>The paper addresses exploration in environments with sparse or absent external rewards. The authors propose using curiosity as an intrinsic motivation signal, formulated through an agent's prediction error regarding action consequences in a learned visual feature space. Their approach leverages a self-supervised inverse dynamics model and scales effectively to high-dimensional state spaces like images while avoiding direct pixel prediction.</p>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Novel Curiosity Framework: Formulates intrinsic motivation as prediction error in a learned feature space, enabling exploration without external rewards.</p> </li> <li> <p>Scalability to Visual Domains: Successfully handles image-based environments by operating on learned representations rather than raw pixels.</p> </li> <li> <p>Environmental Relevance: The method focuses on agent-relevant environmental aspects, ignoring irrelevant visual changes.</p> </li> <li> <p>Comprehensive Evaluation: Demonstrates effectiveness across three scenarios:</p> </li> <li>Sparse reward environments requiring fewer interactions</li> <li>Unrewarded exploration showing efficient behavior</li> <li>Generalization to new game levels leveraging prior knowledge</li> </ol>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/1705.05363</li> <li>Website: pathak22.github.io/noreward-rl/</li> <li>PDF: Available on arXiv</li> <li>Code &amp; Demo: Available via project website</li> </ul>"},{"location":"references/rl_navigation/01_curiosity_driven_exploration_ICM/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>This paper forms the theoretical foundation for our ForwardDynamicsModel class. The ICM (Intrinsic Curiosity Module) approach:</p> <ul> <li> <p>Forward Model: Predicts next features from current features + action   <pre><code>predicted_features = forward_model(features_t, action)\nintrinsic_reward = ||predicted_features - features_t1||\u00b2\n</code></pre></p> </li> <li> <p>Feature Learning: Uses inverse dynamics to learn agent-relevant features (solves \"car color problem\")</p> </li> <li>Prediction Error as Reward: High prediction error = novel/surprising state = exploration bonus</li> </ul> <p>In our code (techniques/rl_navigation/forward_dynamics.py:122-133): <pre><code>class ForwardDynamicsModel:\n    def compute_intrinsic_reward(self, features_t, action, features_t1):\n        predicted_features = self.forward(features_t, action)\n        return torch.sum((predicted_features - features_t1) ** 2, dim=-1)\n</code></pre></p> <p>Our implementation differs by: 1. Using pre-trained DINOv2 features instead of learning features from scratch 2. Two-phase training: frozen encoder \u2192 fine-tuned encoder 3. Exponential distance weighting for multi-step lookahead rewards</p>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/","title":"Exploration by Random Network Distillation (RND)","text":""},{"location":"references/rl_navigation/02_random_network_distillation_RND/#paper-information","title":"Paper Information","text":"<p>ArXiv ID: 1810.12894 Submission Date: October 30, 2018 Field: Computer Science &gt; Machine Learning</p>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#authors","title":"Authors","text":"<ul> <li>Yuri Burda</li> <li>Harrison Edwards</li> <li>Amos Storkey</li> <li>Oleg Klimov</li> </ul>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#abstract","title":"Abstract","text":"<p>The paper introduces \"an exploration bonus for deep reinforcement learning methods that is easy to implement and adds minimal overhead to the computation performed.\" The mechanism leverages prediction error from a neural network trained on fixed random features as an intrinsic reward signal.</p>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Random Network Distillation (RND) Bonus: A novel exploration mechanism based on measuring how well a trainable predictor can estimate features from a fixed random network.</p> </li> <li> <p>Flexible Reward Combination: A method for adaptively merging intrinsic (exploration) and extrinsic (task) rewards during training.</p> </li> <li> <p>State-of-the-Art Results: Achieved breakthrough performance on Montezuma's Revenge, \"the first method that achieves better than average human performance on this game without using demonstrations or having access to the underlying state,\" occasionally completing the first level.</p> </li> <li> <p>Hard Exploration Benchmarks: Demonstrated significant improvements across multiple challenging Atari games known for sparse reward signals.</p> </li> </ol>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/1810.12894</li> <li>DOI: https://doi.org/10.48550/arXiv.1810.12894</li> <li>Subject Categories: Machine Learning (cs.LG), Artificial Intelligence (cs.AI), Machine Learning (stat.ML)</li> </ul>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>RND provides an alternative intrinsic motivation mechanism to ICM. Our implementation includes both options (see techniques/rl_navigation/forward_dynamics.py:143-275):</p>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#rnd-architecture","title":"RND Architecture","text":"<pre><code>class RNDIntrinsicMotivation:\n    def __init__(self, feature_dim, hidden_dim=512):\n        # Fixed random target network (never updated)\n        self.target_net = self._build_network(feature_dim, hidden_dim)\n        for param in self.target_net.parameters():\n            param.requires_grad = False\n\n        # Trainable predictor network\n        self.predictor_net = self._build_network(feature_dim, hidden_dim)\n\n    def compute_intrinsic_reward(self, features):\n        with torch.no_grad():\n            target = self.target_net(features)\n        prediction = self.predictor_net(features)\n        return torch.sum((prediction - target) ** 2, dim=-1)\n</code></pre>"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#rnd-vs-icm-comparison","title":"RND vs ICM Comparison","text":"Aspect RND ICM (Forward Dynamics) Prediction Target Fixed random features Next state features Requires Actions No Yes Novelty Signal Feature novelty Transition novelty Stability More stable (fixed target) Requires careful tuning Agent Relevance Less focused More action-relevant"},{"location":"references/rl_navigation/02_random_network_distillation_RND/#usage-in-our-code","title":"Usage in Our Code","text":"<pre><code># Create RND-based trainer\ntrainer = RLTrainer(\n    image=image,\n    use_rnd=True,  # Use RND instead of forward dynamics\n    ...\n)\n\n# Or use config\nfrom techniques.rl_navigation.config import get_config\nconfig = get_config(\"rnd\")\n</code></pre> <p>RND is particularly useful when: - Action-conditioned prediction is difficult - You want more stable intrinsic rewards - Feature-space novelty is the primary exploration signal</p>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/","title":"Proximal Policy Optimization Algorithms (PPO)","text":""},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#paper-information","title":"Paper Information","text":"<p>arXiv ID: 1707.06347 Category: Computer Science &gt; Machine Learning (cs.LG) Submitted: July 20, 2017 Last Revised: August 28, 2017 (v2)</p>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#authors","title":"Authors","text":"<ul> <li>John Schulman</li> <li>Filip Wolski</li> <li>Prafulla Dhariwal</li> <li>Alec Radford</li> <li>Oleg Klimov</li> </ul>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#abstract","title":"Abstract","text":"<p>The authors introduce a novel approach to reinforcement learning through \"a new family of policy gradient methods\" that alternates between environment interaction and objective function optimization. Unlike conventional policy gradient techniques that perform single updates per sample, this work presents a method enabling \"multiple epochs of minibatch updates.\" The resulting algorithm, termed PPO, incorporates advantages from trust region methods while maintaining simplicity and improved empirical sample efficiency. Testing encompasses robotic simulation and Atari games, demonstrating superior performance relative to comparable online policy approaches.</p>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Novel Objective Function: Enables multiple minibatch optimization epochs per data collection phase, improving computational efficiency</p> </li> <li> <p>Simplified Implementation: Maintains benefits of trust region policy optimization (TRPO) while reducing implementation complexity</p> </li> <li> <p>Enhanced Performance: Empirical results show improved sample complexity and wall-time efficiency across benchmark tasks</p> </li> <li> <p>Versatile Application: Successfully applies to diverse domains including simulated robotics and video game playing</p> </li> </ol>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/1707.06347</li> <li>DOI: https://doi.org/10.48550/arXiv.1707.06347</li> <li>OpenAI Blog: https://openai.com/blog/openai-baselines-ppo/</li> </ul>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>PPO is the core policy learning algorithm in our navigation system. Implementation in techniques/rl_navigation/policy.py:148-341.</p>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#ppo-clipped-objective","title":"PPO Clipped Objective","text":"<p>The key innovation is the clipped surrogate objective that prevents too-large policy updates:</p> <pre><code>class PPOTrainer:\n    def update(self, rollout_buffer, n_epochs=4, batch_size=64):\n        # Compute probability ratio\n        ratio = torch.exp(log_probs - batch_old_log_probs)\n\n        # Clipped surrogate objective\n        surr1 = ratio * batch_advantages\n        surr2 = torch.clamp(\n            ratio,\n            1 - self.clip_epsilon,  # Typically 0.2\n            1 + self.clip_epsilon\n        ) * batch_advantages\n        policy_loss = -torch.min(surr1, surr2).mean()\n</code></pre>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#complete-ppo-update","title":"Complete PPO Update","text":"<p>Our implementation includes all PPO components:</p> <ol> <li>Policy Loss (clipped objective)</li> <li>Value Loss (MSE with returns)</li> <li>Entropy Bonus (exploration)</li> </ol> <pre><code># Total loss (policy.py:308-312)\nloss = (\n    policy_loss\n    + self.value_loss_coef * value_loss\n    + self.entropy_coef * entropy_loss\n)\n</code></pre>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#ppo-hyperparameters-configpy","title":"PPO Hyperparameters (config.py)","text":"<pre><code>@dataclass\nclass RLConfig:\n    gamma: float = 0.99           # Discount factor\n    gae_lambda: float = 0.95      # GAE lambda\n    clip_epsilon: float = 0.2     # PPO clipping\n\n    rollout_steps: int = 2048     # Steps per rollout\n    ppo_epochs: int = 4           # PPO update epochs\n    ppo_batch_size: int = 64\n    max_grad_norm: float = 0.5\n</code></pre>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#why-ppo-for-image-navigation","title":"Why PPO for Image Navigation?","text":"<p>PPO is ideal for our task because:</p> <ol> <li>Sample Efficiency: Reuses experience through multiple epochs</li> <li>Stability: Clipped objective prevents catastrophic policy collapse</li> <li>Simplicity: No complex second-order optimization (vs TRPO)</li> <li>Works with Continuous Features: Handles high-dimensional DINOv2 features (384-1024 dims)</li> </ol>"},{"location":"references/rl_navigation/03_proximal_policy_optimization_PPO/#training-loop-trainerpy295-343","title":"Training Loop (trainer.py:295-343)","text":"<pre><code>def train(self):\n    # Phase 1: Frozen encoder\n    for episode in range(self.phase1_episodes):\n        rollout = self.collect_rollout()  # Gather experiences\n        ppo_metrics = self.ppo_trainer.update(rollout)  # PPO update\n\n    # Phase 2: Fine-tuned encoder\n    self.encoder.unfreeze_top_layers(n_layers=2)\n    for episode in range(self.phase2_episodes):\n        ...\n</code></pre>"},{"location":"references/rl_navigation/04_dinov2_visual_features/","title":"DINOv2: Learning Robust Visual Features without Supervision","text":""},{"location":"references/rl_navigation/04_dinov2_visual_features/#paper-information","title":"Paper Information","text":"<p>arXiv ID: 2304.07193 Field: Computer Science &gt; Computer Vision and Pattern Recognition Submitted: April 14, 2023 (v1) Last Revised: February 2, 2024 (v2) DOI: https://doi.org/10.48550/arXiv.2304.07193</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#authors","title":"Authors","text":"<p>Maxime Oquab, Timoth\u00e9e Darcet, Th\u00e9o Moutakanni, Huy Vo, Marc Szafraniec, Vasil Khalidov, Pierre Fernandez, Daniel Haziza, Francisco Massa, Alaaeldin El-Nouby, Mahmoud Assran, Nicolas Ballas, Wojciech Galuba, Russell Howes, Po-Yao Huang, Shang-Wen Li, Ishan Misra, Michael Rabbat, Vasu Sharma, Gabriel Synnaeve, Hu Xu, Herv\u00e9 Jegou, Julien Mairal, Patrick Labatut, Armand Joulin, and Piotr Bojanowski</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#abstract","title":"Abstract","text":"<p>The paper demonstrates that \"existing pretraining methods, especially self-supervised methods, can produce features that work across image distributions and tasks without finetuning if trained on enough curated data.\" The researchers combine established techniques to scale pretraining efforts and introduce methods for accelerating and stabilizing large-scale training.</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#key-contributions","title":"Key Contributions","text":"<ol> <li> <p>Dataset Pipeline: Created an automatic pipeline for building a diverse, curated image dataset rather than relying on unstructured data typically used in self-supervised learning</p> </li> <li> <p>Model Scale: Trained a Vision Transformer with one billion parameters and distilled it into smaller, more practical models</p> </li> <li> <p>Performance: Achieved results surpassing OpenCLIP across most benchmarks at both image and pixel levels</p> </li> <li> <p>Technical Optimizations: Implemented multiple techniques focused on improving training efficiency and stability at scale</p> </li> </ol>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/2304.07193</li> <li>Project Page: https://dinov2.metademolab.com/</li> <li>GitHub: https://github.com/facebookresearch/dinov2</li> <li>Models: Available via torch.hub</li> </ul>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>DINOv2 is the semantic encoder backbone that solves the critical \"car color problem\" - the need for features invariant to low-level variations. Implementation in techniques/rl_navigation/encoder.py.</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#the-car-color-problem","title":"The Car Color Problem","text":"<p>Problem: Pixel-level prediction penalizes irrelevant variations - Red car vs Blue car \u2192 huge pixel difference - Model shouldn't be penalized for unpredictable color</p> <p>Solution: Semantic feature space - Red car \u2192 <code>embedding_car</code> - Blue car \u2192 <code>embedding_car</code> - Prediction in semantic space, not pixel space</p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#dinov2-in-our-code","title":"DINOv2 in Our Code","text":"<pre><code>class SemanticEncoder(nn.Module):\n    def __init__(self, model_name=\"dinov2_vits14\", freeze=True):\n        # Load pre-trained DINOv2\n        self.model = torch.hub.load(\n            \"facebookresearch/dinov2\",\n            model_name\n        )\n        self.feature_dim = self.model.embed_dim  # 384/768/1024\n\n        if freeze:\n            self.freeze()  # Phase 1: frozen features\n</code></pre>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#available-models","title":"Available Models","text":"Model Parameters Feature Dim Patch Size <code>dinov2_vits14</code> 21M 384 14\u00d714 <code>dinov2_vitb14</code> 86M 768 14\u00d714 <code>dinov2_vitl14</code> 300M 1024 14\u00d714 <code>dinov2_vitg14</code> 1.1B 1536 14\u00d714"},{"location":"references/rl_navigation/04_dinov2_visual_features/#two-phase-training-strategy","title":"Two-Phase Training Strategy","text":"<p>Our implementation uses a critical two-phase approach:</p> <p>Phase 1: Frozen Encoder (10k episodes) <pre><code>self.encoder.freeze()  # All parameters frozen\n# Stable semantic space\n# Policy and predictor learn with fixed features\n</code></pre></p> <p>Phase 2: Fine-tuned Encoder (5k episodes) <pre><code>self.encoder.unfreeze_top_layers(n_layers=2)  # Last 2 transformer blocks\n# encoder_lr = 1e-5  # Very small!\n# Task-specific feature adaptation\n</code></pre></p>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#why-this-matters","title":"Why This Matters","text":"<ol> <li>Semantic Invariance: DINOv2 features capture object identity, not appearance</li> <li>Different colored cars \u2192 similar features</li> <li>Different car poses \u2192 similar features</li> <li> <p>Car vs road vs sky \u2192 different features</p> </li> <li> <p>Zero-Shot Transfer: Pre-trained on 142M images</p> </li> <li>Works on any image domain</li> <li> <p>No task-specific training needed for Phase 1</p> </li> <li> <p>Patch-Level Features: Natural grid structure for image navigation    <pre><code># Image: 224\u00d7224 pixels\n# DINOv2 patches: 16\u00d716 grid (patch_size=14)\n# Features: (16, 16, 384) for vits14\n</code></pre></p> </li> </ol>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#feature-extraction-encoderpy189-226","title":"Feature Extraction (encoder.py:189-226)","text":"<pre><code>def get_patch_features_at_position(self, image, position, patch_radius=1):\n    \"\"\"Get local patch features around a position.\"\"\"\n    # Convert pixel position \u2192 patch coordinates\n    patch_row = position[0] // self.patch_size\n    patch_col = position[1] // self.patch_size\n\n    # Extract local neighborhood\n    features = self.patch_features[\n        max(0, patch_row - patch_radius):patch_row + patch_radius + 1,\n        max(0, patch_col - patch_radius):patch_col + patch_radius + 1,\n    ]\n\n    # Average pool \u2192 single feature vector\n    return features.mean(dim=(0, 1))\n</code></pre>"},{"location":"references/rl_navigation/04_dinov2_visual_features/#co-training-architecture","title":"Co-Training Architecture","text":"<pre><code>Image \u2192 DINOv2 \u2192 Features (z) \u2192 Forward Model (P) \u2192 Predicted Features (\u1e91)\n                     \u2193                                         \u2193\n                 Policy (\u03c0)                            Prediction Error\n                     \u2193                                         \u2193\n                  Action                              Intrinsic Reward\n</code></pre> <p>Critical: Policy uses <code>z.detach()</code> to prevent policy gradients from corrupting encoder features!</p> <pre><code># trainer.py:203\nwith torch.no_grad():\n    action, log_prob, value = self.policy.act(features.detach())\n</code></pre> <p>Encoder only updated via predictor gradients in Phase 2.</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/","title":"High-Dimensional Continuous Control Using Generalized Advantage Estimation (GAE)","text":""},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#paper-information","title":"Paper Information","text":"<p>arXiv ID: 1506.02438 Submitted: June 8, 2015 (v1) Last Revised: October 20, 2018 (v6) Subject Areas: Machine Learning (cs.LG), Robotics (cs.RO), Systems and Control (eess.SY)</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#authors","title":"Authors","text":"<ul> <li>John Schulman</li> <li>Philipp Moritz</li> <li>Sergey Levine</li> <li>Michael Jordan</li> <li>Pieter Abbeel</li> </ul>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#abstract","title":"Abstract","text":"<p>The paper introduces an approach to policy gradient methods in reinforcement learning that addresses two key challenges: high sample complexity and training instability. The authors employ value functions to reduce variance in policy gradient estimates while introducing some bias through \"an exponentially-weighted estimator of the advantage function that is analogous to TD(lambda).\" For stability, they use trust region optimization for both policy and value function neural networks.</p> <p>The method demonstrates strong performance on complex 3D locomotion tasks, including bipedal and quadrupedal robot locomotion, with policies learned directly from raw kinematics to joint torques\u2014requiring simulated experience equivalent to 1-2 weeks of real time for 3D bipeds.</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#key-contributions","title":"Key Contributions","text":"<ul> <li>Variance reduction technique: Leverages value functions with a generalized advantage estimation method to improve sample efficiency</li> <li>Stable optimization: Applies trust region methods to both policy and value functions</li> <li>End-to-end learning: Direct mapping from kinematics to control without hand-crafted representations</li> <li>Empirical validation: Successful learning of complex locomotion behaviors in high-dimensional continuous control domains</li> </ul>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#resources","title":"Resources","text":"<ul> <li>arXiv: https://arxiv.org/abs/1506.02438</li> </ul>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#relevance-to-our-implementation","title":"Relevance to Our Implementation","text":"<p>GAE provides the advantage estimation used in our PPO implementation to compute policy gradients with reduced variance. Implementation in techniques/rl_navigation/policy.py:190-223.</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#the-bias-variance-tradeoff","title":"The Bias-Variance Tradeoff","text":"<p>Problem: Policy gradient estimation - High variance \u2192 slow learning, unstable - High bias \u2192 incorrect gradients, poor convergence</p> <p>Solution: GAE with lambda parameter to balance bias-variance</p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#gae-formula","title":"GAE Formula","text":"<p>The advantage function estimates \"how much better is action a than average at state s\":</p> <pre><code>A(s_t, a_t) = Q(s_t, a_t) - V(s_t)\n</code></pre> <p>GAE uses an exponentially-weighted average of n-step advantages:</p> <pre><code>GAE(\u03bb) = (1-\u03bb) * [A^(1) + \u03bb*A^(2) + \u03bb\u00b2*A^(3) + ...]\n</code></pre> <p>where: <pre><code>A^(n) = r_t + \u03b3*r_{t+1} + ... + \u03b3^{n-1}*r_{t+n-1} + \u03b3^n*V(s_{t+n}) - V(s_t)\n</code></pre></p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#implementation-policypy190-223","title":"Implementation (policy.py:190-223)","text":"<pre><code>def compute_gae(self, rewards, values, dones):\n    \"\"\"\n    Compute Generalized Advantage Estimation (GAE).\n\n    Args:\n        rewards: Rewards (T,)\n        values: Value estimates (T+1,) - includes next value\n        dones: Done flags (T,)\n\n    Returns:\n        advantages: GAE advantages (T,)\n        returns: Discounted returns (T,)\n    \"\"\"\n    advantages = torch.zeros_like(rewards)\n    last_advantage = 0\n\n    # Compute advantages backwards in time\n    for t in reversed(range(len(rewards))):\n        if t == len(rewards) - 1:\n            next_value = values[t + 1]\n        else:\n            next_value = values[t + 1]\n\n        # TD error: \u03b4_t = r_t + \u03b3*V(s_{t+1}) - V(s_t)\n        delta = rewards[t] + self.gamma * next_value * (1 - dones[t]) - values[t]\n\n        # GAE: A_t = \u03b4_t + \u03b3*\u03bb*A_{t+1}\n        advantages[t] = delta + self.gamma * self.gae_lambda * (1 - dones[t]) * last_advantage\n        last_advantage = advantages[t]\n\n    # Returns = advantages + values\n    returns = advantages + values[:-1]\n\n    return advantages, returns\n</code></pre>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#gae-hyperparameter-lambda","title":"GAE Hyperparameter: Lambda (\u03bb)","text":"<p>Controls the bias-variance tradeoff:</p> \u03bb value Behavior Variance Bias \u03bb = 0 1-step TD (bootstrapping) Low High \u03bb = 1 Monte Carlo (full returns) High Low \u03bb = 0.95 Balanced (our default) Medium Medium <p>In our config (config.py:26): <pre><code>gae_lambda: float = 0.95  # GAE lambda\n</code></pre></p>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#why-gae-for-image-navigation","title":"Why GAE for Image Navigation?","text":"<ol> <li>Long Episodes: Our episodes can be 500+ steps</li> <li>Monte Carlo (\u03bb=1) would have huge variance</li> <li> <p>Pure TD (\u03bb=0) would have high bias</p> </li> <li> <p>Sparse Rewards: Intrinsic rewards are noisy</p> </li> <li>GAE smooths reward signal</li> <li> <p>Reduces impact of prediction noise</p> </li> <li> <p>Credit Assignment: Multi-step lookahead rewards</p> </li> <li>GAE helps assign credit across time</li> <li>Important for exponentially-weighted future prediction</li> </ol>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#integration-with-ppo-policypy251-256","title":"Integration with PPO (policy.py:251-256)","text":"<pre><code># Compute advantages with GAE\nadvantages, returns = self.compute_gae(rewards, values, dones)\n\n# Normalize advantages (reduce variance)\nadvantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n\n# Use in PPO loss\npolicy_loss = -torch.min(ratio * advantages, clipped_ratio * advantages).mean()\nvalue_loss = F.mse_loss(values_pred, returns)\n</code></pre>"},{"location":"references/rl_navigation/05_generalized_advantage_estimation_GAE/#empirical-results","title":"Empirical Results","text":"<p>The paper showed GAE significantly improves: - Sample efficiency (fewer episodes to convergence) - Training stability (smoother learning curves) - Final performance (higher rewards)</p> <p>These benefits directly apply to our image navigation task, where we need stable learning with limited interaction budget (15k episodes total).</p>"},{"location":"techniques/rl_navigation/","title":"RL-Based Image Navigation","text":""},{"location":"techniques/rl_navigation/#overview","title":"Overview","text":"<p>RL-based image navigation learns semantic paths through images by maximizing prediction accuracy over a rolling window of future semantic features. An agent learns to navigate by finding paths where it can build up predictive understanding of semantic regions.</p>"},{"location":"techniques/rl_navigation/#the-core-idea","title":"The Core Idea","text":""},{"location":"techniques/rl_navigation/#traditional-single-step-prediction","title":"Traditional Single-Step Prediction \u274c","text":"<p>Most approaches optimize immediate prediction accuracy:</p> <pre><code>Reward = -||predicted_pixels - actual_pixels||\u00b2\n</code></pre> <p>Problem: Agent seeks trivially predictable regions (uniform sky, blank walls)</p>"},{"location":"techniques/rl_navigation/#our-approach-rolling-window-accuracy","title":"Our Approach: Rolling Window Accuracy \u2705","text":"<p>We maximize prediction accuracy over multiple future steps:</p> <pre><code>Reward = \u03a3 exp(-\u03bb\u00b7d) \u00b7 accuracy_at_distance_d\n       d=1 to horizon\n</code></pre> <p>Result: Agent seeks semantically coherent paths!</p> <p>Example - hitting a car edge: 1. Step 0\u21921: Poor prediction (road \u2192 car color jump) 2. Steps 1\u219210: Excellent predictions (\"more car\" \u2192 \"more car\") 3. High rolling-window accuracy = semantic coherence = information-rich</p> <p>This is fundamentally different from staying in uniform regions (which have high single-step accuracy but no semantic structure).</p>"},{"location":"techniques/rl_navigation/#why-this-works","title":"Why This Works","text":"<p>The key insight: rolling-window prediction accuracy correlates with semantic coherence.</p> <p>Consider navigating from a car:</p> Next Region Initial Error Subsequent Accuracy Rolling Window Score More of same car Low High (redundant) Medium Road beneath High High (new object, then coherent) High Sky above High High (new context, then coherent) High Uniform wall Low High (but trivial) Low (no structure) <p>The agent learns to follow paths with semantic transitions that build predictive understanding.</p>"},{"location":"techniques/rl_navigation/#the-car-color-problem","title":"The \"Car Color Problem\"","text":"<p>A critical challenge: cars can be any color.</p>"},{"location":"techniques/rl_navigation/#problem","title":"Problem","text":"<p>If we predict in pixel space: - Red car \u2192 Blue car: HUGE prediction error - Model is penalized for unpredictable but irrelevant variation</p>"},{"location":"techniques/rl_navigation/#solution-semantic-features","title":"Solution: Semantic Features","text":"<p>Use DINOv2 to map pixels \u2192 semantic space:</p> <pre><code># Pixel space (\u274c)\nred_car = [255, 0, 0, ...]  # 3\u00d7224\u00d7224 = 150,528 dims\nblue_car = [0, 0, 255, ...] # Very different!\n\n# Semantic space (\u2705)\nencoder(red_car) = [0.12, -0.34, ...]  # 384 dims\nencoder(blue_car) = [0.11, -0.33, ...] # Nearly identical!\n</code></pre> <p>DINOv2 features are invariant to appearance, capturing object identity: - Different colored cars \u2192 similar embeddings - Car vs road \u2192 different embeddings</p>"},{"location":"techniques/rl_navigation/#system-architecture","title":"System Architecture","text":"<pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    RGB Image                         \u2502\n\u2502                  (H \u00d7 W \u00d7 3)                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502  Semantic Encoder    \u2502\n          \u2502     (DINOv2)         \u2502\n          \u2502  Frozen \u2192 Fine-tuned \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502                   \u2502\n           \u2502    Features (z)   \u2502 \u25c4\u2500\u2500 Detach for policy!\n           \u2502                   \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                       \u2502\n         \u25bc                       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Policy    \u2502     \u2502 Forward Dynamics \u2502\n  \u2502    (\u03c0)      \u2502     \u2502      (P)         \u2502\n  \u2502  Actor-     \u2502     \u2502  Predict next    \u2502\n  \u2502  Critic     \u2502     \u2502  features from   \u2502\n  \u2502             \u2502     \u2502  current + action\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                     \u2502\n         \u2502 Action              \u2502 Predicted z'\n         \u25bc                     \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Environment Step                 \u2502\n  \u2502   - Execute action (move)          \u2502\n  \u2502   - Get actual next features (z')  \u2502\n  \u2502   - Compute prediction error:      \u2502\n  \u2502     reward = ||P(z,a) - z'||\u00b2     \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 PPO Update  \u2502\n    \u2502 - Policy    \u2502\n    \u2502 - Value fn  \u2502\n    \u2502 - Predictor \u2502\n    \u2502 - Encoder*  \u2502 * Phase 2 only\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"techniques/rl_navigation/#key-components","title":"Key Components","text":""},{"location":"techniques/rl_navigation/#1-semantic-encoder-e","title":"1. Semantic Encoder (E)","text":"<p>Role: Map pixels \u2192 semantic features</p> <pre><code>class SemanticEncoder(nn.Module):\n    def __init__(self, model_name=\"dinov2_vits14\"):\n        self.model = torch.hub.load(\"facebookresearch/dinov2\", model_name)\n        self.freeze()  # Phase 1: frozen\n\n    def unfreeze_top_layers(self, n_layers=2):\n        # Phase 2: fine-tune last 2 transformer blocks\n        ...\n</code></pre> <p>Why DINOv2? - Pre-trained on 142M images - Semantic features out-of-the-box - Patch-level features (14\u00d714 grid) - Multiple model sizes (21M - 1.1B params)</p>"},{"location":"techniques/rl_navigation/#2-forward-dynamics-model-p","title":"2. Forward Dynamics Model (P)","text":"<p>Role: Predict next features from current features + action</p> <pre><code>class ForwardDynamicsModel(nn.Module):\n    def forward(self, features_t, action):\n        action_onehot = F.one_hot(action, num_classes=8)\n        x = torch.cat([features_t, action_onehot], dim=-1)\n        return self.model(x)  # \u2192 predicted features_{t+1}\n\n    def compute_intrinsic_reward(self, features_t, action, features_t1):\n        predicted = self.forward(features_t, action)\n        return torch.sum((predicted - features_t1) ** 2, dim=-1)\n</code></pre> <p>Alternative: RND (Random Network Distillation) - Fixed random target network - No action conditioning - More stable for some tasks</p>"},{"location":"techniques/rl_navigation/#3-navigation-policy","title":"3. Navigation Policy (\u03c0)","text":"<p>Role: Choose actions based on semantic features</p> <pre><code>class NavigationPolicy(nn.Module):\n    def __init__(self, feature_dim, action_dim=8):\n        self.actor = ...   # features \u2192 action logits\n        self.critic = ...  # features \u2192 value estimate\n\n    def act(self, features):\n        logits, value = self.forward(features)\n        action = Categorical(logits=logits).sample()\n        return action, log_prob, value\n</code></pre> <p>Actions: 8-connected movement - 0: RIGHT, 1: DOWN-RIGHT, 2: DOWN, ... - Extensions: jump actions, scout actions</p>"},{"location":"techniques/rl_navigation/#4-image-navigation-environment","title":"4. Image Navigation Environment","text":"<p>Role: MDP formulation for navigation</p> <p>State: <code>(position, visited_mask, features)</code></p> <p>Action: 8-connected movement (or extended with jumps)</p> <p>Reward: <pre><code># Prediction error (intrinsic curiosity)\npred_error = predictor.compute_intrinsic_reward(z_t, action, z_t1)\n\n# Multi-step lookahead (exponentially weighted)\nlookahead = sum(exp(-\u03bb*d) * prediction_error_at_distance_d\n                for d in range(1, horizon))\n\n# Coverage bonus (prevent loops)\ncoverage_bonus = \u03b2 / sqrt(visit_count)\n\ntotal_reward = pred_error + lookahead + coverage_bonus\n</code></pre></p>"},{"location":"techniques/rl_navigation/#two-phase-training","title":"Two-Phase Training","text":"<p>Critical design choice: don't train encoder and policy simultaneously!</p>"},{"location":"techniques/rl_navigation/#phase-1-frozen-encoder-10k-episodes","title":"Phase 1: Frozen Encoder (10k episodes)","text":"<pre><code>encoder.freeze()  # All parameters frozen\n\n# Train policy and predictor with stable features\nfor episode in range(10000):\n    rollout = collect_rollout()\n    update_policy(rollout)      # PPO\n    update_predictor(rollout)   # Forward dynamics\n</code></pre> <p>Why freeze? - Policy needs stable feature space to learn - Prevents encoder collapse - Focuses learning on navigation, not features</p>"},{"location":"techniques/rl_navigation/#phase-2-fine-tuned-encoder-5k-episodes","title":"Phase 2: Fine-Tuned Encoder (5k episodes)","text":"<pre><code>encoder.unfreeze_top_layers(n_layers=2)\nencoder_lr = 1e-5  # Very small!\n\nfor episode in range(5000):\n    rollout = collect_rollout()\n    update_policy(rollout)\n    update_predictor(rollout)\n    # Encoder updated via predictor gradients only!\n</code></pre> <p>Critical: Policy uses <code>features.detach()</code> to prevent policy loss from corrupting encoder:</p> <pre><code># trainer.py:203\nwith torch.no_grad():\n    action, log_prob, value = policy.act(features.detach())\n</code></pre> <p>Encoder only updated through predictor loss!</p>"},{"location":"techniques/rl_navigation/#learned-behaviors","title":"Learned Behaviors","text":"<p>After training, the agent learns to:</p> <ol> <li>Seek semantic boundaries</li> <li>Car \u2192 road, road \u2192 sky transitions</li> <li> <p>Object edges have high prediction error</p> </li> <li> <p>Follow co-occurrence patterns</p> </li> <li>Cars \u2192 roads \u2192 lane markers</li> <li> <p>Buildings \u2192 windows \u2192 sky</p> </li> <li> <p>Avoid redundant exploration</p> </li> <li>Coverage bonus prevents loops</li> <li> <p>Visited regions have lower value</p> </li> <li> <p>Maximize information gain</p> </li> <li>High-frequency saccades in information-dense regions</li> <li>Quick traversal through uniform areas</li> </ol>"},{"location":"techniques/rl_navigation/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Details - Deep dive into components</li> <li>Training Guide - Best practices and tips</li> <li>Extensions - Jump/scout actions for long-range exploration</li> </ul>"},{"location":"techniques/rl_navigation/architecture/","title":"Architecture Deep Dive","text":"<p>This page provides detailed architectural information for the RL navigation system.</p>"},{"location":"techniques/rl_navigation/architecture/#system-overview","title":"System Overview","text":"<p>The Visual Next Token system consists of several key components working together to enable curiosity-driven image navigation:</p> <pre><code>\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    Image (RGB)                       \u2502\n\u2502                  (H \u00d7 W \u00d7 3)                         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n                     \u25bc\n          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n          \u2502  Semantic Encoder    \u2502\n          \u2502     (DINOv2)         \u2502\n          \u2502  Frozen \u2192 Fine-tuned \u2502\n          \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n           \u2502                   \u2502\n           \u2502    Features (z)   \u2502 \u25c4\u2500\u2500 Detach for policy!\n           \u2502                   \u2502\n           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2502\n         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n         \u2502                       \u2502\n         \u25bc                       \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Policy    \u2502     \u2502 Forward Dynamics \u2502\n  \u2502    (\u03c0)      \u2502     \u2502      (P)         \u2502\n  \u2502  Actor-     \u2502     \u2502  Predict next    \u2502\n  \u2502  Critic     \u2502     \u2502  features from   \u2502\n  \u2502             \u2502     \u2502  current + action\u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502                     \u2502\n         \u2502 Action              \u2502 Predicted z'\n         \u25bc                     \u25bc\n  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n  \u2502   Environment Step                 \u2502\n  \u2502   - Execute action (move)          \u2502\n  \u2502   - Get actual next features (z')  \u2502\n  \u2502   - Compute rolling-window         \u2502\n  \u2502     accuracy reward                \u2502\n  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n    \u2502 PPO Update  \u2502\n    \u2502 - Policy    \u2502\n    \u2502 - Value fn  \u2502\n    \u2502 - Predictor \u2502\n    \u2502 - Encoder*  \u2502 * Phase 2 only\n    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre>"},{"location":"techniques/rl_navigation/architecture/#component-details","title":"Component Details","text":""},{"location":"techniques/rl_navigation/architecture/#1-semantic-encoder","title":"1. Semantic Encoder","text":"<p>File: <code>techniques/rl_navigation/encoder.py</code></p> <p>The semantic encoder maps raw pixels to semantic feature vectors that are invariant to appearance variations.</p> <p>Key Methods: - <code>freeze()</code>: Freeze all parameters (Phase 1) - <code>unfreeze_top_layers(n_layers=2)</code>: Unfreeze top transformer blocks (Phase 2) - <code>get_patch_features_at_position()</code>: Extract local patch features</p> <p>Feature Dimensions: - <code>dinov2_vits14</code>: 384 dims - <code>dinov2_vitb14</code>: 768 dims - <code>dinov2_vitl14</code>: 1024 dims</p>"},{"location":"techniques/rl_navigation/architecture/#2-navigation-policy","title":"2. Navigation Policy","text":"<p>File: <code>techniques/rl_navigation/policy.py</code></p> <p>PPO-based actor-critic architecture for action selection.</p> <p>Actor: <code>features \u2192 action_logits</code> (8 directions) Critic: <code>features \u2192 value_estimate</code> (expected return)</p>"},{"location":"techniques/rl_navigation/architecture/#3-forward-dynamics-model","title":"3. Forward Dynamics Model","text":"<p>File: <code>techniques/rl_navigation/forward_dynamics.py</code></p> <p>Predicts next semantic features given current features and action.</p> <p>Variants: - ICM: Action-conditioned prediction - RND: Random network distillation</p>"},{"location":"techniques/rl_navigation/architecture/#4-image-navigation-environment","title":"4. Image Navigation Environment","text":"<p>File: <code>techniques/rl_navigation/environment.py</code></p> <p>MDP formulation for image exploration.</p> <p>State: Position, visited mask, semantic features Action Space: 8-connected movement (or extended) Reward: Rolling-window prediction accuracy</p>"},{"location":"techniques/rl_navigation/architecture/#data-flow","title":"Data Flow","text":""},{"location":"techniques/rl_navigation/architecture/#training-loop","title":"Training Loop","text":"<ol> <li> <p>Rollout Collection:    <pre><code>state = env.reset()\nfor step in range(rollout_steps):\n    features = state[\"features\"].detach()  # Critical!\n    action, log_prob, value = policy.act(features)\n    next_state, reward, done, info = env.step(action)\n    buffer.store(features, action, reward, ...)\n</code></pre></p> </li> <li> <p>PPO Update:    <pre><code>advantages, returns = compute_gae(rewards, values, dones)\nfor epoch in range(ppo_epochs):\n    for batch in minibatches:\n        policy_loss, value_loss = compute_losses(batch)\n        optimizer.step()\n</code></pre></p> </li> <li> <p>Predictor Update:    <pre><code>pred_features = predictor(features_t, actions)\npredictor_loss = mse(pred_features, features_t1)\npredictor_optimizer.step()\n# In Phase 2, encoder gradients flow through this!\n</code></pre></p> </li> </ol>"},{"location":"techniques/rl_navigation/architecture/#critical-design-choices","title":"Critical Design Choices","text":""},{"location":"techniques/rl_navigation/architecture/#gradient-decoupling","title":"Gradient Decoupling","text":"<p>Problem: If policy loss backprops into encoder, features become policy-specific.</p> <p>Solution: Use <code>features.detach()</code> when passing to policy:</p> <pre><code># techniques/rl_navigation/trainer.py:203\nwith torch.no_grad():\n    action, log_prob, value = self.policy.act(features.detach())\n</code></pre> <p>Encoder only updated via predictor gradients in Phase 2.</p>"},{"location":"techniques/rl_navigation/architecture/#two-phase-training","title":"Two-Phase Training","text":"<p>Phase 1 (10k episodes): - Encoder frozen - Stable semantic space - Policy learns navigation</p> <p>Phase 2 (5k episodes): - Top 2 layers unfrozen - Very small LR (1e-5) - Task-specific adaptation</p>"},{"location":"techniques/rl_navigation/architecture/#implementation-notes","title":"Implementation Notes","text":""},{"location":"techniques/rl_navigation/architecture/#memory-efficiency","title":"Memory Efficiency","text":"<ul> <li>DINOv2 features precomputed and cached</li> <li>Rollout buffer uses fixed-size tensors</li> <li>Gradient checkpointing not needed (features detached)</li> </ul>"},{"location":"techniques/rl_navigation/architecture/#computational-cost","title":"Computational Cost","text":"<ul> <li>Forward pass: ~10ms (vits14 on GPU)</li> <li>Training step: ~50ms (includes PPO + predictor updates)</li> <li>Full Phase 1 (10k episodes): ~2-4 hours on V100</li> </ul>"},{"location":"techniques/rl_navigation/architecture/#next-steps","title":"Next Steps","text":"<p>See the Training Guide for practical tips on running experiments.</p>"},{"location":"techniques/rl_navigation/extensions/","title":"Extensions: Jump and Scout Actions","text":"<p>Extended action spaces for long-range image exploration.</p>"},{"location":"techniques/rl_navigation/extensions/#overview","title":"Overview","text":"<p>The base RL navigation system uses 8-connected movement (up, down, left, right, and diagonals) with 1-pixel steps. The extensions module adds:</p> <ul> <li>Jump Actions: Long-range movement (5-10 pixels)</li> <li>Scout Actions: Peek at distant regions without committing</li> </ul> <p>These are optional extensions that can be enabled when needed.</p>"},{"location":"techniques/rl_navigation/extensions/#quick-start","title":"Quick Start","text":"<pre><code>from techniques.rl_navigation import RLTrainer\nfrom techniques.rl_navigation.extensions import (\n    ExtendedActionSpace,\n    HierarchicalPolicy,\n)\n\n# Create extended action space\naction_space = ExtendedActionSpace(\n    jump_distance=7,      # How far to jump\n    use_jumps=True,       # Enable jump actions\n    use_scouts=False,     # Disable scout actions (optional)\n)\n\n# Use hierarchical policy for extended actions\n# (More efficient than flat policy with 16-24 actions)\npolicy = HierarchicalPolicy(\n    feature_dim=384,\n    action_space=action_space,\n    hidden_dim=256,\n)\n\n# Note: Full training integration coming soon\n# Current: Manual construction for experiments\n</code></pre>"},{"location":"techniques/rl_navigation/extensions/#extended-action-space","title":"Extended Action Space","text":""},{"location":"techniques/rl_navigation/extensions/#action-types","title":"Action Types","text":"<p>Base Actions (0-7): 8-connected movement - 0: RIGHT, 1: DOWN-RIGHT, 2: DOWN, etc. - 1 pixel step</p> <p>Jump Actions (8-15): Long-range movement - Same 8 directions - Configurable distance (default: 7 pixels) - Useful for crossing uniform regions quickly</p> <p>Scout Actions (16-23): Non-committing peeks - Look at distant region - Return to current position - Useful for planning</p>"},{"location":"techniques/rl_navigation/extensions/#configuration","title":"Configuration","text":"<pre><code>action_space = ExtendedActionSpace(\n    jump_distance=7,        # Pixels to jump\n    use_jumps=True,         # Enable jumps\n    use_scouts=False,       # Enable scouts (optional)\n)\n\nprint(f\"Total actions: {action_space.action_dim}\")\n# Output: 16 (base + jumps) or 24 (base + jumps + scouts)\n</code></pre>"},{"location":"techniques/rl_navigation/extensions/#hierarchical-policy","title":"Hierarchical Policy","text":"<p>For extended action spaces (16-24 actions), a hierarchical policy is more efficient than a flat policy.</p>"},{"location":"techniques/rl_navigation/extensions/#two-level-decision-making","title":"Two-Level Decision Making","text":"<ol> <li>Meta-Policy: Choose action type (base, jump, or scout)</li> <li>Direction-Policy: Choose direction (8 directions)</li> </ol> <p>This factorization reduces the action space from 24 discrete actions to: - 3 action types \u00d7 8 directions = 11 logits total</p>"},{"location":"techniques/rl_navigation/extensions/#architecture","title":"Architecture","text":"<pre><code>class HierarchicalPolicy:\n    def forward(self, features):\n        shared = self.shared(features)\n\n        # Meta-policy: action type\n        type_logits = self.meta_policy(shared)    # (batch, 3)\n\n        # Direction policy: direction\n        dir_logits = self.direction_policy(shared)  # (batch, 8)\n\n        # Value estimate\n        value = self.critic(shared)\n\n        return type_logits, dir_logits, value\n</code></pre>"},{"location":"techniques/rl_navigation/extensions/#advantages","title":"Advantages","text":"<p>\u2705 Fewer parameters to learn \u2705 Better sample efficiency \u2705 Structured exploration (try different types) \u2705 Interpretable (which type is preferred?)</p>"},{"location":"techniques/rl_navigation/extensions/#jump-actions","title":"Jump Actions","text":""},{"location":"techniques/rl_navigation/extensions/#use-cases","title":"Use Cases","text":"<p>Good for: - Images with large uniform regions (sky, grass) - Quickly reaching distant semantic regions - Exploratory phase (early training)</p> <p>Not ideal for: - Dense semantic images (every pixel matters) - Fine-grained exploration - When 1-pixel precision needed</p>"},{"location":"techniques/rl_navigation/extensions/#example","title":"Example","text":"<pre><code># Get next position with jump action\naction_id = 9  # DOWN_RIGHT jump\nnext_pos, action_type = action_space.get_next_position(\n    current_pos=(100, 100),\n    action_id=action_id,\n    image_shape=(224, 224)\n)\n\nprint(next_pos)       # (107, 107) - jumped 7 pixels\nprint(action_type)    # \"jump\"\n</code></pre>"},{"location":"techniques/rl_navigation/extensions/#scout-actions","title":"Scout Actions","text":""},{"location":"techniques/rl_navigation/extensions/#use-cases_1","title":"Use Cases","text":"<p>Good for: - Planning ahead (look before you leap) - Multi-step reasoning - Avoiding dead ends</p> <p>Not ideal for: - Simple reactive policies - When immediate rewards sufficient - Computational budget limited</p>"},{"location":"techniques/rl_navigation/extensions/#reward-modification","title":"Reward Modification","text":"<p>Scout actions receive modified rewards:</p> <pre><code>from techniques.rl_navigation.extensions import ScoutingRewardModifier\n\nmodifier = ScoutingRewardModifier(\n    scout_reward_scale=0.5,   # Scouts get 50% of normal reward\n    scout_penalty=0.01,        # Small penalty to prevent abuse\n)\n\n# In environment step\nif action_type == \"scout\":\n    reward = modifier.modify_reward(\n        reward=base_reward,\n        action_type=\"scout\",\n        is_scout=True\n    )\n</code></pre> <p>This prevents agents from just scouting forever without committing.</p>"},{"location":"techniques/rl_navigation/extensions/#integration-with-training","title":"Integration with Training","text":""},{"location":"techniques/rl_navigation/extensions/#current-status","title":"Current Status","text":"<p>\u26a0\ufe0f Extensions are implemented but not yet integrated into the main training loop.</p> <p>What works: - \u2705 ExtendedActionSpace class - \u2705 HierarchicalPolicy architecture - \u2705 ScoutingRewardModifier - \u2705 Standalone tests</p> <p>What needs integration: - \u274c Modify RLTrainer to support extended actions - \u274c Update environment to handle jump/scout logic - \u274c Add configuration presets for extensions</p>"},{"location":"techniques/rl_navigation/extensions/#manual-usage","title":"Manual Usage","text":"<p>You can experiment with extensions by modifying the training script:</p> <pre><code># experiments/train_rl_navigator.py\n\nfrom techniques.rl_navigation.extensions import (\n    ExtendedActionSpace,\n    HierarchicalPolicy,\n)\n\n# Create extended action space\naction_space = ExtendedActionSpace(jump_distance=7, use_jumps=True)\n\n# Replace standard policy with hierarchical\npolicy = HierarchicalPolicy(\n    feature_dim=encoder.feature_dim,\n    action_space=action_space,\n    hidden_dim=256,\n).to(device)\n\n# Update environment to use extended action space\n# (requires environment modifications - see TODO)\n</code></pre>"},{"location":"techniques/rl_navigation/extensions/#performance-considerations","title":"Performance Considerations","text":""},{"location":"techniques/rl_navigation/extensions/#memory","title":"Memory","text":"<p>Extended action spaces don't significantly increase memory: - Same rollout buffer size - Policy has similar parameter count (hierarchical)</p>"},{"location":"techniques/rl_navigation/extensions/#computation","title":"Computation","text":"<p>Jumps and scouts affect training time: - Jump actions: Slightly faster (fewer total steps per episode) - Scout actions: Slower (must execute and revert)</p>"},{"location":"techniques/rl_navigation/extensions/#sample-efficiency","title":"Sample Efficiency","text":"<p>Theory: - Jumps may improve exploration in sparse images - Scouts may improve planning but cost samples</p> <p>Empirical validation needed (see TODO).</p>"},{"location":"techniques/rl_navigation/extensions/#testing-extensions","title":"Testing Extensions","text":"<p>Run the standalone test:</p> <pre><code># Note: Requires PyTorch installed\npython techniques/rl_navigation/extensions.py\n</code></pre> <p>Expected output: <pre><code>Testing Extended Action Space...\nTotal actions: 16\nBase: 0-7, Jump: 8-15\nBase action 0: (100, 100) -&gt; (100, 101)\nJump action 9: (100, 100) -&gt; (107, 107)\nExtended action space test passed!\n\nTesting Hierarchical Policy...\nHierarchical policy test passed!\n\nAll tests passed!\n</code></pre></p>"},{"location":"techniques/rl_navigation/extensions/#future-work","title":"Future Work","text":"<p>See TODO List for: - Full training integration - Configuration presets (quick_test_jumps, etc.) - Experimental validation (do jumps help?) - Visualization of jump/scout actions</p>"},{"location":"techniques/rl_navigation/extensions/#references","title":"References","text":"<p>Related work on hierarchical RL: - Options Framework: Sutton et al. (1999) - Hierarchical DQN: Kulkarni et al. (2016) - HAM: Parr &amp; Russell (1998)</p>"},{"location":"techniques/rl_navigation/extensions/#next-steps","title":"Next Steps","text":"<ul> <li>See Architecture for system overview</li> <li>Check Training Guide for standard training</li> <li>Review code: <code>techniques/rl_navigation/extensions.py</code></li> </ul>"},{"location":"techniques/rl_navigation/training/","title":"Training Guide","text":"<p>Practical guide to training RL navigation models.</p>"},{"location":"techniques/rl_navigation/training/#quick-start","title":"Quick Start","text":"<pre><code># Test on synthetic image (fast)\npython experiments/train_rl_navigator.py --config quick_test\n\n# Train on your image\npython experiments/train_rl_navigator.py \\\n    --image path/to/image.jpg \\\n    --config default\n\n# Resume from checkpoint\npython experiments/train_rl_navigator.py \\\n    --resume checkpoints/rl_navigator/phase1_ep1000.pt\n</code></pre>"},{"location":"techniques/rl_navigation/training/#configuration-presets","title":"Configuration Presets","text":""},{"location":"techniques/rl_navigation/training/#quick_test","title":"quick_test","text":"<p>Use for: Testing, debugging, CI/CD</p> <ul> <li>Phase 1: 100 episodes</li> <li>Phase 2: 50 episodes</li> <li>Time: ~5-10 minutes on GPU</li> </ul>"},{"location":"techniques/rl_navigation/training/#default","title":"default","text":"<p>Use for: Standard experiments</p> <ul> <li>Phase 1: 10,000 episodes</li> <li>Phase 2: 5,000 episodes</li> <li>Time: ~2-4 hours on V100</li> </ul>"},{"location":"techniques/rl_navigation/training/#rnd","title":"rnd","text":"<p>Use for: Comparing intrinsic motivation methods</p> <ul> <li>Uses RND instead of ICM</li> <li>Same episode counts as default</li> </ul>"},{"location":"techniques/rl_navigation/training/#long","title":"long","text":"<p>Use for: Maximum performance</p> <ul> <li>Phase 1: 20,000 episodes</li> <li>Phase 2: 10,000 episodes</li> <li>Larger model: <code>dinov2_vitb14</code></li> <li>Time: ~8-12 hours on V100</li> </ul>"},{"location":"techniques/rl_navigation/training/#training-phases","title":"Training Phases","text":""},{"location":"techniques/rl_navigation/training/#phase-1-frozen-encoder","title":"Phase 1: Frozen Encoder","text":"<p>Goal: Learn navigation policy with stable semantic space</p> <p>What happens: - DINOv2 encoder completely frozen - Policy learns from pretrained features - Forward model learns semantic transitions</p> <p>Typical metrics: <pre><code>Episode 1000/10000\n  Avg Reward: 8-12\n  Avg Length: 200-400 steps\n  Coverage: 30-60%\n  Pred Error: 0.01-0.05\n  Entropy: 1.5-2.0 (healthy exploration)\n</code></pre></p> <p>Red flags: - Entropy &lt; 0.5 \u2192 Policy collapsed, restart with higher entropy_coef - Coverage &lt; 10% \u2192 Coverage bonus too small, increase weight - Avg Length &lt; 50 \u2192 Episodes ending too early, check termination</p>"},{"location":"techniques/rl_navigation/training/#phase-2-fine-tuned-encoder","title":"Phase 2: Fine-Tuned Encoder","text":"<p>Goal: Adapt encoder to task-specific semantic patterns</p> <p>What happens: - Top 2 transformer layers unfrozen - Very small LR (1e-5) - Encoder updated only via predictor gradients</p> <p>Typical metrics: <pre><code>Episode 1000/5000\n  Avg Reward: 10-15 (slight improvement)\n  Coverage: 40-70%\n  Pred Error: 0.008-0.03 (decreases)\n</code></pre></p> <p>Red flags: - Reward suddenly drops \u2192 LR too high, encoder diverging - No improvement over Phase 1 \u2192 May not need Phase 2 for this image</p>"},{"location":"techniques/rl_navigation/training/#hyperparameter-tuning","title":"Hyperparameter Tuning","text":""},{"location":"techniques/rl_navigation/training/#critical-parameters","title":"Critical Parameters","text":""},{"location":"techniques/rl_navigation/training/#reward_lambda-default-01","title":"reward_lambda (default: 0.1)","text":"<p>Controls: Exponential decay rate for future predictions</p> <ul> <li>Lower (0.05): More emphasis on distant predictions</li> <li>Higher (0.2): More emphasis on immediate predictions</li> </ul> <p>Tune if: Agent seems too short-sighted or too random</p>"},{"location":"techniques/rl_navigation/training/#coverage_bonus_weight-default-01","title":"coverage_bonus_weight (default: 0.1)","text":"<p>Controls: Penalty for revisiting regions</p> <ul> <li>Lower (0.05): More revisiting, deeper exploration</li> <li>Higher (0.5): Less revisiting, broader coverage</li> </ul> <p>Tune if: Coverage too low or agent loops excessively</p>"},{"location":"techniques/rl_navigation/training/#reward_horizon-default-10","title":"reward_horizon (default: 10)","text":"<p>Controls: How many steps ahead to predict</p> <ul> <li>Lower (5): Faster, more local</li> <li>Higher (20): Slower, more global planning</li> </ul> <p>Tune if: Computational budget or exploration style needs</p>"},{"location":"techniques/rl_navigation/training/#ppo-parameters","title":"PPO Parameters","text":"<p>Usually don't need tuning, but if unstable:</p> <pre><code>clip_epsilon: 0.2    # Lower (0.1) for more conservative updates\npolicy_lr: 3e-4      # Lower (1e-4) if policy diverges\nentropy_coef: 0.01   # Higher (0.05) for more exploration\n</code></pre>"},{"location":"techniques/rl_navigation/training/#monitoring-training","title":"Monitoring Training","text":""},{"location":"techniques/rl_navigation/training/#key-metrics-to-watch","title":"Key Metrics to Watch","text":"<p>Avg Reward: Should increase over time - Phase 1: 5 \u2192 12 - Phase 2: 12 \u2192 15</p> <p>Coverage: Percentage of image visited - Target: 50-80% (depends on image complexity)</p> <p>Pred Error: Forward model accuracy - Should decrease as model learns - Too low (&lt; 0.001) \u2192 May be overfitting</p> <p>Entropy: Policy randomness - Healthy: 1.0-2.0 - Too low (&lt; 0.5) \u2192 Exploitation only - Too high (&gt; 2.5) \u2192 Not learning</p> <p>Policy Loss: Should stabilize - Healthy: 0.1-0.3 - Diverging \u2192 Reduce learning rate</p>"},{"location":"techniques/rl_navigation/training/#tensorboard-coming-soon","title":"Tensorboard (Coming Soon)","text":"<pre><code># Future feature\ntensorboard --logdir checkpoints/rl_navigator/logs\n</code></pre>"},{"location":"techniques/rl_navigation/training/#common-issues","title":"Common Issues","text":""},{"location":"techniques/rl_navigation/training/#training-is-slow","title":"Training is slow","text":"<p>Solutions: 1. Use smaller model: <code>dinov2_vits14</code> (default) 2. Reduce <code>rollout_steps</code>: 2048 \u2192 1024 3. Use CPU only for quick tests: <code>--device cpu</code> 4. Reduce image size in script (currently 512x512)</p>"},{"location":"techniques/rl_navigation/training/#out-of-memory","title":"Out of memory","text":"<p>Solutions: 1. Reduce <code>rollout_steps</code>: 2048 \u2192 512 2. Reduce <code>ppo_batch_size</code>: 64 \u2192 32 3. Use smaller model: <code>vitb14</code> \u2192 <code>vits14</code> 4. Clear GPU cache: <code>torch.cuda.empty_cache()</code></p>"},{"location":"techniques/rl_navigation/training/#agent-doesnt-explore","title":"Agent doesn't explore","text":"<p>Symptoms: Coverage &lt; 20%, stays in one region</p> <p>Solutions: 1. Increase <code>coverage_bonus_weight</code>: 0.1 \u2192 0.5 2. Increase <code>entropy_coef</code>: 0.01 \u2192 0.05 3. Check that reward is non-zero 4. Verify prediction error is being computed</p>"},{"location":"techniques/rl_navigation/training/#training-unstable","title":"Training unstable","text":"<p>Symptoms: Reward/loss oscillates wildly</p> <p>Solutions: 1. Reduce learning rates (all of them) 2. Increase <code>ppo_batch_size</code> for stability 3. Reduce <code>clip_epsilon</code>: 0.2 \u2192 0.1 4. Check for NaN in losses (debug mode)</p>"},{"location":"techniques/rl_navigation/training/#checkpointing","title":"Checkpointing","text":""},{"location":"techniques/rl_navigation/training/#automatic-saves","title":"Automatic Saves","text":"<p>Checkpoints saved every <code>save_interval</code> episodes (default: 1000):</p> <pre><code>checkpoints/rl_navigator/\n\u251c\u2500\u2500 phase1_ep1000.pt\n\u251c\u2500\u2500 phase1_ep2000.pt\n\u251c\u2500\u2500 ...\n\u251c\u2500\u2500 phase2_ep1000.pt\n\u2514\u2500\u2500 final.pt\n</code></pre>"},{"location":"techniques/rl_navigation/training/#manual-save","title":"Manual Save","text":"<p>Press <code>Ctrl+C</code> during training:</p> <pre><code># trainer.py handles this\nexcept KeyboardInterrupt:\n    print(\"Saving checkpoint...\")\n    trainer.save_checkpoint(\"interrupted.pt\")\n</code></pre>"},{"location":"techniques/rl_navigation/training/#resume-training","title":"Resume Training","text":"<pre><code>python experiments/train_rl_navigator.py \\\n    --resume checkpoints/rl_navigator/phase1_ep5000.pt\n</code></pre>"},{"location":"techniques/rl_navigation/training/#best-practices","title":"Best Practices","text":""},{"location":"techniques/rl_navigation/training/#1-start-small","title":"1. Start Small","text":"<p>Always run <code>quick_test</code> first to verify: - Code runs without errors - Agent explores (coverage &gt; 10%) - Metrics look reasonable</p>"},{"location":"techniques/rl_navigation/training/#2-monitor-regularly","title":"2. Monitor Regularly","text":"<p>Check training every ~1000 episodes: - Is coverage increasing? - Is entropy healthy? - Are there NaNs?</p>"},{"location":"techniques/rl_navigation/training/#3-compare-configurations","title":"3. Compare Configurations","text":"<p>Run ablations to understand what matters: - ICM vs RND - Different reward horizons - With/without Phase 2</p>"},{"location":"techniques/rl_navigation/training/#4-visualize-results","title":"4. Visualize Results","text":"<p>After training, always visualize: <pre><code>python experiments/visualize_rl_paths.py \\\n    --checkpoint checkpoints/rl_navigator/final.pt \\\n    --n_episodes 5\n</code></pre></p>"},{"location":"techniques/rl_navigation/training/#advanced-topics","title":"Advanced Topics","text":""},{"location":"techniques/rl_navigation/training/#multi-image-training","title":"Multi-Image Training","text":"<p>Coming soon - currently single image only</p>"},{"location":"techniques/rl_navigation/training/#custom-reward-functions","title":"Custom Reward Functions","text":"<p>Modify <code>environment.py:_compute_reward()</code>:</p> <pre><code>def _compute_reward(self, prev_pos, action, current_pos):\n    # Base: rolling-window accuracy\n    base_reward = self._compute_lookahead_reward(current_pos)\n\n    # Add custom rewards here\n    semantic_bonus = self._compute_semantic_diversity(current_pos)\n\n    return base_reward + semantic_bonus\n</code></pre>"},{"location":"techniques/rl_navigation/training/#distributed-training","title":"Distributed Training","text":"<p>Coming soon - multi-GPU support</p>"},{"location":"techniques/rl_navigation/training/#next-steps","title":"Next Steps","text":"<ul> <li>See Extensions for jump/scout actions</li> <li>Check TODO list for planned features</li> <li>Review Architecture for implementation details</li> </ul>"}]}